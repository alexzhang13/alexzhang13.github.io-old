<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://alexzhang13.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexzhang13.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-02T08:30:07+00:00</updated><id>https://alexzhang13.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">The Annotated Kolmogorov-Arnold Network (KAN)</title><link href="https://alexzhang13.github.io/blog/2024/annotated-kan/" rel="alternate" type="text/html" title="The Annotated Kolmogorov-Arnold Network (KAN)" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://alexzhang13.github.io/blog/2024/annotated-kan</id><content type="html" xml:base="https://alexzhang13.github.io/blog/2024/annotated-kan/"><![CDATA[<p>This post is analogous to and heavily inspired by the <a href="https://nlp.seas.harvard.edu/annotated-transformer/">Annotated Transformer</a> but for KANs. It is fully functional as a standalone notebook, and provides intuition along with the code. Most of the code was written to be easy to follow and to mimic the structure of a standard deep learning model in PyTorch, but some parts like training loops and visualization code were adapted from the <a href="https://github.com/KindXiaoming/pykan">original codebase</a>. We decided to remove some sections from the original paper that were deemed unimportant, and also includes some extra works to motivate future research on these models.</p>
<p>The original paper is titled <a href="https://arxiv.org/abs/2404.19756">“KAN: Kolmogorov-Arnold Networks”</a> <d-cite key="liu2024kankolmogorovarnoldnetworks"></d-cite>, and the authors on this paper are: <strong>Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljačić, Thomas Y. Hou, and Max Tegmark.</strong></p>
<h2 id="introduction">Introduction</h2>
<p>Deep neural networks have been the driving force of developments in AI in the last decade. However, they currently suffer from several known issues such as a lack of interpretability, scaling issues, and data inefficiency – in other words, while they are powerful, they are not a perfect solution.</p>
<figure>
<img src="/assets/img/kan2024.jpg" width="400" alt="KAN Teaser Figure" />
<figcaption><center>Teaser figure taken from the original <a href="">KAN paper.</a> <d-cite key="liu2024kankolmogorovarnoldnetworks"></d-cite></center> </figcaption>
</figure>
<p>Kolmogorov-Arnold Networks (KANs) are an alternative representation to standard multi-layer perceptrons (MLPs). In short, they parameterize activation functions by re-wiring the “multiplication” in an MLP’s weight matrix-vector multiplication into function application. While KANs are not nearly as provably accomplished as MLPs, they are an exciting prospect for the field of AI and deserve some time for exploration.</p>
<p>I have separated this article into two sections. Parts I &amp; II describe a minimal KAN architecture and training loop without an emphasis on B-spline optimizations. You can use <a href="https://github.com/alexzhang13/Annotated-KAN/blob/main/notebooks/MinimalKAN.ipynb">the minimal KAN notebook</a> if you’re interested in KANs at a high-level. Parts III &amp; IV describe B-spline specific optimizations and an application of KANs, which includes a bit of extra machinery in the KAN code. You can use the <a href="https://github.com/alexzhang13/Annotated-KAN/blob/main/notebooks/AnnotatedKAN.ipynb">full KAN notebook</a> if you want to follow along there.</p>
<h2 id="background-and-motivation">Background and Motivation</h2>
<p>Before jumping into the implementation details, it is important to take a step back and understand why one should even care about these models. It is quite well known that Multi-layer Perceptrons (MLPs) have the “Universal Approximation Theorem”<d-cite key="Cybenko1989"></d-cite>, which provides a theoretical guarantee for the <strong>existence</strong> of an MLP that can approximate any function<d-footnote>This is a very strong guarantee that usually isn't actually true. Generally, we have some provable guarantee for a class of functions that we actually care about approximating, like say the set of functions in L1 or the set of smooth, continuous functions. </d-footnote> up to some error \(\epsilon\). While this guarantee is important, in practice, it says nothing about how difficult it is to find such an MLP through, say, optimization with stochastic gradient descent.</p>
<p>KANs admit a similar guarantee through the Kolmogorov-Arnold representation theorem, though with a caveat<d-footnote>See section [Are Stacked KAN Layers a Universal Approximator?]</d-footnote>. Formally, the theorem states that for a set of covariates \((x_1,x_2,...,x_n)\), we can write any <em>continuous, smooth</em><d-footnote>Smooth in this context means in $C^{\infty}$, or infinitely differentiable.</d-footnote> function \(f(x_1,...,x_n) : \mathcal{D} \rightarrow \mathbb{R}\) over a bounded domain \(\mathcal{D}\)<d-footnote>Because it is bounded, the authors argue that we can normalize the input to the space $[0,1]^{n}$, which is what is assumed in the original paper.</d-footnote> in the form</p>
\[f(x_1,...,x_n) = \sum_{q=0}^{2n} \Phi_{q} \left( \sum_{p=1}^{n} \Phi_{q,p} (x_p) \right)\]
<p>where \(\Phi_{q,p}, \Phi_{q}\) are univariate functions from $\mathbb{R}$ to $\mathbb{R}$. In theory, we can parameterize and learn these (potentially non-smooth and highly irregular) univariate functions \(\Phi_{q,p}, \Phi_{q}\) by optimizing a loss function similar to any other deep learning model. But it’s not that obvious how one would “parameterize” a function the same way you would parameterize a weight matrix. For now, just assume that it is possible to parameterize these functions – the original authors choose to use a B-spline, but there is little reason to be stuck on this choice.</p>
<h3 id="what-is-a-kan">What is a KAN?</h3>
<p>The expression from the theorem above does not describe a KAN with $L$ layers. This was an initial point of confusion for me. The universal approximation guarantee is only for models specifically in the form of the Kolmogorov-Arnold representation, but currently we have no notion of a “layer” or anything scalable. In fact, the number of parameters in the above theorem is a function of the number of covariates and not the choice of the engineer! Instead, the authors define a KAN layer $\mathcal{K}_{m,n}$ with input dimension $n$ and output dimension $m$ as a parameterized matrix of univariate functions, \(\Phi = \{\Phi_{i,j}\}_{i \in [m], j \in [n]}\).</p>
\[\mathcal{K}_{m,n} (\boldsymbol{x}) = \Phi \boldsymbol{x} \quad \quad \text{ where } \quad \quad \forall i \in [m], (\Phi \boldsymbol{x})_{i} = \sum_{j=1}^n \Phi_{i,j} (x_j)\]
<p>It may seem like the authors pulled this expression out of nowhere, but it is easy to see that the KAN representation theorem can be re-written as follows. For a set of covariates \(\boldsymbol{x} = (x_1,x_2,...,x_n)\), we can write any <em>continuous, smooth</em> function \(f(x_1,...,x_n) : \mathcal{D} \rightarrow \mathbb{R}\) over a bounded domain \(\mathcal{D}\) in the form</p>
\[f(x_1,...,x_n) = \mathcal{K}_{1,{2n+1}} \mathcal{K}_{2n+1, n} (x_1,...,x_n)\]
<p>The KAN architecture, is therefore written as a composition of stacking these KAN layers, similar to how you would compose an MLP. I want to emphasize that unless the KAN is written in the form above, there is currently no <em>proven</em><d-footnote>I suspect that there are some provable guarantees that can be made for deep KANs. The original universal approximation theorem for MLPs refers to models with a single hidden dimension, but later works have also derived guarantees for deep MLPs. We also technically don't have very strong provable guarantees for mechanisms like self-attention (not to my knowledge at least), so I don't think it's that important in predicting the usefulness of KANs.</d-footnote> theoretical guarantee that there exists a KAN represents that approximates the desired function.</p>
<h3 id="are-stacked-kan-layers-a-universal-approximator">Are Stacked KAN Layers a Universal Approximator?</h3>
<p>When first hearing about KANs, I was under the impression that the Kolmogorov-Arnold Representation Theorem was an analogous guarantee for KANs, but this is seemingly <em>not true</em>. Recall from the <a href="#background-and-motivation">Kolmogorov-Arnold representation theorem</a> that our guarantee is only for specific 2-layer KAN models. Instead, the authors prove that there exists a KAN using B-splines as the univariate functions \(\{\Phi_{i,j}\}_{i \in [m], j \in [n]}\) that can approximate a composition of continuously-differentiable functions within some <em>nice</em> error margin<d-footnote>This article serves mainly as a concept to code guide, so I didn't want to dive too much into theory. The error bound that the authors prove is quite strange, as the constant $C$ is not *really* a constant in the traditional sense (it depends on the function you are approximating). Also, the function family they choose to approximate seems pretty general, but I'm actually not that sure what types of functions it cannot represent well. I'd recommend reading Theorem 2.1 on your own, but it mainly serves as justification for the paper's use of B-splines rather than a universal approximation theorem for generic KAN networks. </d-footnote>. Their primary guarantees are proven to justify the use of B-splines as their learnable activations, but other works have recently sprung up that propose different learnable activations like Chebyshev polynomials<d-cite key="ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks"></d-cite>, RBFs <d-cite key="ta2024bsrbfkancombinationbsplinesradial"></d-cite>, and wavelet functions <d-cite key="bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks"></d-cite>.</p>
<p><em>tldr; no, we have not shown that a generic KAN model serves as the same type of universal approximator as an MLP (yet).</em></p>
<h3 id="polynomials-splines-and-b-splines">Polynomials, Splines, and B-Splines</h3>
<p>We talked quite extensively about “learnable activation functions”, but this notion might be unclear to some readers. In order to parameterize a function, we have to define some kind of “base” function that uses coefficients. When learning the function, we are actually learning the coefficients. The original Kolmogorov-Arnold representation theorem places no conditions on the family of learnable univariate activation functions. Ideally, we would want some kind of parameterized family of functions that can approximate any function, whether it be non-smooth, fractal, or some other kind of nasty property <em>on a bounded domain</em><d-footnote>Not only is the original KAN representation theorem over a bounded domain, but generally in most practical applications we are not dealing with data over an unbounded domain.</d-footnote>.</p>
<p><strong>Enter the B-spline</strong>. B-splines are a generalization of spline functions, which themselves are piecewise polynomials. Polynomials of degree/order $k$ are written as $p(x) = a_0 + a_1x + a_2x^2 + … + a_kx^k$ and can be parameterized according to their coefficients $a_0,a_1,…,a_k$. From the Stone-Weierstrass theorem <d-cite key="weierstrassZz"></d-cite>, we can guarantee that every continuous function over a bounded domain can be approximated by a polynomial. Splines, and by extension B-splines, extend this guarantee to more complicated functions over a bounded domain. I don’t want to take away from the focus on KANs, so for more background I’d recommend reading <a href="https://rohangautam.github.io/blog/b_spline_intro/">this resource</a><d-cite key="rohan2024bspline"></d-cite>.</p>
<p>Rather than be chunked explicitly like a spline, B-spline functions are written as a sum of basis functions of the form</p>
\[B(x) \triangleq \sum_{i=1}^{G} c_i B_{i,k}(x).\]
<p>where $G$ denotes the number of grid points and therefore basis functions (which we have not defined yet), $k$ is the order of the B-spline, and $c_i$ are learnable parameters. Like a spline, a B-spline has a set of $G$ grid points<d-footnote>These are also called knots. B-splines are determined by control points, which are the data points we're trying to fit. Sometimes knots and control points can be the same, but generally knots are fixed beforehand and can be adjusted.</d-footnote> $(t_1,t_2,…,t_G)$. In the KAN paper, they augment these points to $(t_{-k}, t_{-k+1},…,t_{G+k-1},t_{G+k})$ to account for the order of the B-spline<d-footnote>Read https://web.mit.edu/hyperbook/Patrikalakis-Maekawa-Cho/node17.html for a better explanation for why you need to do this. It is mainly so the basis functions are well defined.</d-footnote> to give us an augmented grid size of $G+2k$. The simplest definition for the grid points is to uniformly divide the bounded domain into $G$ equally spaced points – from our definition of the basis functions, you will see that the augmented points just need to be at the ends. The Cox-de Boor formula characterizes these basis functions recursively as follows:</p>
\[\begin{aligned}
B_{i,\color{red}{0}}(x) &amp;\triangleq \mathbf{1}_{\{x \geq t_i\}} * \mathbf{1}_{\{x &lt; t_{i+1}\}}
\\
B_{i,\color{red}{j}}(x) &amp;\triangleq \frac{x - t_i}{t_{i+j} - t_i} B_{i,\color{red}{j-1}}(x) + \frac{t_{i+j+1} - x}{t_{i+j+1} - t_{i+1}} B_{i+1,\color{red}{j-1}}(x)
\end{aligned}\]
<p>We can plot an example for the basis functions of a B-spline with $G=5$ grid points of order $k=3$. In other words, the augmented grid size is $G+2k=11$:</p>
<figure>
<img src="/assets/img/B-spline.png" width="500" alt="B-spline Basis Functions" />
<figcaption> <center>Matplotlib plot of B-spline basis functions. Notably, the basis functions, like spline polynomials, are $0$ on most of the domain. But they overlap, unlike for splines. I generated this graph by adapting code from <a href="https://github.com/johntfoster/bspline/">https://github.com/johntfoster/bspline/</a>.</center> </figcaption>
</figure>
<p>When implementing B-splines for our KAN, we are not interested in the function $f(\cdot)$ itself, rather we care about efficiently computing the function evaluated at a point $f(x)$. We will later see a nice iterative bottom-up dynamic programming formulation of the Cox-de Boor recursion – I’ve colored the indices red to make it the DP relation clear.</p>
<h2 id="part-i-the-minimal-kan-model-architecture">Part I: The Minimal KAN Model Architecture</h2>
<p>In this section, we describe a barebones, minimal KAN model. The goal is to show that the architecture is structured quite similarly to deep learning code that the reader has most likely seen in the past. To summarize the components, we modularize our code into (1) a high-level KAN module, (2) the KAN layer, (3) the parameter initialization scheme, and (4) the plotting function for interpreting the model activations.</p>
<h3 id="preliminaries">Preliminaries</h3>
<p>If you’re using Colab, you can run the following as if they were code blocks. This implementation is also quite GPU-unfriendly, so a CPU will suffice.</p>
<d-code block="" language="python" style="font-size:0.7em">
# Code was written in Python 3.11.9, but most usable versions of Python and torch suffice.
!pip install torch==2.3.1
!pip install numpy==1.26.4
!pip install matplotlib==3.9.0
!pip install tqdm==4.66.4
!pip install torchvision==0.18.1
</d-code>
<p>In an attempt to make this code barebones, I’ve tried to use as little dependencies as possible. I’ve also included type annotations for the code.</p>
<d-code block="" language="python" style="font-size:0.7em">
# Python libraries
import os
from typing import List, Dict, Optional, Self
import random
import warnings
# Installed libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
</d-code>
<p>The following config file holds some preset hyperparameters described in the paper. Most of these can be changed and may not even apply to a more generic KAN architecture.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KANConfig:
"""
Configuration struct to define a standard KAN.
"""
residual_std = 0.1
grid_size = 5
spline_order = 3
grid_range = [-1.0, 1.0]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
</d-code>
<h3 id="the-kan-architecture-skeleton">The KAN Architecture Skeleton</h3>
<p>If you understand how MLPs work, then the following architecture should look familiar. As always, given some set of input features \((x_1,...,x_n)\) and a desired output \((y_1,...,y_m)\), we can think of our KAN as a function $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m} $ parameterized by weights $\theta$. Like any other deep learning model, we can decompose KANs in a layer-wise fashion and offload the computational details to the layer class. We will fully describe our model in terms of a list of integers <code class="language-plaintext highlighter-rouge">layer_widths</code>, where the first number denotes the input dimension $n$, and the last number denotes the output dimension $m$.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KAN(nn.Module):
"""
Standard architecture for Kolmogorov-Arnold Networks described in the original paper.
Layers are defined via a list of layer widths.
This minimal implementation doesn't include optimizations used specifically
for B-splines.
"""
def __init__(
self,
layer_widths: List[int],
config: KANConfig,
):
super(KAN, self).__init__()
self.layers = torch.nn.ModuleList()
self.layer_widths = layer_widths
# If layer_widths is [2,4,5,1], the layer
# inputs are [2,4,5] and the outputs are [4,5,1]
in_widths = layer_widths[:-1]
out_widths = layer_widths[1:]
for in_dim, out_dim in zip(in_widths, out_widths):
self.layers.append(
KANLayer(
in_dim=in_dim,
out_dim=out_dim,
grid_size=config.grid_size,
spline_order=config.spline_order,
device=config.device,
residual_std=config.residual_std,
grid_range=config.grid_range,
)
)
def forward(self, x: torch.Tensor):
"""
Standard forward pass sequentially across each layer.
"""
for layer in self.layers:
x = layer(x)
return x
</d-code>
<h3 id="the-kan-representation-layer">The KAN Representation Layer</h3>
<p>The representation used at each layer is quite intuitive. For an input \(x \in \mathbb{R}^{n}\), we can directly compare a standard MLP layer with output dimension $m$ to an equivalent KAN layer:</p>
\[\begin{aligned}
h_{MLP} = \sigma (W \boldsymbol{x} + b) \quad \quad &amp;\text{ where } \quad \quad \forall i \in [m], (W\boldsymbol{x})_{i} = \sum_{k=1}^n W_{i,k} x_k
\\
h_{KAN} = \Phi \boldsymbol{x} + b \quad \quad &amp;\text{ where } \quad \quad \forall i \in [m], (\Phi \boldsymbol{x})_{i} = \sum_{k=1}^n \Phi_{i,k} (x_k)
\end{aligned}\]
<p>In other words, both layers can be written in terms of a generalized matrix-vector operation, where for an MLP it is scalar multiplication, while for a KAN it is some <em>learnable</em> non-linear function \(\Phi_{i,k}\). Interestingly, both layers look extremely similar! <d-footnote>Remark. As a GPU enthusiast, I should mention that while these two expressions look quite similar, this minor difference can have a huge impact on efficiency. Having the same instruction (e.g. multiplication) applied to every operation fits well within the warp abstraction used in writing CUDA kernels, while having a different function application per operation has many issues like control divergence that significantly slow down performance.</d-footnote></p>
<p>Let’s think through how we would perform this computation. For our analysis, we will ignore the batch dimension, as generally this is an easy extension. Suppose we have a KAN layer $\mathcal{K}_{m,n}$ with input dimension $n$ and output dimension $m$. As we discussed earlier, for input $(x_1,x_2,…,x_n)$,</p>
\[\mathcal{K}_{m,n}(x_1,x_2,...,x_n) \triangleq \left(\sum_{k=1}^n \Phi_{1,k} (x_k), \sum_{k=1}^n \Phi_{2,k} (x_k),...,\sum_{k=1}^n \Phi_{m,k} (x_k) \right)\]
<p>In matrix form, this is can be nicely written as</p>
\[\begin{bmatrix}
\Phi_{1,1} (\cdot) &amp; \Phi_{1,2} (\cdot) &amp; ... &amp; \Phi_{1,n} (\cdot)\\
\Phi_{2,1} (\cdot) &amp; \Phi_{2,2} (\cdot) &amp; ... &amp; \Phi_{2,n} (\cdot) \\
\vdots &amp; \vdots &amp; ... &amp; \vdots \\
\Phi_{m,1} (\cdot) &amp; \Phi_{m,2} (\cdot) &amp; ... &amp; \Phi_{m,n} (\cdot) \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
\Phi_{1,1}(x_1) + \Phi_{1,2}(x_2) + ... +  \Phi_{1,n}(x_n) \\
\Phi_{2,1}(x_1) + \Phi_{2,2}(x_2) + ... +  \Phi_{2,n}(x_n) \\
\vdots \\
\Phi_{m,1}(x_1) + \Phi_{m,2}(x_2) + ... +  \Phi_{m,n}(x_n) \\
\end{bmatrix}\]
<p>The observant reader may notice that this looks exactly like the $Wx$ matrix used in an MLP. In other words, we have to compute and materialize<d-footnote>For convenience sake, we will materialize the matrix of values below all at once. I suspect that, similar to matrix multiplication, there may be a way to avoid materializing the full matrix all at once, but this requires a clever choice of the family of functions for $\Phi$.</d-footnote> each term in the matrix below, then sum along the rows.</p>
\[\text{The terms we need to compute are }
\begin{bmatrix}
\Phi_{1,1}(x_1), \Phi_{1,2}(x_2), ...,  \Phi_{1,n}(x_n) \\
\Phi_{2,1}(x_1), \Phi_{2,2}(x_2), ...,\Phi_{2,n}(x_n) \\
\vdots \\
\Phi_{m,1}(x_1), \Phi_{m,2}(x_2), ...,  \Phi_{m,n}(x_n) \\
\end{bmatrix}\]
<p>To finish off the abstract KAN layer (remember, we haven’t defined what the learnable activation function is), the authors define each learnable activation function $\Phi_{i,j}(\cdot)$ as a function of a learnable activation function $s_{i,j}(\cdot)$ to add residual connections in the network:</p>
\[\begin{aligned}
\Phi_{i,j}(x) &amp;\triangleq w^{(b)}_{i,j} \cdot \text{SiLU}(x) + w^{(s)}_{i,j} \cdot s_{i,j}(x) \quad \quad \forall i \in [m], j \in [n] \\
\text{SiLU}(x) &amp;\triangleq \frac{x}{1 + e^{-x}}
\end{aligned}\]
<p>We can modularize the operation above into a “weighted residual layer” that acts over a matrix of \((\text{out_dim}, \text{in_dim})\) values. This layer is parameterized by each \(w^{(b)}_{i,j}\) and \(w^{(s)}_{i,j}\), so we can store \(\boldsymbol{w}^{(b)}\) and \(\boldsymbol{w}^{(s)}\) as parameterized weight matrices. The paper also specifies the initialization scheme of \(w^{(b)}_{i,j} \sim \mathcal{N}(0, 0.1)\) and \(w^{(s)}_{i,j} = 1\).<d-footnote>For all the code comments below, I notate `bsz` as the batch size. Generally, this is just an extra dimension that can be ignored during the analysis.</d-footnote></p>
<d-code block="" language="python" style="font-size:0.7em">
class WeightedResidualLayer(nn.Module):
"""
Defines the activation function used in the paper,
phi(x) = w_b SiLU(x) + w_s B_spline(x)
as a layer.
"""
def __init__(
self,
in_dim: int,
out_dim: int,
residual_std: float = 0.1,
):
super(WeightedResidualLayer, self).__init__()
self.univariate_weight = torch.nn.Parameter(
torch.Tensor(out_dim, in_dim)
)  # w_s in paper
# Residual activation functions
self.residual_fn = F.silu
self.residual_weight = torch.nn.Parameter(
torch.Tensor(out_dim, in_dim)
)  # w_b in paper
self._initialization(residual_std)
def _initialization(self, residual_std):
"""
Initialize each parameter according to the original paper.
"""
nn.init.normal_(self.residual_weight, mean=0.0, std=residual_std)
nn.init.ones_(self.univariate_weight)
def forward(self, x: torch.Tensor, post_acts: torch.Tensor):
"""
Given the input to a KAN layer and the activation (e.g. spline(x)),
compute a weighted residual.
x has shape (bsz, in_dim) and act has shape (bsz, out_dim, in_dim)
"""
# Broadcast the input along out_dim of post_acts
res = self.residual_weight * self.residual_fn(x[:, None, :])
act = self.univariate_weight * post_acts
return res + act
</d-code>
<p>With these operations laid out in math, we have enough information to write a basic KAN layer by abstracting away the choice of learnable activation \(s_{i,j}(\cdot)\). Note that in the code below, the variables <code class="language-plaintext highlighter-rouge">spline_order</code>, <code class="language-plaintext highlighter-rouge">grid_size</code>, and <code class="language-plaintext highlighter-rouge">grid_range</code> are specific to B-splines as the activation, and are only passed through the constructor. You can ignore them for now. In summary, we will first compute the matrix</p>
\[\begin{bmatrix}
s_{1,1}(x_1), s_{1,2}(x_2), ...,  s_{1,n}(x_n) \\
s_{2,1}(x_1), s_{2,2}(x_2), ...,s_{2,n}(x_n) \\
\vdots \\
s_{m,1}(x_1), s_{m,2}(x_2), ...,  s_{m,n}(x_n) \\
\end{bmatrix}\]
<p>following by the weighted residual across each entry, then we will finally sum along the rows to get our layer output. We also define a <code class="language-plaintext highlighter-rouge">cache()</code> function to store the input vector \(\boldsymbol{x}\) and the \(\Phi \boldsymbol{x}\) matrix to compute regularization terms defined later.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KANLayer(nn.Module):
"Defines a KAN layer from in_dim variables to out_dim variables."
def __init__(
self,
in_dim: int,
out_dim: int,
grid_size: int, # B-spline parameter
spline_order: int, # B-spline parameter
device: torch.device,
residual_std: float = 0.1,
grid_range: List[float] = [-1, 1], # B-spline parameter
):
super(KANLayer, self).__init__()
self.in_dim = in_dim
self.out_dim = out_dim
self.grid_size = grid_size
self.spline_order = spline_order
self.device = device
# Define univariate function (splines in original KAN)
self.activation_fn = KANActivation(
in_dim,
out_dim,
spline_order,
grid_size,
device,
grid_range,
)
# Define the residual connection layer used to compute \phi
self.residual_layer = WeightedResidualLayer(in_dim, out_dim, residual_std)
# Cache for regularization
self.inp = torch.empty(0)
self.activations = torch.empty(0)
def cache(self, inp: torch.Tensor, acts: torch.Tensor):
self.inp = inp
self.activations = acts
def forward(self, x: torch.Tensor):
"""
Forward pass of KAN. x is expected to be of shape (bsz, in_dim) where in_dim
is the number of input scalars and the output is of shape (bsz, out_dim).
"""
# Compute each s_{i,j}, shape: [bsz x out_dim x in_dim]
spline = self.activation_fn(x)
# Form the batch of matrices phi(x) of shape [bsz x out_dim x in_dim]
phi = self.residual_layer(x, spline)
# Cache activations for regularization during training.
self.cache(x, phi)
# Really inefficient matmul
out = torch.sum(phi, dim=-1)
return out
</d-code>
<h3 id="kan-learnable-activations-b-splines">KAN Learnable Activations: B-Splines</h3>
<p>Recall from the <a href="#polynomials-splines-and-b-splines">section on B-splines</a> that each activation $s_{i,j}(\cdot)$ is a sum of products<d-footnote>We can equivalently think of this as a dot product between two vectors $\langle c_{i,j}, B_{i,j} (x_j) \rangle$.</d-footnote> of $G + k$ learnable coefficients and basis functions \(\sum_{h=1}^{G} c^{h}_{i,j}, B^h_{i,j} (x_j)\) where $G$ is the grid size. The recursive definition of the B-spline basis functions requires us to define the grid points $(t_1,t_2,…,t_G)$, as well as the augmented grid points \((t_{-k},t_{-k+1},...,t_{-1},t_{G+1},....,t_{G+k})\)<d-footnote>In the original paper, you may have noticed a G + k - 1 term. I don't define $t_0$ here, and opt to not include it for indexing sake, but you can basically just shift everything by $1$ to achieve the same effect.</d-footnote>. For now, we will define them to be the endpoints of $G+1$ equally-sized intervals on the bounded interval <code class="language-plaintext highlighter-rouge">[low_bound, up_bound]</code><d-footnote>I mentioned this earlier, but you may notice that the augmented grid points go out of the bounded domain. This is just for convenience, but as long as they are at the bounds or outside them in the right direction, it doesn't matter what they are. You can also just set them to be the boundary points.</d-footnote> but you can also choose / learn the grid point positions. Finally, we note that we need to use the grid points in the calculation of each activation $s_{i,j}(x)$, so we broadcast into a 3D tensor.</p>
<d-code block="" language="python" style="font-size:0.7em">
def generate_control_points(
low_bound: float,
up_bound: float,
in_dim: int,
out_dim: int,
spline_order: int,
grid_size: int,
device: torch.device,
):
"""
Generate a vector of {grid_size} equally spaced points in the interval
[low_bound, up_bound] and broadcast (out_dim, in_dim) copies.
To account for B-splines of order k, using the same spacing, generate an additional
k points on each side of the interval. See 2.4 in original paper for details.
"""
# vector of size [grid_size + 2 * spline_order + 1]
spacing = (up_bound - low_bound) / grid_size
grid = torch.arange(-spline_order, grid_size + spline_order + 1, device=device)
grid = grid * spacing + low_bound
# [out_dim, in_dim, G + 2k + 1]
grid = grid[None, None, ...].expand(out_dim, in_dim, -1).contiguous()
return grid
</d-code>
<p>Again recall the <a href="#polynomials-splines-and-b-splines">Cox-de Boor recurrence from before</a>.
As a general rule of thumb we would like to avoid writing recurrent functions in the forward pass of a model. A common trick is to turn our recurrence into a dynamic-programming solution, which we make clear by writing in array notation:</p>
\[\begin{aligned}
B_x[i][0] &amp;\triangleq [x \geq t[i]] * [x &lt; t[i+1]]
\\
B_{x}[i][\color{red}{j} \color{black}] &amp;\triangleq \frac{x - t[i]}{t[i+j] - t[i]} B_{x}[i][\color{red}{j-1} \color{black}] + \frac{t[i+j+1] - x}{t[i+j+1] - t[i+1]} B_{x}[i+1][\color{red}{j-1} \color{black}]
\end{aligned}\]
<h3 id="computing-the-b-spline-basis-functions">Computing the B-Spline Basis Functions</h3>
<p><em>The tricky part is writing this in tensor notation</em><d-footnote>I'd recommend drawing this out yourself. It's quite hard to explain without visualizations, but quite simple to reason about. </d-footnote>. We take advantage of broadcasting rules in PyTorch/Numpy to make copies of tensors when needed. Recall that to materialize our activation matrix \(\{s_{i,j}(x_j)\}_{i \in [m], j \in [n]}\) we need to compute the bases for each activation, i.e. \(\{B^{(i,j)}_{h,k} (x_j)\}_{h \in [G+k], i \in [m], j \in [n]}\).</p>
<p>The following explanation is a bit verbose, so bear with me. Our grid initialization function above generates a rank-3 tensor of shape <code class="language-plaintext highlighter-rouge">(out_dim, in_dim, G+2k+1)</code> while the input $x$ is a rank-2 tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, in_dim)</code>. We first notice that our grid applies to every input in the batch, so we broadcast it to a rank-4 tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, out_dim, in_dim, G+2k+1)</code>.  For the input $x$, we similarly need a copy for every output dimension and every basis function to evaluate over, giving us the same shape through broadcasting. We can align the <code class="language-plaintext highlighter-rouge">in_dim</code> axis of both the grid and the input because $j$ aligns in $s_{i,j}(x_j)$. The $i$ indexes over the basis functions, or the last dimension of our tensors. We write out the vectorized DP in this form, as we note that we can fix $j$. Finally, we perform DP over our $j$ index based on the recurrence rule, yielding the B-spline basis functions evaluated on each input dimension to be used for each output dimension. This notation may be confusing, but the operation is actually quite simple – I would recommend ignoring the batch dimension and drawing out what you need to do.</p>
<p><em>tldr; we need to compute something for each element in a batch, for each activation, for each B-spline basis. we can use broadcasting to do this concisely, from the code below</em></p>
<d-code block="" language="python" style="font-size:0.7em">
# Helper functions for computing B splines over a grid
def compute_bspline(x: torch.Tensor, grid: torch.Tensor, k: int, device: torch.device):
"""
For a given grid with G_1 intervals and spline order k, we *recursively* compute
and evaluate each B_n(x_{ij}). x is a (batch_size, in_dim) and grid is a
(out_dim, in_dim, # grid points + 2k + 1)
Returns a (batch_size, out_dim, in_dim, grid_size + k) intermediate tensor to
compute sum_i {c_i B_i(x)} with.
"""
grid = grid[None, :, :, :].to(device)
x = x[:, None, :, None].to(device)
# Base case: B_{i,0}(x) = 1 if (grid_i &lt;= x &lt;= grid_{i+k}) 0 otherwise
bases = (x &gt;= grid[:, :, :, :-1]) * (x &lt; grid[:, :, :, 1:])
# Recurse over spline order j, vectorize over basis function i
for j in range (1, k + 1):
n = grid.size(-1) - (j + 1)
b1 = ((x[:, :, :, :] - grid[:, :, :, :n]) / (grid[:, :, :, j:-1] - grid[:, :, :, :n]))
b1 = b1 * bases[:, :, :, :-1]
b2 = ((grid[:, :, :, j+1:] - x[:, :, :, :])  / (grid[:, :, :, j+1:] - grid[:, :, :, 1:n+1]))
b2 = b2 * bases[:, :, :, 1:]
bases = b1 + b2
return bases
</d-code>
<h3 id="computing-the-b-spline-activations">Computing the B-Spline Activations</h3>
<p>With the B-spline logic out of the way, we have all of our intermediate computation logic done. We still have to define our parameters $c_i$ and compute the B-splines from the basis functions, but this is just a simple element-wise multiplication and sum. We can now pass the B-spline output into the weighted residual layer defined earlier and compute our output vector. In summary, we are computing</p>
\[\begin{aligned}
s_{i,j}(x) &amp;\triangleq \sum_{h=1}^{G+k} c_h B^{(i,j)}_{h,k}(x_j) \\
\Phi_{i,j}(x) &amp;\triangleq w^{(b)}_{i,j} \cdot \text{SiLU}(x) + w^{(s)}_{i,j} \cdot s_{i,j}(x) \quad \quad \forall i \in [m], j \in [n]
\end{aligned}\]
<d-code block="" language="python" style="font-size:0.7em">
class KANActivation(nn.Module):
"""
Defines a KAN Activation layer that computes the spline(x) logic
described in the original paper.
"""
def __init__(
self,
in_dim: int,
out_dim: int,
spline_order: int,
grid_size: int,
device: torch.device,
grid_range: List[float],
):
super(KANActivation, self).__init__()
self.in_dim = in_dim
self.out_dim = out_dim
self.spline_order = spline_order
self.grid_size = grid_size
self.device = device
self.grid_range = grid_range
# Generate (out, in) copies of equally spaced control points on [a, b]
grid = generate_control_points(
grid_range[0],
grid_range[1],
in_dim,
out_dim,
spline_order,
grid_size,
device,
)
self.register_buffer("grid", grid)
# Define the univariate B-spline function
self.univarate_fn = compute_bspline
# Spline parameters
self.coef = torch.nn.Parameter(
torch.Tensor(out_dim, in_dim, grid_size + spline_order)
)
self._initialization()
def _initialization(self):
"""
Initialize each parameter according to the original paper.
"""
nn.init.xavier_normal_(self.coef)
def forward(self, x: torch.Tensor):
"""
Compute and evaluate the learnable activation functions
applied to a batch of inputs of size in_dim each.
"""
# [bsz x in_dim] to [bsz x out_dim x in_dim x (grid_size + spline_order)]
bases = self.univarate_fn(x, self.grid, self.spline_order, self.device)
# [bsz x out_dim x in_dim x (grid_size + spline_order)]
postacts = bases * self.coef[None, ...]
# [bsz x out_dim x in_dim] to [bsz x out_dim]
spline = torch.sum(postacts, dim=-1)
return spline
</d-code>
<p>If you’ve gotten to this point, congratulations! You’ve read through the hardest and most important part of this article. The rest of this post talks about a generic model training loop, visualization functions, and optimizations that can be made to B-spline specific KANs. If you’re interested in future directions for these models, I’d recommend reading into <a href="https://github.com/mintisan/awesome-kan">Awesome-KAN</a> and getting started! Otherwise, if you’d like to have a deeper understanding of the original KAN paper, keep reading!</p>
<h3 id="sparsity-through-regularization">Sparsity through Regularization</h3>
<p>Rather unsurprisingly, regularization is an important component of KANs. The authors of KAN motivate two types of regularization – L1 regularization to limit the number of active activation functions, and entropy regularization to penalize duplicate activation functions.</p>
<p>L1 regularization for a weight matrix \(W\) in an MLP is straightforward – just take the Frobenius norm of the matrix. However, for activation functions, using the parameters of the function are not necessarily a good choice. Instead, the magnitude of the <strong>function evaluated on the data</strong> is used. More formally, suppose we have a batch of inputs \(\{x^{(b)}_1,...,x^{(b)}_n \}_{b \in \mathcal{B}}\) into a KAN layer $\mathcal{K}_{m,n}$. The L1 norm of an activation from input node $j$ to output node $i$ is defined as the absolute value of the mean of that activation on $x_j$, averaged over the batch. In other words,</p>
\[\|\Phi_{i,j}\|_1 \triangleq \left| \frac{1}{|\mathcal{B}|} \sum_{b=1}^{|\mathcal{B}|} \Phi_{i,j}(x^{(b)}_j) \right|\]
<p>The L1 norm of the layer is then defined as</p>
\[\|\Phi\|_1 \triangleq  \sum_{j=1}^{n} \sum_{i=1}^{m} \| \Phi_{i,j} \|_1\]
<d-code block="" language="python" style="font-size:0.7em">
def l1_regularization(model: KAN):
"""
Compute L1 regularization of activations by using
cached activations. Must be called after KAN forward pass
during training.
"""
reg = torch.tensor(0.)
# regularize coefficient to encourage spline to be zero
for i in range(len(model.layers)):
acts = model.layers[i].activations
l1_activations = torch.sum(torch.mean(torch.abs(acts), dim=0))
reg += l1_activations
return reg
</d-code>
<p>In addition to wanting sparse activations for better interpretability and performance<d-footnote>In our implementation, sparsification does not yield performance benefits because we do not take advantage of any kind of efficient sparse kernels, at least not explicitly. While this post is mainly designed to be readable, an efficient implementation of KANs is very important for attempts to scale these models.</d-footnote>, we generally want to ensure we do not have duplicate activation functions. Another form of regularization is naturally entropy, which is defined as</p>
\[S(\boldsymbol{\Phi}) \triangleq -\sum_{j=1}^{n} \sum_{i=1}^{m} \frac{\|\Phi_{i,j}\|_1}{\|\Phi\|_1} \log \left( \frac{\|\Phi_{i,j}\|_1}{\|\Phi\|_1} \right)\]
<d-code block="" language="python" style="font-size:0.7em">
def entropy_regularization(model: KAN):
"""
Compute entropy regularization of activations by using
cached activations. Must be called after KAN forward pass
during training.
"""
reg = torch.tensor(0.)
eps = 1e-4
# regularize coefficient to encourage spline to be zero
for i in range(len(model.layers)):
acts = model.layers[i].activations
l1_activations = torch.sum(torch.mean(torch.abs(acts), dim=0))
activations = (
torch.mean(torch.abs(l1_activations), dim=0)
/ l1_activations
)
entropy = -torch.sum(activations * torch.log(activations + eps))
reg += entropy
return reg
</d-code>
<p>The regularization term is just a weighted sum of the two terms above. These regularization expressions are not specific to the B-splines representation chosen by the authors, but their effect on other choices of learnable activation functions is underexplored at the moment.</p>
<d-code block="" language="python" style="font-size:0.7em">
def regularization(
model: KAN,
l1_factor: float = 1,
entropy_factor: float = 1,
):
"""
Regularization described in the original KAN paper. Involves an L1
and an entropy factor.
"""
return l1_factor * l1_regularization(model) + \
entropy_factor * entropy_regularization(model)
</d-code>
<h2 id="part-ii-model-training">Part II: Model Training</h2>
<p>In this section, we will discuss the basic training loop for a KAN, including a script for visualizing the network activations. As you will notice, the framework for training a KAN is almost identical to a standard deep learning train loop.</p>
<h3 id="training-loop">Training Loop</h3>
<p>Despite the extra machinery necessary to apply our model parameters to our input, it is easy to see that the operations themselves are differentiable. In other words, barring some extra optimization tricks that we will discuss in <a href="#Part III - KAN-specific Optimizations">Part III</a>, the training loop for KANs is basically just a generic deep learning train loop that takes advantage of autodifferentiation and backpropagation. We first define a function for generating training data for a function $f(x_1,…,x_n)$ over a bounded domain $\mathcal{D} \in \mathbb{R}^{d}$.</p>
<d-code block="" language="python" style="font-size:0.7em">
# Helper function derived from https://github.com/KindXiaoming/pykan/blob/master/kan/utils.py
def create_dataset(
f,
n_var: int=2,
ranges=[-1, 1],
train_num: int =1000,
test_num: int=1000,
device: torch.device = torch.device("cpu"),
seed: int=0,
):
"""
Create a synthetic dataset as a function of n_var variables
"""
def normalize(data, mean, std):
return (data - mean) / std
np.random.seed(seed)
torch.manual_seed(seed)
if len(np.array(ranges).shape) == 1:
ranges = np.array(ranges * n_var).reshape(n_var, 2)
else:
ranges = np.array(ranges)
train_input = torch.zeros(train_num, n_var)
test_input = torch.zeros(test_num, n_var)
for i in range(n_var):
train_input[:, i] = (
torch.rand(
train_num,
)
* (ranges[i, 1] - ranges[i, 0])
+ ranges[i, 0]
)
test_input[:, i] = (
torch.rand(
test_num,
)
* (ranges[i, 1] - ranges[i, 0])
+ ranges[i, 0]
)
train_label = f(train_input)
test_label = f(test_input)
mean_input = torch.mean(train_input, dim=0, keepdim=True)
std_input = torch.std(train_input, dim=0, keepdim=True)
train_input = normalize(train_input, mean_input, std_input)
test_input = normalize(test_input, mean_input, std_input)
mean_label = torch.mean(train_label, dim=0, keepdim=True)
std_label = torch.std(train_label, dim=0, keepdim=True)
train_label = normalize(train_label, mean_label, std_label)
test_label = normalize(test_label, mean_label, std_label)
dataset = {}
dataset["train_input"] = train_input.to(device)
dataset["test_input"] = test_input.to(device)
dataset["train_label"] = train_label.to(device)
dataset["test_label"] = test_label.to(device)
return dataset
</d-code>
<p>As the reader will see below, the KAN training loop is extremely simple, and uses the familiar <code class="language-plaintext highlighter-rouge">zero_grad()</code>, <code class="language-plaintext highlighter-rouge">backward</code>, <code class="language-plaintext highlighter-rouge">step()</code> PyTorch loop. We do not even use the L-BFGS<d-cite key="liu1989limited"></d-cite> optimizer specified in the original KAN paper to highlight the similarities, and opt to use the widely used Adam<d-cite key="kingma2017adammethodstochasticoptimization"></d-cite> optimizer instead. In our code, we also store and load the best validation checkpoint after training.</p>
<d-code block="" language="python" style="font-size:0.7em">
# Adapted from https://github.com/KindXiaoming/pykan
def train(
model: KAN,
dataset: Dict[str, torch.Tensor],
batch_size: int,
batch_size_test: int,
device: torch.device,
reg_lambda: float = 0.1,
steps: int = 10000,
loss_fn=None,
loss_fn_eval=None,
log: int = 20,
lr: float = 3e-5,
save_path: str ='./saved_models/',
ckpt_name: Optional[str] = 'best.pt',
):
"""
Train loop for KANs. Logs loss every {log} steps and uses
the best checkpoint as the trained model. Returns a dict of
the loss trajectory.
"""
if not os.path.exists(save_path):
os.makedirs(save_path)
pbar = tqdm(range(steps), desc="KAN Training", ncols=200)
if loss_fn is None:
loss_fn = lambda x, y: torch.mean((x - y) ** 2)
if loss_fn_eval is None:
loss_fn_eval = lambda x, y: torch.mean((x - y) ** 2)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
results = {}
results["train_loss"] = []
results["test_loss"] = []
results["regularization"] = []
results["best_test_loss"] = []
train_size = dataset["train_input"].shape[0]
test_size = dataset["test_input"].shape[0]
best_test_loss = torch.tensor(1e9)
for step in pbar:
train_id = np.random.choice(train_size, batch_size, replace=False)
test_id = np.random.choice(test_size, batch_size_test, replace=False)
x = dataset["train_input"][train_id].to(device)
y = dataset["train_label"][train_id].to(device)
x_eval = dataset["test_input"][test_id].to(device)
y_eval = dataset["test_label"][test_id].to(device)
pred = model.forward(x)
train_loss = loss_fn(pred, y)
ent_l1_reg = regularization(model)
loss = train_loss + reg_lambda * ent_l1_reg
optimizer.zero_grad()
loss.backward()
optimizer.step()
test_loss = loss_fn_eval(model.forward(x_eval), y_eval)
if best_test_loss &gt; test_loss:
best_test_loss = test_loss
if ckpt_name is not None:
torch.save(model.state_dict(), os.path.join(save_path, ckpt_name))
if step % log == 0:
pbar.set_description(
"train loss: %.2e | test loss: %.2e | reg: %.2e "
% (
train_loss.cpu().detach().numpy(),
test_loss.cpu().detach().numpy(),
ent_l1_reg.cpu().detach().numpy(),
)
)
results["train_loss"].append(train_loss.cpu().detach().numpy())
results["test_loss"].append(test_loss.cpu().detach().numpy())
results["best_test_loss"].append(best_test_loss.cpu().detach().numpy())
results["regularization"].append(ent_l1_reg.cpu().detach().numpy())
if ckpt_name is not None:
model.load_state_dict(torch.load(os.path.join(save_path, ckpt_name)))
return results
</d-code>
<p>We can also define a simple plotting function that takes the <code class="language-plaintext highlighter-rouge">results</code> dictionary from above.</p>
<d-code block="" language="python" style="font-size:0.7em">
def plot_results(results: Dict[str, List[float]]):
"""
Function for plotting the interior of a KAN, similar to the original paper.
"""
for key, value in results.items():
plt.plot(value)
plt.title(key)
plt.show()
</d-code>
<h3 id="network-visualization">Network Visualization</h3>
<p>We mostly adapt the network visualization code from the original repository. While the code is quite dense, all we need to do is plot our stored activations per layer, save the plots, then draw out the grid of network connections. You can mostly skim this code unless you’re interested in prettifying the visualizations.</p>
<d-code block="" language="python" style="font-size:0.7em">
def plot(model: KAN, folder="./figures", scale=0.5, title=None):
"""
Function for plotting KANs and visualizing their activations adapted from
https://github.com/KindXiaoming/pykan/blob/master/kan/KAN.py#L561
"""
if not os.path.exists(folder):
os.makedirs(folder)
depth = len(model.layer_widths) - 1
for l in range(depth):
w_large = 2.0
for i in range(model.layer_widths[l]):
for j in range(model.layer_widths[l + 1]):
rank = torch.argsort(model.layers[l].inp[:, i])
fig, ax = plt.subplots(figsize=(w_large, w_large))
plt.gca().patch.set_edgecolor("white")
plt.gca().patch.set_linewidth(1.5)
color = "black"
plt.plot(
model.layers[l].inp[:, i][rank].cpu().detach().numpy(),
model.layers[l].activations[:, j, i][rank].cpu().detach().numpy(),
color=color,
lw=5,
)
plt.gca().spines[:].set_color(color)
plt.savefig(
f"{folder}/sp_{l}_{i}_{j}.png", bbox_inches="tight", dpi=400
)
plt.close()
# draw skeleton
width = np.array(model.layer_widths)
A = 1
y0 = 0.4
neuron_depth = len(width)
min_spacing = A / np.maximum(np.max(width), 5)
max_num_weights = np.max(width[:-1] * width[1:])
y1 = 0.4 / np.maximum(max_num_weights, 3)
fig, ax = plt.subplots(figsize=(10 * scale, 10 * scale * (neuron_depth - 1) * y0))
# plot scatters and lines
for l in range(neuron_depth):
n = width[l]
for i in range(n):
plt.scatter(
1 / (2 * n) + i / n,
l * y0,
s=min_spacing**2 * 10000 * scale**2,
color="black",
)
if l &lt; neuron_depth - 1:
# plot connections
n_next = width[l + 1]
N = n * n_next
for j in range(n_next):
id_ = i * n_next + j
color = "black"
plt.plot(
[1 / (2 * n) + i / n, 1 / (2 * N) + id_ / N],
[l * y0, (l + 1 / 2) * y0 - y1],
color=color,
lw=2 * scale,
)
plt.plot(
[1 / (2 * N) + id_ / N, 1 / (2 * n_next) + j / n_next],
[(l + 1 / 2) * y0 + y1, (l + 1) * y0],
color=color,
lw=2 * scale,
)
plt.xlim(0, 1)
plt.ylim(-0.1 * y0, (neuron_depth - 1 + 0.1) * y0)
# -- Transformation functions
DC_to_FC = ax.transData.transform
FC_to_NFC = fig.transFigure.inverted().transform
# -- Take data coordinates and transform them to normalized figure coordinates
DC_to_NFC = lambda x: FC_to_NFC(DC_to_FC(x))
plt.axis("off")
# plot splines
for l in range(neuron_depth - 1):
n = width[l]
for i in range(n):
n_next = width[l + 1]
N = n * n_next
for j in range(n_next):
id_ = i * n_next + j
im = plt.imread(f"{folder}/sp_{l}_{i}_{j}.png")
left = DC_to_NFC([1 / (2 * N) + id_ / N - y1, 0])[0]
right = DC_to_NFC([1 / (2 * N) + id_ / N + y1, 0])[0]
bottom = DC_to_NFC([0, (l + 1 / 2) * y0 - y1])[1]
up = DC_to_NFC([0, (l + 1 / 2) * y0 + y1])[1]
newax = fig.add_axes((left, bottom, right - left, up - bottom))
newax.imshow(im)
newax.axis("off")
if title is not None:
plt.title(title)
plt.show()
</d-code>
<p>For example, we can visualize the base network activations with the script below.</p>
<d-code block="" language="python" style="font-size:0.7em">
f = lambda x: (torch.sin(x[:, [0]]) + x[:, [1]] ** 2)
dataset = create_dataset(f, n_var=2, train_num=1000, test_num=100)
# Initialize and plot KAN
config = KANConfig()
layer_widths = [2, 1, 1]
model = KAN(layer_widths, config)
model(dataset["train_input"])
plot(model)
</d-code>
<figure>
<img src="/assets/img/example_viz.png" width="400" alt="KAN Visualization" />
<figcaption><center>Visualizing the activations of a randomly initialized KAN network.</center> </figcaption>
</figure>
<h3 id="synthetic-example">Synthetic Example</h3>
<p>We can put this all together with a simple example. I would recommend scaling this further to a more interesting task, but for now you can verify that the model training is correct. Consider a function of the form $f(x_1,x_2) = \exp \left( \sin(\pi x_1) + x_2^3 \right)$. We are going to learn this function using a KAN of the form \(f(x) = \mathcal{K}_{1,1} \left( \mathcal{K}_{1,2} \left( x_1, x_2 \right) \right)\).</p>
<d-code block="" language="python" style="font-size:0.7em">
seed = 7
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
f = lambda x: torch.exp(torch.sin(torch.pi * x[:, [0]]) + x[:, [1]] ** 3)
dataset = create_dataset(f, n_var=2, train_num=1000, test_num=100)
config = KANConfig()
layer_widths = [2, 1, 1]
model = KAN(layer_widths, config)
results = train(
model,
dataset=dataset,
steps=50000,
batch_size=128,
batch_size_test=32,
lr=0.01,
device=config.device,
)
# Plot training results
plot_results(results)
# Plot network activations
model(dataset["train_input"])
plot_model(model)
</d-code>
<figure>
<img src="/assets/img/simple_eval.png" width="400" alt="KAN Visualization" />
<figcaption><center>Visualizing the activations of a trained KAN network. As expected, the activations learn (affine transformation of) the correct symbolic functions compose to form the original desired function. </center> </figcaption>
</figure>
<h2 id="part-iii-kan-specific-optimizations">Part III: KAN-specific Optimizations</h2>
<p>The attentive reader may have noticed that the choice of B-spline is somewhat arbitrary, and the KAN itself is not necessarily tied to this choice of function approximator. In fact, B-splines are not the only choice to use, even among the family of different spline regressors. <d-footnote>https://stats.stackexchange.com/questions/422702/what-is-the-advantage-of-b-splines-over-other-splines</d-footnote></p>
<p>A large portion of the original paper covers computation tricks to construct KANs with B-splines as the learnable activation function. While the authors prove a (type of) universal approximation theorem for KANs with B-splines, there are other choices of parameterized function classes that can be explored, potentially for computational efficiency.<d-footnote>B-splines are defined over an interval, and evaluating B-spline functions on an input $x$ inherently requires branching logic because the basis functions are only non-zero over a certain interval. To take advantage of modern deep learning hardware, we would ideally like to use a representation that uses a minimal number of the same type of instruction (e.g. multiplication for MLPs) to compute the layer forward pass.</d-footnote></p>
<p><strong>Remark</strong>. Because we are modifying the code from <a href="#part-i-the-minimal-kan-model-architecture">Part I</a>, I’ve tried to keep the code compact by only including areas where changes were made. You can either follow along, or use the full KAN notebook.</p>
<h3 id="b-spline-optimizations-grid-extension">B-Spline Optimizations: Grid Extension</h3>
<p>Recall that the flexibility of our B-splines are determined by the number of learnable coefficients, and therefore the number of basis functions that it has. Furthermore, the number of basis functions is determined by the number of knot points \(G\). Suppose now that we want to include \(G'\) knots for a finer granularity on our learnable activations. Ideally, we want to add more knot points while preserving the original shape of the function. In other words, we want</p>
\[\sum_{h=0}^{G + k - 1} c_h B_{h, k} (x) \approx \sum_{h'=0}^{G' + k - 1} c_{h'} B_{h', k} (x)\]
<p>We can tensorize this expression with respect to a batch of inputs $(z_1,…,z_b)$<d-footnote>You may be confused why I use the variable $z$. Recall that we have a unique B-spline for every activation, or $m \times n$ of them. For edge $j \rightarrow i$, each $z_1,...,z_b$ would be each $x_j$ in the batch. Using $x_1,...,x_b$ would conflate the input vector $x$ and an individual coordinate of the input. </d-footnote></p>
<p>\(\begin{bmatrix}
B_{1, k} (z_1) &amp; B_{2, k} (z_1) &amp; ... &amp; B_{G+k-1, k} (z_1) \\
B_{1, k} (z_2) &amp; B_{2, k} (z_2) &amp; ... &amp; B_{G+k-1, k} (z_2) \\
\vdots \\
B_{1, k} (z_b) &amp; B_{2, k} (z_b) &amp; ... &amp; B_{G+k-1, k} (z_b) \\
\end{bmatrix}
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_{G+k-1} \\
\end{bmatrix}
\approx
\begin{bmatrix}
\sum_{h'=0}^{G' + k - 1} c_{h'} B_{h', k} (z_1) \\
\sum_{h'=0}^{G' + k - 1} c_{h'} B_{h', k} (z_2) \\
\vdots \\
\sum_{h'=0}^{G' + k - 1} c_{h'} B_{h', k} (z_b) \\
\end{bmatrix}\)
which is of the form $AX = B$. We can thus use least-square to solve for $X$, giving us our new coefficients on our finer set of knot points.</p>
<d-code block="" language="python" style="font-size:0.7em">
def grid_extension(self, x: torch.Tensor, new_grid_size: int):
"""
Increase granularity of B-spline activation by increasing the
number of grid points while maintaining the spline shape.
"""
# Re-generate grid points with extended size (uniform)
new_grid = generate_control_points(
self.grid_range[0],
self.grid_range[1],
self.in_dim,
self.out_dim,
self.spline_order,
new_grid_size,
self.device,
)
# bsz x out_dim x in_dim x (old_grid_size + spline_order)
old_bases = self.univarate_fn(x, self.grid, self.spline_order, self.device)
# bsz x out_dim x in_dim x (new_grid_size + spline_order)
bases = self.univarate_fn(x, new_grid, self.spline_order, self.device)
# out_dim x in_dim x bsz x (new_grid_size + spline_order)
bases = bases.permute(1, 2, 0, 3)
# bsz x out_dim x in_dim
postacts = torch.sum(old_bases * self.coef[None, ...], dim=-1)
# out_dim x in_dim x bsz
postacts = postacts.permute(1, 2, 0)
# solve for X in AX = B, A is bases and B is postacts
new_coefs = torch.linalg.lstsq(
bases.to(self.device),
postacts.to(self.device),
driver="gelsy" if self.device == "cpu" else "gelsd",
).solution
# Set new parameters
self.grid_size = new_grid_size
self.grid = new_grid
self.coef = torch.nn.Parameter(new_coefs, requires_grad=True)
</d-code>
<p>I wanted to mention that for the <code class="language-plaintext highlighter-rouge">driver</code> parameter in <code class="language-plaintext highlighter-rouge">torch.linalg.lstsq</code>, there are certain solvers like QR decomposition that require full-rank columns on the basis functions. I’ve chosen to avoid these solvers, but there are several ways to go about solving the least-squares problem efficiently.</p>
<p>We can visually evaluate the accuracy of our grid extension algorithm by simply looking at the activations before and after a grid extension.</p>
<d-code block="" language="python" style="font-size:0.7em">
seed = 7
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
f = lambda x: (x[:, [0]] ** 3 + x[:, [1]] ** 2)
dataset = create_dataset(f, n_var=2, train_num=1000, test_num=100)
config = KANConfig()
layer_widths = [2, 1, 1]
model = KAN(layer_widths, config)
results = train(
model,
dataset=dataset,
steps=10000,
batch_size=32,
batch_size_test=8,
lr=0.01,
device=config.device,
)
model(dataset["train_input"])
plot(model)
model.grid_extension(dataset["train_input"], new_grid_size=50)
model(dataset["train_input"])
plot(model)
</d-code>
<figure>
<img src="/assets/img/grid_extension_2.png" width="400" alt="KAN Grid Extension from 5 to 50" />
<figcaption><center>You will notice in the generated plot above that the KAN learns the correct function $f(x_1,x_2) = (x_1^3 + x_2^2)$. Grid extending from a grid size of 5 (left) to 50 (right) using least-squares. You can see some poor fitting behavior on the right activation, possibly due to an insufficient spread of data sampled for grid extension. </center> </figcaption>
</figure>
<h3 id="activation-pruning">Activation Pruning</h3>
<p>Pruning network weights is not unique to KANs, but they help the models become more readable and interpretable. Our implementation of pruning is going to be <em>extremely inefficient</em>, as we will mask out activations <strong>after they are calculated</strong>. There is already a large body of works for neural networks dedicated to bringing about performance benefits through pruning<d-footnote>There are both memory footprint and computation benefits to pruning. On the memory side, reducing the number of parameters is a clear benefit. On the compute side, specific pruning patterns like 2:4 pruning can be made into efficient kernels. Our implementation yields none of these benefits, and is only useful for interpreting the model.</d-footnote> so we choose to make the code simple. To begin, we can first define a mask over the activations \(\mathcal{M}_{i,j} \in \{0,1\}^{m \times n}\) that zeros out activations belonging to pruned edges. In practice, we would want to prune <em>before</em> the computation, but tensorizing this process efficiently is not clean.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KANLayer(nn.Module):
"Defines a KAN layer from in_dim variables to out_dim variables."
"Updated to include pruning mechanism."
def __init__(self, ...)
self.activation_mask = nn.Parameter(
torch.ones((out_dim, in_dim), device=device), requires_grad=False
) # &lt;-- added mask
...
def forward(self, x: torch.Tensor):
...
# Form the batch of matrices phi(x) of shape [batch_size x out_dim x in_dim]
phi = self.residual_layer(x, spline)
# Mask out pruned edges
phi = phi * self.activation_mask[None, ...] # &lt;-- added mask logic
...
</d-code>
<p>We also need to define a metric for pruning. We can define this function at the high-level KAN module. For every layer, each node is assigned two scores: the input score is the absolute value of the maximum activation averaged over the training batch input<d-footnote>Ideally we want to pass in the entire training dataset when computing this, but it seems costly. For now, we just assume a large batch of data can sufficiently approximate the whole dataset.</d-footnote>, while the output score is computed the same, but for its output activations. More formally,</p>
\[\begin{align}
\text{score}^{(\ell, \text{in})}_{i} &amp;\triangleq \max_{j} \left( \|\Phi^{(\ell-1)}_{i,j}\|_1 \right) \\
\text{score}^{(\ell, \text{out})}_{i} &amp;\triangleq \max_{k} \left( \|\Phi^{(\ell+1)}_{k,i}\|_1 \right)
\end{align}\]
<p>If \(\text{score}^{(\ell, \text{in})}_{i} &lt; \theta \lor \text{score}^{(\ell, \text{out})}_{i} &lt; \theta\) for some threshold $\theta = 0.01$, then we can prune the node by masking its incoming and outgoing activations. We tensorize this operation as a product of two indicators below.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KAN(nn.Module):
...
@torch.no_grad
def prune(self, x: torch.Tensor, mag_threshold: float = 0.01):
"""
Prune (mask) a node in a KAN layer if the normalized activation
incoming or outgoing are lower than mag_threshold.
"""
# Collect activations and cache
self.forward(x)
# Can't prune at last layer
for l_idx in range(len(self.layers) - 1):
# Average over the batch and take the abs of all edges
in_mags = torch.abs(torch.mean(self.layers[l_idx].activations, dim=0))
# (in_dim, out_dim), average over out_dim
in_score = torch.max(in_mags, dim=-1)[0]
# Average over the batch and take the abs of all edges
out_mags = torch.abs(torch.mean(self.layers[l_idx + 1].activations, dim=0))
# (in_dim, out_dim), average over out_dim
out_score = torch.max(out_mags, dim=0)[0]
# Check for input, output (normalized) activations &gt; mag_threshold
active_neurons = (in_score &gt; mag_threshold) * (out_score &gt; mag_threshold)
inactive_neurons_indices = (active_neurons == 0).nonzero()
# Mask all relevant activations
self.layers[l_idx + 1].activation_mask[:, inactive_neurons_indices] = 0
self.layers[l_idx].activation_mask[inactive_neurons_indices, :] = 0
</d-code>
<p>In practice, you will call the <code class="language-plaintext highlighter-rouge">prune(...)</code> function after a certain number of training steps or post-training. Our current plotting function does not support these pruned activations, but we add this feature in the <a href="#appendix">Appendix</a>.</p>
<h3 id="fixing-symbolic-activations">Fixing Symbolic Activations</h3>
<p>A large selling point of the original paper is that KANs can be thought of as a sort of “pseudo-symbolic regression”. In some sense, if you know the original activations before-hand or realize that the activations are converging to a known non-linear function (e.g. $b \sin(x)$), we can choose to fix these activations. There are many ways to implement this feature, but similar to <a href="#activation-pruning">the pruning section</a>, I’ve chosen to favor readability over efficiency. The original paper mentions two features that <strong>are not implemented below</strong>. Namely, storing coefficients affine transformations of known functions (e.g. $a f(b x + c) + d$) and fitting the current B-spline approximation to a known function. The code below allows the programmer to directly fix symbolic functions in the form of univariate Python <code class="language-plaintext highlighter-rouge">lambda</code> functions. First, we provide a function for a KAN model to fix (or unfix to the B-spline) a specific layer’s activation to a specified function.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KAN(nn.Module):
...
@torch.no_grad
def set_symbolic(
self,
layer: int,
in_index: int,
out_index: int,
fix: bool,
fn,
):
"""
For layer {layer}, activation {in_index, out_index}, fix (or unfix if {fix=False})
the output to the function {fn}. This is grossly inefficient, but works.
"""
self.layers[layer].set_symbolic(in_index, out_index, fix, fn)
</d-code>
<p>We first define a <code class="language-plaintext highlighter-rouge">KANSymbolic</code> module that is analogous to the <code class="language-plaintext highlighter-rouge">KANActivation</code> module used to compute B-spline activations. Here, we store an array of functions \(\{f_{i,j}(\cdot)\}_{i \in [m], j \in [n]}\) that are applied in the forward pass to form a matrix \(\{f_{i,j}(x_j)\}_{i \in [m], j \in [n]}\). Each function is initialized to be an identity function. Unfortunately, there is not (to my knowledge) an efficient way to perform this operation in the general case where all the symbolic functions are unique.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KANSymbolic(nn.Module):
"Defines and stores the Symbolic functions fixed / set for a KAN."
def __init__(self, in_dim: int, out_dim: int, device: torch.device):
"""
We have to store a 2D array of univariate functions, one for each
edge in the KAN layer.
"""
super(KANSymbolic, self).__init__()
self.in_dim = in_dim
self.out_dim = out_dim
self.fns = [[lambda x: x for _ in range(in_dim)] for _ in range(out_dim)]
def forward(self, x: torch.Tensor):
"""
Run symbolic activations over all inputs in x, where
x is of shape (batch_size, in_dim). Returns a tensor of shape
(batch_size, out_dim, in_dim).
"""
acts = []
# Really inefficient, try tensorizing later.
for j in range(self.in_dim):
act_ins = []
for i in range(self.out_dim):
o = torch.vmap(self.fns[i][j])(x[:,[j]]).squeeze(dim=-1)
act_ins.append(o)
acts.append(torch.stack(act_ins, dim=-1))
acts = torch.stack(acts, dim=-1)
return acts
def set_symbolic(self, in_index: int, out_index: int, fn):
"""
Set symbolic function at specified edge to new function.
"""
self.fns[out_index][in_index] = fn
</d-code>
<p>We now have to define the symbolic activation logic inside the KAN layer. When computing the output activations, we use a similar trick to the pruning implementation by introducing a mask that is $1$ when the activation should be symbolic<d-footnote>Remember that this solution has the same inefficiencies as the pruning solution. We end up computing activations for both the B-splines and the symbolic activations. For readability, we've chosen to implement it this way, but in practice you will probably want to change this.</d-footnote> and $0$ when it should be the B-spline activation. We also add the function for setting an activation to be a symbolic function and modify the forward pass to support this operation.</p>
<d-code block="" language="python" style="font-size:0.7em">
class KANLayer(nn.Module):
def __init__(self, ...):
...
self.symbolic_fn = KANSymbolic(
in_dim,
out_dim,
device
)
self.symbolic_mask = torch.nn.Parameter(
torch.zeros(out_dim, in_dim, device=device)
).requires_grad_(False) # &lt;-- added mask
...
def set_symbolic(self, in_index: int, out_index: int, fix:bool, fn):
"""
Set the symbolic mask to be fixed (fix=1) or unfixed.
"""
if fix:
self.symbolic_mask[out_index, in_index] = 1
self.symbolic_fn.set_symbolic(in_index, out_index, fn)
else:
self.symbolic_mask[out_index, in_index] = 0
def forward(self, x: torch.Tensor):
...
# Perform symbolic computations
sym_phi = self.symbolic_fn(x)
phi = phi * (self.symbolic_mask == 0) + sym_phi * self.symbolic_mask
# Mask out pruned edges
phi = phi * self.activation_mask[None, ...]
...
</d-code>
<p>We can test our implementation by learning the function \(f(x_1,x_2) = \sin(x_1) + x_2^2\) and plotting the result.</p>
<d-code block="" language="python" style="font-size:0.7em">
config = KANConfig()
layer_widths = [2, 1, 1]
model = KAN(layer_widths, config)
model.set_symbolic(0, 0, 0, True, lambda x : torch.sin(x))
model.set_symbolic(0, 1, 0, True, lambda x : x ** 2)
results = train(
model,
dataset=dataset,
steps=10000,
batch_size=32,
batch_size_test=8,
lr=0.01,
device=config.device,
)
plot_results(results)
model(dataset["train_input"])
plot(model)
</d-code>
<figure>
<img src="/assets/img/fix_activation.jpg" width="400" alt="Fixing the activation function." />
<figcaption><center>We learn a [2,1,1] KAN for the function $f(x_1,x_2) = \sin(x_1) + x_2^2$, but we fix the first layer to have symbolic activations using a lambda function. </center> </figcaption>
</figure>
<h2 id="part-iv-applied-example">Part IV: Applied Example</h2>
<p>This section will be focused on applying KANs to a standard machine learning problem. The original paper details a series of examples where KANs learn to fit a highly non-linear or compositional function. Of course, while these functions are difficult to learn, the use of learnable univariate functions makes KANs suitable for these specific tasks. I emphasized the similarities between KANs and standard deep learning models throughout this post, so I also wanted to present a deep learning example (even though it doesn’t work very well). We will run through a simple example of training a KAN on the canonical MNIST handwritten digits dataset<d-cite key="lecun1998gradient"></d-cite> to show how easy it is to adapt these models for standard deep learning settings. We first download the relevant data.</p>
<d-code block="" language="python" style="font-size:0.7em">
# Run these without ! in terminal, or run this cell if using colab.
!wget www.di.ens.fr/~lelarge/MNIST.tar.gz
!tar -zxvf MNIST.tar.gz -C data/
</d-code>
<p>In the interest of reusing the existing train logic we created <a href="#training-loop">earlier</a>, we write a function to turn a <code class="language-plaintext highlighter-rouge">torch.Dataset</code> with MNIST into the dictionary format. <em>For general applications, I recommend sticking with the torch Dataloader framework</em>.</p>
<d-code block="" language="python" style="font-size:0.7em">
def split_torch_dataset(train_data, test_data):
"""
Quick function for splitting dataset into format used
in rest of notebook. Don't do this for your own code.
"""
dataset = {}
dataset['train_input'] = []
dataset['train_label'] = []
dataset['test_input'] = []
dataset['test_label'] = []
for (x,y) in train_data:
dataset['train_input'].append(x.flatten())
dataset['train_label'].append(y)
dataset['train_input'] = torch.stack(dataset['train_input']).squeeze()
dataset['train_label'] = torch.tensor(dataset['train_label'])
dataset['train_label'] = F.one_hot(dataset['train_label'], num_classes=10).float()
for (x,y) in test_data:
dataset['test_input'].append(x.flatten())
dataset['test_label'].append(y)
dataset['test_input'] = torch.stack(dataset['test_input']).squeeze()
dataset['test_label'] = torch.tensor(dataset['test_label'])
dataset['test_label'] = F.one_hot(dataset['test_label'], num_classes=10).float()
print('train input size', dataset['train_input'].shape)
print('train label size', dataset['train_label'].shape)
print('test input size', dataset['test_input'].shape)
print('test label size', dataset['test_label'].shape)
return dataset
</d-code>
<p>Finally, like all previous examples, we can run a training loop over the MNIST dataset. We compute the training loss using the standard binary cross-entropy loss and define the KAN to produce logits from 0-9. Due to restrictions in our <code class="language-plaintext highlighter-rouge">train()</code> function, we define our test loss as the total number of incorrectly marked samples out of $100$ validation samples.</p>
<d-code block="" language="python" style="font-size:0.7em">
config = KANConfig()
config.grid_size = 10
layer_widths = [28 * 28, 64, 10]
model = KAN(layer_widths, config)
transform = transforms.Compose(
[transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
)
train_data = datasets.MNIST("./data", train=True, download=False, transform=transform)
test_data = datasets.MNIST("./data", train=False, transform=transform)
dataset = split_torch_dataset(train_data, test_data)
loss = nn.BCEWithLogitsLoss()
results = train(
model,
dataset=dataset,
steps=500,
batch_size=128,
batch_size_test=100,
lr=0.1,
log=1,
device=config.device,
loss_fn=lambda x, y: loss(x, y),
loss_fn_eval=lambda x, y: (torch.argmax(x, dim=-1) != torch.argmax(y, dim=-1)).sum()
)
plot_results(results)
</d-code>
<p>You may notice that the training is significantly slower even for such a small model. Furthermore, the results here are not good as expected. I’m confident that with sufficient tuning of the model you can get MNIST to work (there are examples of more <a href="https://github.com/1ssb/torchkan">sophisticated KAN implementations</a> <d-cite key="torchkan"></d-cite> that perform extremely well), but the above example raises questions about the efficiency of the original implementation. Before we are able to properly scale these models, we need to first study the choice of parameterization and whether we should even treat KANs the way we treat MLPs.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope this resource was useful to you – whether you learned something new, or gained a certain perspective along the way. I wrote up this annotated blog to clean up my notes on the topic, as I am interested in improving these models from an efficiency perspective. If you find any typos or have feedback about this resource, feel free to reach out!</p>
<h2 id="appendix">Appendix</h2>
<p>I may re-visit this section in the future with some more meaningful experiments when I get the time.</p>
<h3 id="plotting-symbolic-and-pruned-kans">Plotting Symbolic and Pruned KANs</h3>
<p>The plotting function defined in <a href="#network-visualization">Network Visualization</a> doesn’t include logic for handling the pruned activation masks and the symbolic activations. We will include this logic separately, or you can follow the rest of the visualization code in the original repository.</p>
<h3 id="open-research-making-kans-efficient">Open Research: Making KANs Efficient</h3>
<p>It is known that these models currently do not scale well due to both memory and compute inefficiencies. Of course, it is unknown whether scaling these models will be useful, but the authors posit that they are more parameter efficient than standard deep learning models because of the flexibility of their learned univariate functions. As you saw in the <a href="#part-iv-applied-example">MNIST example</a>, it is not easy to scale the model even for MNIST training. I sort of avoided this question before, but I want to highlight a few reasons for these slowdowns.</p>
<ol>
<li>We fully materialize a lot of intermediate activations for the sake of demonstration, but even in an optimized implementation, some of these intermediate activations are unavoidable. Generally, materializing intermediate activations means lots of movement between DRAM and the processors, which can cause significant slowdown. There is a repository called <a href="#https://github.com/Jerry-Master/KAN-benchmarking">KAN-benchmarking</a> dedicated to evaluating different KAN implementations. <em>I may include an extra section on profiling in the future.</em></li>
<li>Each activation \(\Phi_{i,j}\) or edge in the network is potentially different. At an machine instruction level, this means that we cannot take advantage of SIMD or SIMT that standard GEMM or GEMV operations have on the GPU. There are alternative implementations of KANs that were mentioned earlier that attempt to get around these issues <d-cite key="ta2024bsrbfkancombinationbsplinesradial,bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks,ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks">, but even then they do not scale well compared to MLPs. I suspect the choice of the family of parameterized activations will be extremely important moving forward.</d-cite></li>
</ol>
<h3 id="b-spline-optimizations-changing-knot-points">B-Spline Optimizations: Changing Knot Points</h3>
<p>A natural question is whether we have to fix the knot points to be uniformly spaced, or if we can use the data to adjust our knot points. The original paper does not detail this optimization, but their codebase actually includes this feature. If time permits, I may later include a section on this – I think it may be important for performance of KANs with B-splines, but for general KANs maybe not.</p>
<h2 id="citation">Citation</h2>
<p>Just as a formality, if you want to cite this for whatever reason, use the BibTeX below.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2024annotatedkan,
  title   = "Annotated KAN",
  author  = "Zhang, Alex",
  year    = "2024",
  month   = "June",
  url     = "https://alexzhang13.github.io/blog/2024/annotated-kan/"
}
</code></pre></div></div>]]></content><author><name>Alex Zhang</name></author><category term="blog" /><category term="annotated" /><category term="ai" /><category term="kan" /><category term="kolmogorov-arnold" /><summary type="html"><![CDATA[An annotated guide to the Kolmogorov-Arnold Network]]></summary></entry><entry><title type="html">Highlights of NeurIPS 2023 from Reading All 3584 Abstracts</title><link href="https://alexzhang13.github.io/blog/2024/neurips2023/" rel="alternate" type="text/html" title="Highlights of NeurIPS 2023 from Reading All 3584 Abstracts" /><published>2024-01-09T00:00:00+00:00</published><updated>2024-01-09T00:00:00+00:00</updated><id>https://alexzhang13.github.io/blog/2024/neurips2023</id><content type="html" xml:base="https://alexzhang13.github.io/blog/2024/neurips2023/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>To celebrate the end of my graduate school application deadlines and finals week disaster, I decided
to spend my winter break <b>going through and reading every single abstract of the accepted papers
in the NeurIPS 2023</b> (which I unfortunately couldn’t attend). It was a long and sometimes mind-numbing process (especially since the TeX wasn’t rendering on the <a href="neurips.cc">neurips.cc</a> website), but it was really cool to see all these works and ideas that I had no idea were being done. Luckily, I am somewhat familiar with quite a number of these papers because they popped off on <a href="https://arxiv.org/list/cs.AI/recent">arXiv</a> or
<a href="https://twitter.com/home">Twitter</a> when they were first announced, so I wasn’t discovering every
single paper for the first time. Here is just a highlight of what I found interesting and the
general vibes I had while reading over the last two weeks, but keep in mind that I am an
undergraduate student that has not worked with or spent a lot of time with a variety of popular
topics (e.g. <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a>, <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential
privacy</a>, <a href="https://en.wikipedia.org/wiki/Causal_inference">causal
inference</a>). I’ve structured this post into a
<strong>high-level overview for each topic</strong> of what I observed, followed by <strong>short discussions on papers I
found interesting</strong>. Each discussion is loaded with references to the relevant NeurIPS papers, and I’ve tried to ensure that almost every citations in this post are from NeurIPS 2023.
If you want to read the abstracts on your own, they’re available publicly at
<a href="https://neurips.cc/virtual/2023/papers.html">https://neurips.cc/virtual/2023/papers.html</a>. Finally,
I ended up searching up so many terms, I eventually started keeping track of them in case the
curious reader wants to know at the bottom of this post.</p>
<h2 id="the-overall-vibes">The Overall Vibes</h2>
<figure>
<img src="/assets/img/neurips2023.png" width="400" alt="Visualization of NeurIPS 2023" />
<figcaption>Visualization generated from <a href="https://neurips2023.vizhub.ai/?ref=blog.roboflow.com&amp;brushed=%255B%255B14.600000381469727%252C-10%255D%252C%255B798.7249755859375%252C796.664306640625%255D%255D">here.</a></figcaption>
</figure>
<p>2023 was a great year for AI (generative models especially), and the number of submissions to
NeurIPS 2023 reflects that. This year, there were <b>3584</b> accepted papers out of an astonishing <b>12345</b> submissions. Honestly, I expected to go into this process marking only my top 10 or 20 favorite papers, but I discovered so many absolutely fascinating works on the most random things like applying 3D neural rendering methods to Unreal Engine. By the end of this process, I ended up reading or skimming probably 50 or so of the papers (excluding the ones that I had seen before). Of course, from the abstract alone, it is not possible to grasp the quality or impact of a work, and in an ideal world I would have read each paper in their entirety and tried their code as well. Regardless, after reading through, these are some things that stuck out to me this year:</p>
<ol>
<li><strong>Multimodal is the new “thing”.</strong> Rather unsurprisingly, combining vision and language is a hot topic <d-cite key="huang2023language,wang2023connecting,luo2023cheap,gadre2023datacomp,zhu2023multimodal,mizrahi2023m,yin2023vlattack,cheng2023metaadapter,dai2023instructblip,wortsman2023stable,wu2023parameter"></d-cite>. With LLMs demonstrating impressive performance in the past few years, leveraging these models to reason over visual data is a natural progression of the technology.</li>
<li><strong>In-context Learning (ICL) belongs everywhere.</strong> Anything that even resembles a transformer should exhibit in-context learning, whether its a diffusion model <d-cite key="wang2023incontext"></d-cite>, for RL <d-cite key="brooks2023large"></d-cite>, or even for generic regression <d-cite key="raventós2023pretraining"></d-cite>. I wonder what’ll happen when more gradient-free learning methods like hyperdimensional computing <d-footnote>https://arxiv.org/abs/2111.06077</d-footnote> take off…</li>
<li><strong>Diffusion models are really good at generating things.</strong> Diffusion models are getting so good at generating stuff, it seems like people are confident that they can be used for synthetic data<d-cite key="yang2023freemask,pronovost2023scenario,ntavelis2023autodecoding,zhu2023genimage"></d-cite>. There is a lot of exploration into using diffusion for more than just de-noising and text-to-image generation <d-cite key="saxena2023surprising,mo2023dit3d"></d-cite>.</li>
<li><strong>Leveraging model priors.</strong> Foundation models are a blessing because they contain so much information about a specific domain. Leveraging these models beyond just in-domain downstream tasks is going to be extremely important to understand for the next few years, especially concerning data multiple modalities <d-cite key="wang2023connecting"></d-cite>. We also want to understand how to use the information present in these models in an interpretable way.</li>
<li><strong>Model robustness.</strong> We want to be able to trust our models, and this means that our models behave according to our expectations. These models should also be robust to malicious actors and data, and should not leak our information, especially if we continue to feed them with our data. There are many tradeoffs and metrics for model robustness, so identifying the upper bounds both theoretically and empirically are important.</li>
<li><strong>What does it mean to be similar?</strong> How do we measure similarity being two “concepts”? Or even just semantics? An intuitive way is to use similarity metrics like inner products over an embedding space, but how do we know that this embedding space is truly representative of the data <d-cite key="oh2023geodesic"></d-cite>? Do we measure similarity in terms of probability distributions, or do we use mutual information <d-cite key="dunion2023conditional,wang2023mutualinformation"></d-cite>?</li>
<li><strong>When are inductive biases useful?</strong> We don’t necessarily always have the luxury of being able to scale our models and datasets for every domain, and we don’t know whether or not our models should not use inductive biases <d-cite key="bachmann2023scaling"></d-cite>. We want to understand whether we can leverage our own understanding of problems to introduce effective inductive biases like symmetry to solve problems <d-cite key="wu2023equivariant,lengyel2023color"></d-cite>.</li>
</ol>
<h2 id="large-language-models">Large Language Models</h2>
<p>This is the section of papers I was most familiar with before reading through the abstracts.
Generally, there have been a lot of works exploring the reasoning capabilities of LLMs through
prompting or fine-tuning. I think the LLM papers at this conference reflect the
interests of the NLP community for the past year even though a lot of these works can be considered
<strong>at least a year old at this point</strong> (which in this field is old!). One general gripe I have with these works is that they often like to make claims about general LLM behavior, but often evaluate on an arbitrary LLM of their choosing. It would be nicer if there was some consistency here, but it’s probably because the field is so fast-moving combined with the fact that many models are just inaccessible to 95% of labs. There is also this interesting interplay of whether data purity or extra machinery is the driving factor towards improving performance. These are the general themes that I’ve noticed:</p>
<ol>
<li><strong>Reasoning.</strong> How can we apply algorithms or intuitions to fine-tune
LLMs to be better planners, reasoners, etc. like in <d-cite key="phan2023training"></d-cite>? Furthermore, how can we explicitly improve their reasoning capabilities without gradient updates like in <d-cite key="yao2023tree"></d-cite>?</li>
<li><strong>Logical Reasoning.</strong> Can LLMs be used for logical tasks like mathematics <d-cite key="zhang2023evaluating,frieder2023mathematical"></d-cite>  and coding <d-cite key="yang2023intercode"></d-cite>? What are the pitfalls and potential solutions? We are interested in analyzing the limits of their abilities, as well as understand why and when they fail <d-cite key="hanna2023does"></d-cite>.</li>
<li><strong>Agents.</strong> Can LLMs be considered cognitive agents? That is, can we
equip them with capabilities (tools, externals APIs <d-cite key="schick2023toolformer"></d-cite>, etc.) such that they can interact with environments through text generation? Furthermore, can they interact, think, and reflect in an anthropomorphic way? <d-cite key="madaan2023selfrefine,shinn2023reflexion"></d-cite></li>
<li><strong>Efficiency.</strong> A key bottleneck in autoregressive LLM inference on GPUs is data movement, so research into circumventing this issue by exploiting parallelism and cached data is key (e.g. speculative decoding) <d-cite key="jin2023s3,kim2023speculative,sun2023spectr,zhu2023optimal"></d-cite>. Furthermore, can we improve the speed and cost of both training and inference on LLMs at both a systems-level (e.g. take advantage of properties of GPU memory footprints and throughput speeds) and model-level (low-rank fine-tuning, adaptors, sparsity, etc.) <d-cite key="portes2023mosaicbert,pagliardini2023faster,dettmers2023qlora"></d-cite>?</li>
<li><strong>Scaling and Fine-tuning Models.</strong> What advances can we make to push the capabilities of foundation models (e.g. MoE) <d-cite key="xue2023repeat"></d-cite>? What procedures can we use to efficiently fine-tune <d-cite key="malladi2023finetuning,dubois2023alpacafarm"></d-cite> these models for downstream tasks or direct their generations towards user-aligned behavior (also can we do this without reinforcement learning (RL) <d-cite key="rafailov2023direct,zhou2023lima"></d-cite>) ? Or, what techniques can we use to bridge the gap between smaller models and huge models? Also, what kind of scaling laws can we identify between model parameters and data <d-cite key="muennighoff2023scaling"></d-cite>?</li>
<li><strong>Memory and Context Windows.</strong> The context window of an LLM is inherently limited by memory constraints and the quadratic runtime of the standard attention mechanism. How do we increase the size of the input context window without degrading performance? The primary methods that are being investigated are external memory stores in LLMs to process huge chunks of texts like a book <d-cite key="wang2023augmenting,bertsch2023unlimiformer,mohtashami2023landmark"></d-cite> and summarization tokens <d-cite key="mu2023learning"></d-cite>.</li>
<li><strong>Emergent Capabilities.</strong> It has been observed that LLMs seem to be suddenly able to perform tasks after scaling up to a certain point. Can we characterize these emergent abilities of LLMs effectively and why we observe them <d-cite key="schaeffer2023emergent"></d-cite>?</li>
<li><strong>Controlling Generation.</strong> Can we 1) hard constrain the outputs and 2) steer LLM generations to be what we want? Typically (2) has been done with instruction-tuning and RLHF<d-footnote>Reinforcement Learning with Human Feedback, an application of reinforcement learning to further fine-tune language models to better reflect our human preferences. Recommend reading this source: https://huggingface.co/blog/rlhf</d-footnote>, but methods like <d-cite key="li2023inferencetime"> </d-cite> modify activation patterns in the model during inference while <d-cite key="li2023guiding"></d-cite> learns a smaller auxiliary model to edit the prompt of the larger model, which we can effectively treat as a set of parameters. Furthermore, what methods can we come up with to ensure preference-based alignment is accurate and robust <d-cite key="wang2023aligning"></d-cite></li>
<li><strong>LLMs can do [insert task]</strong>. How far can we go with zero-shot, few-shot, and in-context
learning (ICL) to mimic known algorithmic procedures <d-cite key="chen2023evoprompting"></d-cite> entirely through prompting? For example, policy iteration from RL <d-cite key="brooks2023large"> </d-cite> or time-series forecasting <d-cite key="gruver2023large"> </d-cite>.</li>
<li><strong>Evaluating Language Models.</strong> What kind of metrics and
benchmarks are needed to effectively evaluate the abilities of different LMs? Is it by using a strong LLM as a judge <d-cite key="zheng2023judging"></d-cite>? Also, we generally just want more benchmarks for evaluating abilities like factuality <d-cite key="chen2023felm"></d-cite>, coding <d-cite key="yang2023intercode"></d-cite>, and domain-specific information <d-cite key="liu2023benchmarking,guha2023legalbench,guo2023large"></d-cite>.</li>
</ol>
<h3 id="llm-interesting-papers">[<span style="color:orange">LLM</span>] Interesting Papers</h3>
<p>Below is a list of papers I particularly liked and think are worth reading in their entirety. Of course, there were plenty of other extremely interesting and useful works at this conference, so please do not take this as some kind of definitive ranking of papers. Also, I’m mainly going to be giving a brief sentence about why I think each paper is cool/important and not a <em>tldr</em>, as I think you can get a lot more out of just reading it yourself. I have included this type of subsection at the end of every topic, so enjoy!</p>
<ol>
<li><strong>Tree-of-Thoughts</strong> <d-cite key="yao2023tree"></d-cite>: A simple yet highly useful idea,
tree-of-thoughts (ToT) is simply an extension of chain-of-thoughts where a model can now traverse
through its own “thought” chains as a tree. This simple application of graph traversal to CoT
(which also gives beam search vibes) has been used extensively for prompting in the last year. This paper actually came from my PI’s lab :)</li>
<li><strong>Toolformer</strong> <d-cite key="schick2023toolformer"></d-cite>: Another well-known and simple
approach, Toolformer is an extremely general framework for fine-tuning a model to be able to use
user-specified APIs like a calculator or a search engine. It’s really practical and their
experiments use GPT-J <d-footnote>https://huggingface.co/docs/transformers/model_doc/gptj</d-footnote>, so it’s quite easy to replicate for your own use cases.</li>
<li><strong>Are Emergent Abilities of Large Language Models a Mirage?</strong> <d-cite key="schaeffer2023emergent"></d-cite>: I remember this paper being widely debated when it came out, with many arguing that the author’s conclusion was not indicative of anything useful. I think this paper makes a great point, in that emergent capabilities are a consequence of the evaluation metrics we choose to measure abilities, even if these evaluation metrics are natural (e.g. % of math questions answered). The problem with relying on intuitive of pre-existing metrics is that they don’t tell the full picture about scale vs. performance. I’ll put it like this. Suppose our LLM has never been able to solve task A. No matter how we have scaled it so far, it can never solve A. Scaling is expensive, so an important question is whether scaling will lead to emergent capabilities on A. Instead, if we have an auxiliary task B that is indicative of solving A, we can measure the relationship between scale and performance on A by looking at performance on B. The usefulness of reframing is to hopefully find metrics that show linear relationships between things like scale and downstream task performance, but this is still hard to do.</li>
<li><strong>SPRING: Studying Papers and Reasoning to Play Games</strong> <d-cite key="wu2023spring"></d-cite>: This paper is exciting to me because I am very interested in models that can utilize external sources of information in an intuitive way. Outside of their reasoning module, they read information using an LLM from a game manual (honestly I don’t really get why they use the LaTeX source though, maybe for equations?) and store it as context for their agent (also an LLM). The point is that an LLM can act as a retriever and can intuit about what information is relevant. I think that this work is in a very preliminary stage though, and there is a lot of future research to be done in generalizing this type of framework.</li>
<li><strong>RapidBERT: Pretraining BERT from Scratch for $20</strong> <d-cite key="portes2023mosaicbert"></d-cite>: This paper is kind of crazy… With lots of modern tricks (FlashAttention<d-cite key="dao2022flashattention"></d-cite>, ALiBi<d-cite key="press2022train"></d-cite>, low-precision layernorm, etc.) they can pre-train a BERT with the same performance as the original in just 9 hours on an A100. I think it goes to show far how we’ve gone in optimizing our language models. It is also important to note that they use a more modern scraped dataset in this paper (C4 <d-footnote>https://github.com/allenai/allennlp/discussions/5056</d-footnote>) for pre-training.</li>
<li><strong>Scaling Data-Constrained Language Models</strong> <d-cite key="muennighoff2023scaling"></d-cite>: Understanding the relationship between model scale and data scale is important, and this paper looks into scaling laws under limited amounts of unique data. In their experiments, they found that repeating data during training works but diminishes in performance compared to using completely unique data. As an aside, their results focus on cross-entropy loss, but with any work that focuses on CE loss, it is important to distinguish inherent entropy in the data and the actual performance gap you want to mitigate<d-footnote>https://arxiv.org/abs/2307.15936</d-footnote>.</li>
<li><strong>Collaborative Alignment of NLP Models</strong><d-cite key="khani2023collaborative"></d-cite>: We have been conditioned to use single, huge foundation models because they work well in practice, but this paper looks into whether the learning of several, concept-aligned models with some meta-chooser on top actually works just as well. The primary benefit is the ability to modularize and parallelize LLMs, making them more flexible and also potentially faster.</li>
<li><strong>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.</strong>   <d-cite key="wen2023hard"></d-cite>: You can sort of think of (hard) prompts as parameters to an LLM that transferrable to other LLMs (e.g. If I prompt GPT-4 with “Explain Newton’s laws like I’m 5”, I expect LLAMA 2 to answer similarly). In the case of text-to-image models, the images we get are often not exactly what we want (there are various reasons for this we discuss in the multi-modal section!). This work is basically a way to steer text prompts with gradient-based optimization so they generate the images you want.</li>
<li><strong>OpenAssistant Conversations - Democratizing Large Language Model Alignment</strong> <d-cite key="kopf2023openassistant"></d-cite>: So I actually followed this project on YouTube through Yannic Kilcher’s channel <d-footnote>https://youtube.com/yannickilcher</d-footnote>, and the central idea is to open-source the data required for RLHF because its extremely expensive to curate 60k+ human preference samples. I believe that open-source communities are extremely important for AI, and it’s exciting to see them produce extremely useful projects like this one. Because it is open-source, a large chunk of the paper discusses quality control and reducing “bad” or “toxic” data.</li>
<li><strong>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</strong> <d-cite key="rafailov2023direct"></d-cite>: For a while, the standard preference-alignment approach was to learn a reward model over preference data and fine-tune an LLM with RL on this reward model (RLHF). The issue with this approach is that RL is generally quite unstable and hard to work with, so this paper first motivates re-parameterizing the RLHF objective into a new objective that we can directly minimize. I’m not actually sure how well this works relative to RLHF in practice because I’ve never had access to these tools, but it is an RL-free alternative to preference-based alignment.</li>
</ol>
<h2 id="multi-modal-learning">Multi-modal Learning</h2>
<p>This year had a heavy focus on multimodal (mostly vision + language) models, with many companies/labs introducing their own shiny foundation models and associated datasets to compete with GPT-4. A lot of the desirable features in large multimodal models parallel that of large language models, so many of the research questions and themes are quite similar. In my mind, a core difference is the combinatorially larger amount of paired or associated data required to build a model that can interchangebly handle two different modalities. The obvious direction is to continue to scale the size of visual-language datasets with the size of new models, but I suspect that fundamentally answering how to “ground” two different modalities from a representation learning perspective may be able to reduce the necessary scale. At the end of the day though, this is still an open research question which I don’t know the answer to. The general themes I observed were</p>
<ol>
<li>
<p><strong>New Foundation Models.</strong> Large multimodal models, specifically vision-language, are the logical next progression to LLMs. Thus, it’s rather unsurprising that many research groups are racing to build the next big model <d-cite key="huang2023language,mizrahi2023m"></d-cite> with the same capabilities of LLMs like being instruction-tuned <d-cite key="dai2023instructblip"></d-cite> and in-context learning. However, it looks like the training mechanisms for making these models robust are still pretty elementary. For example, <d-cite key="huang2023language"></d-cite>, they simply treat everything as a token, but use a special embedding token for images and attentive pooling to reduce the complexity of these embeddings, then use the interleaved text and image data for standard log-likelihood optimization. At this conference, I didn’t see many actual models being showcased (although I’ve seen them over time on Twitter), but the large number of datasets seem to indicate that this is a growing direction.</p>
</li>
<li>
<p><strong>Datasets and Benchmarks.</strong> We’ve observed several instances of “good” data being key to getting LLMs to work better. The same logic applies to multi-modal models, except because there generally is no 1-1 mapping between tokens in each modality, this is quite hard. Regardless, there are lots of multi-modal datasets and benchmarks being curated, either by scraping and filtering data on the internet <d-cite key="zhu2023multimodal,gadre2023datacomp"></d-cite> or by curating the data <d-cite key="pătrăucean2023perception,zhang2023m3exam"></d-cite>.</p>
</li>
<li>
<p><strong>The Shared Representation.</strong> As far as I’m aware, the two main ways of building a multi-modal learning model are to tokenize each modality and train it as a decoder model on negative-log-likelihood loss <d-cite key="huang2023language"></d-cite> or train it CLIP-style<d-footnote>CLIP or Contrastive Language-Image Pre-Training is an contrastive learning-based encoding method for embedding images and language into the same embedding space. The benefit is that we can query into this embedding space using text or images, and query from this embedding space to generate either text or images. The idea was popularized from its used in DALL-E, and is the standard for text-to-image models. Read more here: https://openai.com/research/clip </d-footnote> as an encoder model. <d-cite key="oldfield2023parts,qiu2023controlling,samuel2023normguided,oh2023geodesic"></d-cite> In both cases, we want to understand this latent representation embedded either in the model layers or in the embedding space to see if we can exploit its properties <d-cite key="samuel2023normguided,oh2023geodesic"></d-cite>.</p>
</li>
<li>
<p><strong>Complex text prompts.</strong> It is known that text-to-image models often exhibit <em>bag-of-words behavior</em>, which means it lacks a strong understanding of syntax and logical quantifiers in a sentence. If you’ve ever played with Midjourney <d-footnote>A popular text-to-image generator: https://www.midjourney.com/home?callbackUrl=%2Fexplore</d-footnote> or DALL-E2 <d-footnote>A popular text-to-image generator by OpenAI: https://openai.com/dall-e-2</d-footnote>. Several works attempt to inject compositional reasoning in these models to solve this <d-cite key="doveh2023dense,zhao2023unicontrolnet"></d-cite>.</p>
</li>
<li>
<p><strong>Multi-modal video understanding.</strong> Processing videos adds a temporal dimension to these models that is quite difficult. Even standard video understanding models have been difficult to get right for a while now. The naive approach is to concatenate frames and pass them into a image-language model, so there has been some interest in making the language component temporally aware <d-cite key="yu2023selfchained"></d-cite>.</p>
</li>
<li>
<p><strong>All the same questions for LLMs.</strong> Fundamentally, these models are just huge transformers. Even the vision components are basically just LLMs where the vocabulary is image patches (although the vocabulary is much bigger and not fixed I suppose). Regardless, any open problem for an LLM (efficiency <d-cite key="luo2023cheap,wortsman2023stable"></d-cite>, robustness<d-cite key="zhao2023evaluating,yin2023vlattack"></d-cite>, fine-tuning algorithms <d-cite key="wu2023parameter,cheng2023metaadapter"></d-cite>) is essentially also a problem for multi-modal models. The difference though, is that we can assume that we are given a model that is good at each modality, so “bridging” the modalities is what we need to actually solve.</p>
</li>
</ol>
<h3 id="multimodal-interesting-papers">[<span style="color:orange">Multimodal</span>] Interesting Papers</h3>
<p>There are a lot of what I like to call <em>low-hanging fruit</em> in multi-modal models that have been solved, and while they are still interesting and definitely more applicable to most problems, I wanted to focus on some works that I thought were cool.</p>
<ol>
<li><strong>4M: Massively Multimodal Masked Modeling</strong> <d-cite key="mizrahi2023m"></d-cite>: Apple doesn’t usually publish in machine learning conferences (e.g. they didn’t let me publish or extensively discuss my work when I was there), but I have to say, I thought this paper was pretty cool. I mentioned in point (3) that a lot multi-modal models are either decoder-based (token-style) or encoder-based (embedding-style), but the authors of this work discretize the shared embedding space and embed using tokens instead of a continuous embedding.</li>
<li><strong>Connecting Multi-modal Contrastive Representations</strong> <d-cite key="wang2023connecting"></d-cite>: The idea here is that as we continue adding modalities to a shared representation space, we ideally want to use as little paired data as possible (for $N$ modalities, you would need $\binom{N}{2}$ sets of paired data). So we want to leverage pre-existing multimodal models, say a visual-language and a language-audio, and combine their representations without the need for visual-audio data. They effectively learn the projection function from both pre-trained models to the shared representation space, and motivate the loss required to align semantically similar embeddings.</li>
<li><strong>A Theory of Multimodal Learning</strong> <d-cite key="lu2023theory"></d-cite>: This is an interesting paper on trying to formalize multimodal learning, although I don’t exactly understand how this differs from standard unimodal training. My understanding is that the primary limitation is data, and we can basically treat two modalities as distinct subspaces in some larger “unimodal” space. But regardless, they provide a formal differentiation between multimodal and unimodal learning and prove some standard ML theory bounds for an empirical risk minimization (ERM) algorithm.</li>
<li><strong>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</strong> <d-cite key="yin2023vlattack"></d-cite>: They use some cute tricks for developing black-box adversarial attacks on vision-language models by considering perturbations for both modalities in isolation, as well as for image-text pairs. Similar to above, it’s not clear exactly why we need a distinct “multimodal” strategy for this kind of stuff, but perhaps more works into this area will provide more insight.</li>
<li><strong>Geodesic Multi-Modal Mixup for Robust Fine-Tuning</strong> <d-cite key="oh2023geodesic"></d-cite>: We
want to understand the landscape of multi-modal embeddings and see if we can impose nice
properties of this space like making it isotropic. In this paper, they first propose that CLIP
embeddings (ZS) and naively fine-tuned embeddings (FT) have an inherent uniformity issue that
distinctly separates “text” and “images” into different subspaces. Ideally though, they argue
that we want the distribution over the space (they constrain it to be a hypersphere) to be based
on the semantics. Their method proposes to mold this space by generating “mixed” hard-negative
samples to use with the standard contrastive loss during fine-tuning.</li>
</ol>
<figure>
<center>
<img src="/assets/img/oh2023geodesic.png" style="width:60%" alt="Visualization of CLIP embedding space." />
</center>
</figure>
<p>I’m curious because in the past, isotropic properties of “word embeddings” was thought to be a
necessary thing, but it turns out we don’t really care, and a lot of methods that try to
constrain this didn’t turn out to be that useful. I wonder how that applies here.</p>
<h2 id="transformers">Transformers</h2>
<p>With the Transformer module being the standard building block for scalable models, it is important that progress is made on improving their usage as a whole. Just as a side note, I think generally the term “transformer” is now overloaded to mean any structure using positional encodings plus an attention-mechanism, feedforward layers, and normalization in some repeated fashion, and does not necessarily refer to the original Transformer architecture. Interestingly, one thing I didn’t really find at this conference was investigating the enforcement of constraints or inductive biases like equivariance to Transformers, which may be an indicator of Sutton’s Bitter Lesson <d-footnote>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</d-footnote>. The general themes were</p>
<ol>
<li><strong>Studying Transformer Models.</strong> Are there provable or empirically well-understood limitations of Transformers that may be severely limiting for future research directions? We know that Transformers seem to excel at reasoning but also frequently fail in simple cases, so <d-cite key="dziri2023faith"></d-cite> argue through a series of compositional tasks that these models (GPT-3,4) tend to <em>pattern match reasoning chains</em>. Furthermore, in <d-cite key="sanford2023representational"></d-cite>, they try to examine the function classes that Transformers can efficiently approximate, which is especially important for upper bounding the representational capacity of models as they scale. Finally, there are works that examine/ablate features of the Transformer <d-cite key="kazemnejad2023impact"></d-cite> to study their impact.</li>
<li><strong>Efficiency.</strong> How do we modify parts of the transformer to be more efficient to 1) scale them for bigger models and 2) use them on low-compute devices? In <d-cite key="baykal2023alternating"></d-cite>, they selectively act on a fixed block of the embeddings at any layer to increase model capacity while keeping inference latency fixed. In <d-cite key="anagnostidis2023dynamic"></d-cite>, they train layers to selectively drop tokens by imposing a sparsity constraint that affects their modified attention mechanism. Meanwhile, works like <d-cite key="xi2023training"></d-cite> focus on preserving performance for quantized transformers and <d-cite key="liang2023mcuformer"></d-cite> focus on deploying transformers on microcontrollers.</li>
<li><strong>Modifications to Attention.</strong> Can we make attention mechanisms more efficient <d-cite key="chen2023primalattention"></d-cite>? As in sub-quadratic runtime <d-cite key="anagnostidis2023dynamic,yu2023megabyte"></d-cite>? Sparse <d-cite key="pagliardini2023faster"></d-cite>? Or can we even replace attention <d-cite key="fu2023monarch"></d-cite>?</li>
<li><strong>Memory.</strong> Can we increase context-window lengths or add external memory for transformers? This question is tied heavily to LLMs, and hence the methods (external memory source or summarization) are similar. Other than the works tied to LLMs, I only really found <d-cite key="zeng2023vcc"></d-cite>, which basically learns to compress token sequences into “VIP”-tokens that represent what’s most important. My only concern is whether these tokens are domain-specific and how a fully-trained model fairs for transfer learning to other modalities.</li>
</ol>
<h3 id="transformers-interesting-papers">[<span style="color:orange">Transformers</span>] Interesting Papers</h3>
<ol>
<li><strong>Faith and Fate: Limits of Transformers on Compositionality</strong>  <d-cite key="dziri2023faith"> </d-cite>: It has always unclear how good Transformers are at compositional reasoning, and this paper tries to uncover this question in a systematic way. As in, they literally break down each task (they are mostly computational tasks like multiplication and dynamic programming puzzles) into a computation graph and train their models in a bunch of different ways, ultimately concluding that Transformers are good at pattern matching reasoning chains, but not necessarily extrapolating reasoning itself.</li>
<li><strong>Geometric Algebra Transformer</strong> <d-cite key="brehmer2023geometric"> </d-cite>: This paper is super cool and really well written. It’s a fairly non-traditional work that enforces equivariance with respect E(3) <d-footnote>https://en.wikipedia.org/wiki/Euclidean_group</d-footnote>, which is all linear combinations of translations, rotations, and reflections of 3D Euclidean space. This is particularly useful for learning representations of geometric data, and they apply their Transformer to a downstream planning task and demonstrate that it still works even when we don’t want to enforce this constraint.</li>
<li><strong>Pretraining Task Diversity and The Emergence of Non-Bayesian In-context Learning for Regression</strong><d-cite key="raventós2023pretraining"> </d-cite>: We’ve always been curious how well models generalize to information not present in the training data, but this paper takes this question with a bit more abstraction, examining the effectiveness of in-context learning on tasks not present in the training data. They propose a “task diversity threshold” and claim that in-context learning emerges if the pre-training data is sufficiently diverse.</li>
<li><strong>Fast Attention Requires Bounded Entries</strong><d-cite key="alman2023fast"> </d-cite>: Theoretically-motivated result on why we want entries in the attention matrix to be relatively small with respect to the matrix size if we want to speed up computations through approximation algorithms. It’s a pretty neat work, and I don’t necessarily think it’s intuitively obvious why this holds.</li>
<li><strong>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment</strong><d-cite key="ni2023transformers"> </d-cite>: Since Transformers are now appearing in almost every non-compute-sensitive task, it’s interesting to understand why they work so well. This paper looks into the performance model-free RL agents on specific tasks designed to evaluate long-term memory and the efficiency of credit assignment. They find that (rather unsurprisingly), Transformers are useful for storing in-episode memory, but they do not solve the long-standing credit assignment problem, which means they are not the key to solving RL.</li>
<li><strong>MotionGPT: Human Motion as a Foreign Language</strong><d-cite key="jiang2023motiongpt"> </d-cite>: We have generally settled on the notion that discrete tokenized representations are quite nice for Transformers, and this paper takes that one-step further by tokenizing human motion frames. The use-case is interesting, but I’m also interested in extending this kind of tricks for arbitrary modalities. Maybe we can see Transformers used for just about any kind of prediction!</li>
</ol>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>Reinforcement Learning was a huge topic this year, with many papers discussing RL in the context of other works like diffusion models <d-cite key="he2023diffusion,kang2023efficient"> </d-cite> or in-context learning <d-cite key="lee2023supervised"> </d-cite>. There is a distinction between classical RL works and deep RL works, the former of which are primarily theoretical, and the latter of which are primarily empirical. The main difference is the use of a neural network to approximate tabular mappings, especially in settings involving an infinite or combinatorially large state and/or action space. I’m not entirely sure what direction the field has been in the past few years because it is so broad, but I have noticed an emphasis on sample efficiency and the utilization of priors to accelerate RL exploration. This is probably because earlier successes in deep RL have primarily been through OpenAI or DeepMind brute forcing domain-specific trajectories into a model, so we have proven that deep RL works and can focus on efficiency. Lastly, I noticed a lot of papers related to offline RL <d-cite key="chen2023conservative,hong2023beyond"></d-cite>, where models make updates without interacting with the environment.</p>
<ol>
<li><strong>Robustness.</strong> I’m specifically referring to the robustness of RL training algorithms and preventing failure modes. It is fairly well-known that RL is quite delicate and requires a lot of tricks <d-cite key="rlblogpost"> </d-cite> to get working in practice, so it is unsurprising that a lot of work goes into improving the robustness of these algorithms. There are many failure modes of RL that are addressed in this conference, and I highlight them below:
<ol>
<li>Balancing the ratio of updates to timesteps is important for trading off convergence and sample efficiency. To prevent <em>primacy bias</em> (favoring early explorations), deep RL methods often perform a “reset” of their weights while storing the transition data in a replay buffer. Doing this can cause the model to diverge on reset, so <d-cite key="kim2023sampleefficient"></d-cite> attempt to circumvent this issue by using an ensemble of agents and perform random resets so at least one agent is not reset.</li>
<li>We often want RL agents to act <em>conservatively</em>, so when they reach an unseen state, they do not act wild. In <d-cite key="chen2023conservative"></d-cite>, they add penalties to out-of-distribution states and prove that in an offline RL setting, they achieve a conservative estimation of the expected value function.</li>
<li>Deploying RL in realistic settings often implies the need for safety constraints to avoid exploring unsafe states. Works like <d-cite key="wachi2023safe,kim2023sampleefficient"></d-cite> look into imposing these constraints with high probability.</li>
<li>There are many tricks involved in RL training, many of which are problem-dependent. For example, importance weighting over the training dataset in offline RL to prevent being influenced by low-reward trajectories <d-cite key="hong2023beyond"></d-cite>. Or reducing variance in the learning process with multi-step surrogate rewards <d-cite key="zhong2022long"></d-cite>. In <d-cite key="ma2023learning"></d-cite>, they analyze why augmenting visual observations leads to better sample efficiency. Some works even investigate tricks from other models like reward scaling and regularization schemes and apply them more generally <d-cite key="sullivan2023reward"></d-cite>.</li>
</ol>
</li>
<li><strong>Improving Exploration by Leveraging Priors.</strong> Reward functions play a huge role in the convergence of RL training. In most settings, the only “true” reward is a sparse reward given for achieving a goal (e.g. winning a chess game). However, propagating this sparse reward through a combinatorially large state and transition function space is fundamentally difficult, so people often design intrinsic reward functions to better guide an agent towards maximizing the true reward. Some methods attempt to generally identify good transitions and states to explore <d-cite key="lin2023mimex,jain2023maximum"></d-cite> by looking for state diversity and orthogonality, while other methods focus on automating a reward designer’s intent by conditioning on images, videos, or language through some pre-trained embedding space and push exploration in a certain direction using these exploration “hints” <d-cite key="kim2023guide,escontrela2023video,wu2023read,gupta2023behavior"></d-cite>. Meanwhile, <d-cite key="nikishin2023deep"></d-cite> propose that even with sufficient exploration, a model may not learn a good policy, so dynamically modifying the model during training may fix that!</li>
<li><strong>Learning World Models for Model-based RL (MBRL).</strong> If you’re familiar with the basic formulation for the dynamic programming update functions used in RL, you’ll know that knowing the <em>transition function</em>, or the model of the environment dynamics, is an extremely powerful guarantee that you generally do not have. However, it is possible to try and “learn” this model to apply MBRL methods. One of my research interests as of late is learning and editing world models using language, so it is pretty exciting to see these works. A central theme in the use of world models is learning and acting “in imagination”, which means treating the world model itself as an environment that the agent can interact in, which is especially useful for environments where interacting is costly or dangerous. In <d-cite key="chung2023thinker"></d-cite>, they use the world model as a way for the model to “think” before it generates an action. In <d-cite key="guan2023leveraging"></d-cite>, they learn strictly formatted world models in a standardized language <d-footnote>https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language</d-footnote> that models can interact with. Furthermore, many world models use an recurrent neural network (RNN) as the backbone for historical reason, so in <d-cite key="deng2023facing"></d-cite>, they experiment with different models for the world model backbone and propose their own with better empirical performance.</li>
<li><strong>Multi-agent RL.</strong> Multi-agent RL (MARL) involves multiple learning agents in the same environment, with most papers I’ve found generally focusing on cooperative or mixed-sum games. A lot of similar research themes in RL apply here as well, but there is also room for game-theoretic analysis. A core limitation in prior MARL work is the assumption of a fixed task and fixed environment, so works like <d-cite key="mao2023multiagent,wang2023mutualinformation"> </d-cite> extend standard frameworks to a multi-task learning setting. There are also works regarding how to leverage reward signals that apply to the team of agents versus a singular agent <d-cite key="yang2023hierarchical,hu2023differ"></d-cite> which is not obvious and directly influences the learned policy of each agent. Finally, because MARL is inherently expensive (you’re launching several agents at once!), creating environments that are compute-friendly is a topic of interest <d-cite key="lechner2023gigastep,suarez2023neural"></d-cite>.</li>
<li><strong>Goal-conditioned RL.</strong> Goal-conditioned RL (GCRL) is a class of algorithms or problems where decisions are conditioned on both a state and a goal. There were only a few GCRL papers at this conference with an emphasis on offline RL strategies such as using sequence modeling over trajectories as a goal <d-cite key="zeng2023goalconditioned"></d-cite>, but I did find an interesting work on using the distance (they define their own metric) between the goal distribution and state visitation distribution as an extra reward signal for exploration <d-cite key="agarwal2023fpolicy"></d-cite>.</li>
<li><strong>Theoretical Analysis.</strong> There were a lot of papers on bandit problems (especially adversarial or contextual bandits) <d-cite key="olkhovskaya2023first"></d-cite> and provable regret/convergence bounds <d-cite key="whitehouse2023on"></d-cite>, most of which I was not really able to understand. While I think following the math itself and reading through it is quite fun, I’m just not familiar with what problems people are interested in and what is unsolved, so it’s hard to gather from the abstracts or even a quick skim of the papers what the immediate impact is. That’s not to say that these works are not interesting or useful, but I am going to be careful not to say something false about their results. However, I did find two works that prove convergence guarantees for <em>deep RL</em>!. In <d-cite key="zhang2023convergence"></d-cite>, they prove convergence guarantees for deep Q learning using $\epsilon$-greedy exploration under accelerated gradient descent (momentum) under some pretty minimal assumptions. In <d-cite key="gaur2023global,zhong2023theoretical"></d-cite>, they limit their analysis to linear MDPs and simplified neural networks, but show convergence guarantees for actor-critic and proximal policy optimization (PPO) methods respectively.</li>
</ol>
<h3 id="rl-interesting-papers">[<span style="color:orange">RL</span>] Interesting Papers</h3>
<ol>
<li><strong>Conditional Mutual Information for Disentangled Representations in Reinforcement Learning</strong> <d-cite key="dunion2023conditional"> </d-cite>: I haven’t really seen prior works in RL that try to tackle disentanglement in feature representations, but the motivating factor here is that the exploring RL agent does not sufficiently capture the environment dynamics and instead learns spurrious feature correlations. I’m not sure if the technique they used was done for image/video works in the past, but it makes sense to me that this is not an RL or agent-specific technique.</li>
<li><strong>Creating Multi-Level Skill Hierarchies in Reinforcement Learning</strong> <d-cite key="evans2023creating"></d-cite>: When I was playing around with PySC2 (Starcraft II RL environment), I used to always be confused how an RL agent would feasibly learn these complex chains of actions (turns out the answer was tons of data). Another approach outlined in this paper is to explicitly map out hierarchical skill trees, where the lowest levels are explicit actions and higher levels are more abstract, learnable skills. I’ve seen a similar idea applied to LLMs where you can explicitly query the LLM to reason about what it should do, but in RL its more robust but less interpretable.</li>
<li><strong>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</strong><d-cite key="chen2023efficient"></d-cite>: An interesting question in RL is how much “error” is induced by a partial loss in observability. They bound the worst-case performance on control systems depending on the expected percentage of missing states and show that RL is still applicable to this class of problems in an efficient way (which generally means poly() any environment parameters).</li>
<li><strong>Learning to Influence Human Behavior with Offline Reinforcement Learning</strong> <d-cite key="hong2023learning"></d-cite>: I feel like in any multi-agent or game theoretic setup, we always assume other players are playing optimally. This paper is unique in that they try to learn a policy in a cooperative multi-agent setup that assists the other agent towards a certain desirable behavior. The environment is a grid-world version of Overcooked <d-footnote>https://www.team17.com/games/overcooked/</d-footnote> which I find really funny, as this is the perfect environment for this kind of model.</li>
<li><strong>Is RLHF More Difficult Than Standard RL?</strong> <d-cite key="wang2023rlhf"></d-cite>: They reduce RLHF and general preference-based reward signals to different classes of known problems in RL, motivating how RLHF is not inherently more difficult than standard RL problems. The paper goes into quite a few instances of preferenced-based RL, and is fully theoretically motivated.</li>
<li><strong>A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</strong> <d-cite key="zhong2023theoretical"></d-cite>: I’ve understood PPO as a series of empirical tricks and approximations to the theoretically motivated Trust Region Policy Optimization (TRPO), so I always thought that studying its theoretical properties to provably converge has been lacking. Even if this study is applied to a simple RL setting, it’s an important first step towards theoretically motivating PPO.</li>
</ol>
<h2 id="generative-ai">Generative AI</h2>
<p>Generative AI as a whole has also been booming through 2023, especially as a marketable product. Given how accessible and easy to customize they have gotten, I think that the common layperson should understand at a high-level what kind of generative AI is out there. I should note that I structured this section to mostly exclude large language models or language as a modality altogether. These papers have mostly been targeted towards diffusion models, although there were still a few works at NeurIPS 2023 that focused on GANs like <d-cite key="hou2023augmentationaware,yang2023learning"></d-cite>. The general themes I’ve observed are as follows:</p>
<ol>
<li><strong>Pre-trained Foundation Models.</strong> Generative models rely on a large backbone model that encodes the knowledge base of the domain that it acts on <d-cite key="wang2023facecomposer,liu2023weakly,chen2023bridging"> </d-cite>. I do wish there were some papers discussing techniques for scaling models robustly and efficiently, but perhaps it comes with experience.</li>
<li><strong>2D to 3D View Synthesis.</strong> 2D generative models are at a pretty decent state, so extending their abilities to create 3D generative models is an open research question. Prior work on novel view synthesis encode scenes in the weights of the model, but recent work has looked models that can generalize to different scenes. For example, in <d-cite key="peng2023gens"></d-cite> they train a signed-distance function (SDF) based reconstruction model to generate 3D meshes from generic 2D image views at inference time. In this field, spatial hash encodings have proven to be effective on GPU hardware for drastically speeding up 3D generative models, so <d-cite key="wang2023masked"></d-cite> enables dynamic scenes (basically add time) to be encoded by learning to selectively employ different hash encodings for static and dynamic parts of the scene. Finally, with Segment Anything (SAM) <d-footnote>https://segment-anything.com</d-footnote> being an extremely powerful 2D vision-language foundation model capable of accurate semantic image segmentation, <d-cite key="cen2023segment"></d-cite> presents a way to extend this to 3D. This process involves generating view-conditioned prompts to generate views of an image and properly inverse projecting these 2D masks back to a 3D voxelized space. This work is exciting and the results are very noisy and not great, but it’s definitely an open research problem that will make significant progress soon!</li>
<li><strong>Single-image 3D Reconstruction.</strong> An alternative to novel view synthesis is take a <em>single image</em> and try to extrapolate using domain knowledge what the 3D model looks like. Most of these methods leverage some kind of pre-trained 2D diffusion model to generate the alternate views, but they are distinct in how they choose to do this. Some do it by inferring 3D projective geometry <d-cite key="li2023generalizable,purushwalkam2023conrad"></d-cite> and others try optimizing directly with generations from a multi-view 3D diffusion model <d-cite key="liu2023one2345"></d-cite>.</li>
<li><strong>Generating on new Modalities.</strong> Generative AI is not limited to language and vision. Audio and speech generative models <d-cite key="copet2023simple,le2023voicebox,deshmukh2023pengi"></d-cite> have found that tokenized representations of other modalities can be used in Transformers. Of course, the details are not that simple, and from my understanding encoding the tokens requires working over a spectrogram representation that is non-trivial.</li>
<li><strong>Text-to-video.</strong> I’m sure you may have seen some clips of text-to-video AI on social media, and while it is impressive, it is far from being as robust as the text-to-image models. A lot of work goes into ensuring causal and cross-frame temporal consistency in these generations <d-cite key="wang2023videocomposer"></d-cite>. While these models have an obvious use case for generating videos, I did find an interesting use-case of these models as a form of planning for policy generalization <d-cite key="du2023learning"></d-cite>.</li>
</ol>
<h3 id="diffusion-models">Diffusion Models</h3>
<p>I decided to add a separate subsection on diffusion models with a focus on techniques that improve the base diffusion model process <d-footnote>As a sidenote, check out my roommate's repository on a really simple and intuitive implementation of diffusion models: https://github.com/edogariu/nice-diffusion</d-footnote>. Honestly, there were so many diffusion model papers (also applied to other fields like RL <d-cite key="he2023diffusion,kang2023efficient"></d-cite>) that this subsection alone is more rich than most of the other sections. From my understanding, diffusion models can be viewed from several different “lenses”, with one being as a Gaussian process and another being through Langevin dynamics. You can even view diffusion models as a sequence of Variational Autoencoders (VAE). For a bit of perspective, I had the opportunity to speak with Jascha-Sohl Dickstein <d-footnote>Jascha was first-author of the original diffusion models paper: https://arxiv.org/abs/1503.03585</d-footnote> through the AI Tiger Trek trip I helped organize last April, and he said that various practical formulations of the diffusion model implementation had sprung up at around the same time from these different viewpoints. There is a lot of ongoing research into using these models effectively, but here is what I noticed from this conference:</p>
<ol>
<li><strong>Inference-speed.</strong> There have been strides by labs to make the training and fine-tuning process of diffusion models cheaper <d-footnote>https://www.mosaicml.com/blog/stable-diffusion-1</d-footnote>, but inference remains quite expensive. In <d-cite key="zhao2023unipc"></d-cite>, they motivate a training-free sampling method for performing an extremely low number of sampling steps (&lt;10) while maintaining generation quality (for comparison, the standard amount is ~1000 steps and ~20 is considered low from prior works), while in <d-cite key="xue2023sasolver"></d-cite> they motivate stochastic sampling solvers and relate them to other popular solvers like the previously mentioned one.</li>
<li><strong>Interpreting the Latent feature representation.</strong> Can we understand the features that are learned by a diffusion model? This is generally done by probing the embedding space and clustering or checking if classes are linearly separable <d-cite key="zhang2023tale"></d-cite>, where they find Stable Diffusion features exhibit good spatial information but worse semantic understanding compared to another popular embedding method. Another step is to investigate and probe the latent seed space used to condition the generator, as done in <d-cite key="samuel2023normguided"></d-cite>.</li>
<li><strong>Multi-input.</strong> Similar theme to multi-modal models in general since they are so closely related, but can we develop diffusion models that take both text and visual data as input <d-cite key="vuong2023languagedriven"></d-cite> and produce any desired output<d-cite key="tang2023anytoany"></d-cite>? Can we also make it robust to composition and more complicated prompting <d-cite key="doveh2023dense,zhao2023unicontrolnet"></d-cite>?</li>
<li><strong>Filling missing information.</strong> Can we leverage diffusion models to fill in the missing gaps of information in an image or a dataset label <d-cite key="zhang2023unified,nguyen2023dataset"></d-cite>? This is actually really cool, because the implication is that unlabelled or noisy images contain enough structure to reconstruct the unknown parts without the model just making things up.</li>
</ol>
<h3 id="gen-ai--diffusion-interesting-papers">[<span style="color:orange">Gen AI + Diffusion</span>] Interesting Papers</h3>
<ol>
<li><strong>Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images</strong> <d-cite key="wen2023treering"></d-cite>: Copyright and generative AI identification is going to become increasingly more important as the technology gets more accurate. This technique slowly applies an invisible watermark during the diffusion sampling process that is easy to recover when inverting the diffusion process. I have a feeling that similar to works on adversarial attacks, there is going to be a constant chase between watermark and watermark removal works in the near future.</li>
<li><strong>Generator Born from Classifier</strong> <d-cite key="yu2023generator"></d-cite>: Can you take a trained image classifier and use it to generate images with minimal extra learning? We have seem class-conditional works like in diffusion models, but this work is trying to do something much stronger. This work is a first-step into leveraging a classifier for image generation, and they use the theory of Maximum-Margin Bias to extract training data information from the parameters of a classifier.</li>
<li><strong>CL-NeRF: Continual Learning of Neural Radiance Fields for Evolving Scene Representation</strong> <d-cite key="wu2023clnerf"></d-cite>: NeRFs implicitly store the scene they are rendering in their weights, but they are generally fixed. But what if we want to capture an ever-changing scene? It seems natural to imply concepts in continual learning, as we mainly want to 1) not forget important static elements of the scene during weight updates and 2) dynamically add components to the scene through weight updates, and this work is a first step into solving this problem.</li>
<li><strong>UE4-NeRF:Neural Radiant Field for Real-Time Rendering of Large-Scale Scene</strong> <d-cite key="gu2023uenerfneural"></d-cite>: As an avid fan of Unreal Engine and the games that have been produced by it, this is a really exciting work to me. There are companies like Luma and Volinga.ai that have a closed-source proprietary software for NeRF rendering in Unreal Engine, but this is the first work that open-sources it. I should note that their rendering process involves rendering sub-NeRFs in a partionined volume for efficiency purposes, but otherwise it follows a ray-marching procedure (ish).</li>
</ol>
<h2 id="computer-vision">Computer Vision</h2>
<p>Computer vision (CV) was the field that introduced me to the world of machine learning, and I had a glimpse when I was a little boy building robots with my Arduino of what it looked like pre-AlexNet. Perhaps because NeurIPS itself is not focused on CV, I was rather surprised by the themes I noticed. There seems to be a much larger emphasis on video understanding and human-centric perception, although fairness and bias still remains an issue that has yet to be addressed.</p>
<ol>
<li>
<p><strong>Open vocabulary methods</strong> for segmentation and understanding of semantics in images involves being able to adapt to labels unseen during training. In essence, we want to be able to generalize our models past fixed class labels so they don’t have to be re-trained every few months. My understanding is that with vision-language models being a thing now, these methods only need to generate suitable embeddings use with these methods <d-cite key="cui2023open"></d-cite>. A lot of works now focus on extending to 3D models as well <d-cite key="liu2023weakly,cao2023coda,vobeck2023popd"></d-cite>. I’m curious though how these methods handle language ambiguity and different abstractions of describing something, and whether or not this limitation is bottlenecked by the model’s language capabilities. I did find <d-cite key="wang2023hierarchical"></d-cite> that tries to address this in a hierarchical way.</p>
</li>
<li>
<p><strong>Video understanding.</strong> I worked on a video understanding benchmark so I’m somewhat aware of the limitations in the field. Generally, video labels are quite difficult to procure, as they’re far more compositional and free-form, and they just take longer. At the same time, having models that can reason over videos is extremely useful because videos are the primary form of media consumed on the internet these days.Compared to language, videos take up much more memory, and finding associated labels through online scraping is hard. So an important work is to build up datasets and benchmarks<d-cite key="yang2023vidchapters7m"></d-cite>. Additionally, even with multi-modal models, they have not been sufficiently trained to understand temporal aspects of a video like actions and long-term causal reasoning, which works like <d-cite key="wang2023paxion,yu2023selfchained"></d-cite> make first steps towards addressing.</p>
</li>
<li>
<p><strong>Human Data and Perception.</strong>
I noticed some interest in human-centric perception, as we ideally want vision models to understand similarity the way we intuitively perceive it. In <d-cite key="fu2023dreamsim"></d-cite>, they propose a margin loss that shapes the embedding space based on human similarity judgement data. Meanwhile, in <d-cite key="tan2023egodistill"></d-cite>, they focus on ego-centric data (video footage from the perspective of a human). We also want models to be more accurate when perceiving humans, which <d-cite key="yuan2023hap"></d-cite> argues starts at the pre-training level. Lastly, there were a few papers on human-pose estimation, mainly for robustness on expressive poses <d-cite key="enpang2023robust,cai2023smplerx"></d-cite> and for improving accuracy on reconstructing the poses in 3D <d-cite key="zhao2023single"></d-cite>.</p>
</li>
<li>
<p><strong>Fairness.</strong> Fairness and bias is a long-standing issue in computer vision that is primarily rooted in dataset selection. A key research question is understanding which factors like human appearance <d-cite key="schumann2024consensus"></d-cite> or geographical location <d-cite key="gustafson2023pinpointing"></d-cite> are biased in our data. A further question is how to augment our data in the short term to mitigate these biases <d-cite key="teo2023measuring"></d-cite>.</p>
</li>
</ol>
<h3 id="computer-vision-interesting-papers">[<span style="color:orange">Computer Vision</span>] Interesting Papers</h3>
<ol>
<li>
<p><strong>Segment Everything Everywhere All at Once</strong> <d-cite key="zou2023segment"></d-cite>: This is Microsoft’s alternative to Meta’s Segment Anything (SAM) model, with a focus on semantic-oriented text prompting for segmentation. I haven’t had the opportunity to compare the two, but they claim that their method captures semantics more accurately.</p>
</li>
<li>
<p><strong>Diversifying Spatial-Temporal Perception for Video Domain Generalization</strong> <d-cite key="lin2023diversifying"></d-cite>: When you build a video understanding model, you of course want it to generalize to unseen domains. For video domains, however, which are high-dimensional and contain a lot of complicated structure, unless the data is perfectly diverse (requiring a lot of video data), you want to be able to filter out domain-specific cues from your training data and identify domain-invariant cues that will help as a prior for generalization. This work attempts to motivate how to identify these cues at a spatial and temporal level, and I think ideas from this work can be extended to other fields as well.</p>
</li>
<li>
<p><strong>DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions</strong> <d-cite key="wang2023droppos"></d-cite>: Empirically, we have found that Vision Transformers (ViT) kind of suck at understanding positional encodings, i.e. they are sort of position invariant. In some cases this is a desirable property, but we do want these Vision Transformers to be spatially aware, so this paper offers a simple fix: in addition to standard ViT training, add a secondary objective to predict the position of the token/patch.</p>
</li>
<li>
<p><strong>Patch N’ Pack: NaViT, A Vision Transformer for Any Aspect Ratio and Resolution</strong> <d-cite key="dehghani2023patch"></d-cite>: They train with token packing strategies used for language models, which involves feeding in multiple images (in tokenized patches) with varying resolutions during train time. They claim it works for arbitrary image resolutions, but I’m pretty sure it’s the resolution change they used during training. Regardless, it is an extremely useful work that applies to a wide range of visual tasks.</p>
</li>
<li>
<p><strong>Color Equivariant Convolutional Networks</strong> <d-cite key="lengyel2023color"></d-cite>: I’m a big fan of equivariance as an inductive bias, and this paper is no exception. We generally want to separate geometry and color in visual models, and this work builds a plug-in block for common convolutional neural network architectures to add color equivariant convolution operations. These layers are not insensitive to color variation; rather, they allow for sharing information about visual geometry across different colors.</p>
</li>
</ol>
<h2 id="adversarial-attacks-and-model-poisoning">Adversarial Attacks and Model Poisoning</h2>
<p>Generally, adversarial attacks can be partitioned into two main classes: white box, where an attacker has access to the model weights (e.g. for any open-source models), and black-box, where an attacker can only use model outputs (e.g. attacking GPT4). I still think that most attacks are pretty domain-specific, so I’ve decided to generally separate the themes based on domain (e.g. language, vision, RL) rather than the type of attack (e.g. red-teaming, gradient-based, etc.) Lastly, this section is primarily dedicated to attacks that alter or manipulate the outputs of a model to be harmful or incorrect. There is another class of attacks that try to reconstruct training data using model outputs, but I decided to move that to the <a href="#privacy-and-federated-learning">section on Privacy and Federated Learning</a>.</p>
<ol>
<li><strong>Robustness vs. Performance &amp; Speed</strong> is an important tradeoff when developing models and considering defense mechanisms against attacks. Adversarial defenses have extra overhead, especially those with certification (provable robustness within $\epsilon$-ball), so it is important to understand this tradeoff. I was only able to recall <d-cite key="mao2023taps"></d-cite> in this conference that tackles this issue.</li>
<li><strong>Model Poisoning.</strong> Distinct from the other attacks, model poisoning involves slightly editing the training data to plant exploits or backdoor triggers into a model <d-cite key="shu2023exploitability"></d-cite>. It is possible that poisoned models are deployed in the wild, so you may not even have access to modify its internals. So essentially, you can either fine-tune the model <d-cite key="zhu2023neural,tang2023setting"></d-cite> or directly augment its outputs with noise <d-cite key="shi2023blackbox"></d-cite> to remove the poisoning.</li>
<li><strong>Attacks on LLMs.</strong> Given the theme of multi-modal models, there has been a few works examining defenses for vision-language models against known attacks for language or vision models <d-cite key="yin2023vlattack,zhao2023evaluating"></d-cite>. However, given the discrete nature of token representations, simple black-box attacks using seemingly harmless tokens like an exclamation point are possible <d-cite key="wang2023punctuationlevel"></d-cite>. Lastly, attacks and defenses against model poisoning were discussed <d-cite key="shu2023exploitability,tang2023setting"></d-cite>.</li>
<li><strong>Attacks on Images.</strong> Unlike language, raw image representations are high-dimensional and therefore easily susceptible to noise and perturbation effects. In <d-cite key="gao2023perturbation"></d-cite>, they propose a noise generator that transfers black-box attacks from one image model to another. These attacks and defenses have evolved over the years, but it’s still an open research question even on older datasets like ImageNet <d-cite key="singh2023revisiting"></d-cite>.</li>
</ol>
<p>As an aside, there were lots of adversarial attack papers this year using specific attacks, targetting specific models (e.g. MARL <d-cite key="liu2023efficient"></d-cite>, federated learning <d-cite key="zhang2023afl"></d-cite>, graph neural networks <d-cite key="gosch2023adversarial"></d-cite>), or proposing specific defenses that are not reflected in the points above. I had a lot of trouble trying to categorize this section properly because of how diverse it is. This field is naturally reactive, as when someone comes up with a defense, someone will come up with an exploit (e.g. <d-cite key="kang2024diffattack"></d-cite>), and vice-versa. Some works even try to theoretically motivate the nature and existence of completely robust models like <d-cite key="pal2023adversarial"></d-cite>, but overall, it was hard for me to pinpoint the direction of these works at this conference.</p>
<h3 id="adversarial-attacks-and-model-poisoning-interesting-papers">[<span style="color:orange">Adversarial Attacks and Model Poisoning</span>] Interesting Papers</h3>
<ol>
<li>
<p><strong>Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypot</strong> <d-cite key="tang2023setting"></d-cite>: The strategy in this paper is really cool: basically, they first notice that backdoor triggers in poisoned models are “obvious”, in the sense that they appear in lower layers of the model in an obviously linearly separable way. Intuitively, this makes sense, as poisoned outputs are structurally out-of-distribution from “human language”. From here, they basically add these small “honeypot” layers (just a 1-layer transformer) with a classification head that purposefully get “poisoned” early on, and they use this auxiliary loss to weight the actual cross entropy loss. I think that is a really neat example of exploiting structure and abstract representations of data to achieve an effect.</p>
</li>
<li>
<p><strong>Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features</strong> <d-cite key="zhu2023neural"></d-cite>: Poisoned data generally looks like a regular image with some small perturbations or tiny trigger features, so this work looks into inserting learnable filters into a trained model that reverses and removes these features while acting as an identity map for everything else. The main issue I see with this approach is that you have to know the type of adversarial attacks against the model a priori, so a fixed filter needs to be updated when new attacks arise, i.e. there are no provable guarantees against general adversarial perturbations.</p>
</li>
</ol>
<h2 id="knowledge-distillation-and-memory-reduction-schemes">Knowledge Distillation and Memory Reduction Schemes</h2>
<p>As much as we like scaling models, building smaller models that can run on accessible hardware is extremely important for the growth of our community. This section is mostly referring to scaling down models so they can run <strong>on inference time</strong> on smaller hardware, which is a matter of memory efficiency. TinyML <d-footnote>A whole field of study is on ultra-low power ML: https://www.tinyml.org</d-footnote> works take it a step further and try to deploy these models on embedded systems and micro-controllers <d-cite key="liang2023mcuformer"></d-cite>. The three primary methods are weight quantization<d-footnote>Weight quantization involves using a lower-precision datatype for representing weights, which can reduce memory complexity by a multiplicative factor.</d-footnote>, pruning<d-footnote>There is extensive literature on network pruning, and it is actually quite complex. Pruning is literally taking out parts of the network (hence the name), but choosing what to take it is important. It is also important to select the time that you prune (before training, during training, after training), as this affects the performance and overhead of the pruning process. </d-footnote>, and knowledge distillation<d-footnote>Knowledge distillation involves taking a larger model and cloning its behavior in a smaller model, also known as "distilling". The idea is that a large models are often over-parameterized (there are benefits for training in this way), so once we have the model trained, we can cut down on its capacity by training a smaller model.</d-footnote>.</p>
<ol>
<li><strong>Memory Reduction Techniques.</strong> We generally are intered in tradeoffs for different memory reduction schemes such as pruning and quantization. In <d-cite key="kuzmin2023pruning"></d-cite>, they claim that quantization is almost always better unless you care about extreme compression. Similar to older work doing weight quantization for MLPs and CNNs, newer works at this conference do it for transformers <d-cite key="xi2023training,dong2023packqvit"></d-cite>.</li>
<li><strong>Lottery Ticket Hypothesis (LTH)</strong><d-footnote>The lottery ticket hypothesis is the notion that dense neural networks contain a much smaller subnetwork that accounts for most of the performance. Finding these subnetworks through pruning would, in theory, preserve performance while significantly reducing memory. Read more from the original paper: https://arxiv.org/abs/1803.03635 </d-footnote><strong>.</strong>  Following the original LTH, we want to understand what metrics (e.g. weight magnitude, gradient flow) and structure are useful for pruning modern architectures like LLMs, but this also depends on what we are pruning for. In <d-cite key="kurtic2023ziplm"></d-cite> they prune LLMs based on run-time bottlenecks for inference speed-ups, while in <d-cite key="ma2023llmpruner"></d-cite> they focus on shrinking the model. Meanwhile, in convolutional neural networks, pruning based on empirics has been widely studied, so <d-cite key="dacunha2023polynomially"></d-cite> provide some theoretical motivation into better pruning based on the structure of the model.</li>
<li><strong>Knowledge Distillation</strong>. Knowledge distillation (KD) is an approach for trying to force a small student model to mimic the output probabilities of a larger teacher model. KD has seen a wide array of techniques being used to preserve functionality of the teacher in the student, and many empirical experiments have been done in the past to evaluate the lower-bound capacity of student models. Nevertheless, the works at this conference were pretty unique. In <d-cite key="huang2023knowledge"></d-cite> they observe that student models have noisier features and attempt to de-noise them using diffusion. In <d-cite key="gupta2023concept"></d-cite>, they motivate “concepts” in intermediate layers as an auxiliary signal for distillation. Finally, <d-cite key="ojha2023knowledge"></d-cite> investigates whether properties like adversarial robustness, invariances, and generalization are transferred effectively during distillation.</li>
</ol>
<h3 id="memory-reduction-interesting-papers">[<span style="color:orange">Memory Reduction</span>] Interesting Papers</h3>
<ol>
<li><strong>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</strong> <d-cite key="liang2023mcuformer"></d-cite>: They push the modern limits of Vision Transformers on ultra-low cost systems, using neural architecture search (NAS) to search for a compute-optimal architecture while also writing a library for performing each inference-level computation in a Vision Transformer efficiently. I’m not that aware of the pre-existing literature in this space, but this is one of the first papers I’ve seen do it for Vision Transformers.</li>
<li><strong>What Knowledge Gets Distilled in Knowledge Distillation?</strong> <d-cite key="ojha2023knowledge"></d-cite>: Knowledge distillation is sort of this black-boxy approach where we try to get a small student model to be the same as a larger teacher model. It would be nice to know what kind of information easily transfers and even nicer to understand why, which this paper attempts to do. Most surprisingly, they find that <em>white-box vulnerabilities</em> in a teacher model transfer over to a student model despite being a different parameterization, which might be indicative of some structural similarities inherent to networks (it is inconclusive in this paper though). They try to motivate this transfer by a dimensionality argument to argue that the student model solution is unique, but honestly the argument is pretty weak because the assumptions are just generally untrue in almost any realistic problem where knowledge distillation is applied.</li>
<li><strong>Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Lottery Tickets</strong> <d-cite key="dacunha2023polynomially"> </d-cite>: So I was really curious about this paper after reading through the abstract, because was is completely unclear to me how the Random Subset Sum problem <d-footnote>Subset sum is a classic NP-hard problem in CS theory where a program must decide if there exists a subset of a set of integers that sums to a number $T$. The randomized version is a set of random variables, and the sum can now be off by an error $\epsilon$ with high probability.</d-footnote> has anything to do with the existence of strong lottery tickets in an over-parameterized convolutional neural network.</li>
</ol>
<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>Graph neural networks (GNN) were really popular this year! I wish I had a stronger understanding overall of GNNs, but unfortunately I just haven’t found any specific use cases for them in my own research. I am aware of their use-cases in structured prediction (e.g. molecular dynamics <d-cite key="wu2023equivariant"></d-cite>, social networks) but their unique design and the prevalence of graph problems has allowed this field of research to grow steadily. I couldn’t really pinpoint the major themes at this conference, but I learned a few things about what people are interested in.</p>
<ol>
<li><strong>Heterophily vs. Homophily.</strong> Earlier works with GNNs worked under the assumption of graph homophily, meaning similarly labelled nodes tend to be linked. This assumption neatly allows for even unsupervised methods to exploit graph structure when making predictions, but it is unclear what the impact of graph heterophily is on GNN performance <d-cite key="luan2024graph"></d-cite>. Thus, there has been work towards solving graphs under heterophily by focusing on non-local structure <d-cite key="liang2023predicting,liao2023ld"></d-cite>. In <d-cite key="platonov2023characterizing"></d-cite>, they even try to rigorously characterize the properties and effects of these node-level relationships.</li>
<li><strong>Unsupervised graph learning.</strong> Unsupervised learning is natural in graph problems because regardless of the problem domain, the graph itself provides extremely useful structural information that can be leveraged for a prediction. There is still a lot ongoing research <d-cite key="tsitsulin2023graph,qiao2023truncated,sun2023lovsz"></d-cite> into identifying and targetting useful structure in graphs, which includes (1).</li>
<li><strong>Spatio-temporal prediction</strong> involves time-series forecasting over spatially-varying data. This problem is significantly harder than stock prediction type forecasting over tabular data because of the inherent high dimensionality and structure (local vs. global) present in spatial data. Thus, a class of works this year <d-cite key="wu2023equivariant,xia2023deciphering,cini2023taming"></d-cite> have emerged to study these problems using GNNs.</li>
<li><strong>Encoding representations in graphs.</strong> Typically GNN methods represent nodes or links with some kind of embedding representation, so understanding the mechanisms that shape these representations is important <d-cite key="wu2023demystifying"></d-cite>.</li>
</ol>
<p>Broadly speaking, a lot of advances in other fields that were discussed above are also active areas of research in GNNs (e.g. adversarial robustness <d-cite key="gosch2023adversarial,zhao2023adversarial"></d-cite>, interpretability <d-cite key="yin2023train"></d-cite>, multimodal <d-cite key="zhao2023gimlet"></d-cite>), so I expect to see a lot more advancements and use-cases of GNNs in the near future.</p>
<h3 id="gnn-interesting-papers">[<span style="color:orange">GNN</span>] Interesting Papers</h3>
<ol>
<li><strong>Zero-One Laws of Graph Neural Networks</strong> <d-cite key="adamday2023zeroone"></d-cite>: Zero-one laws generally study the limiting behavior of probabilities and show that they converges to $0$ or $1$. This paper proves equivalent zero-one laws for the outputs of certain classes of GNNs (e.g. boolean graph convolutional classifiers) as they get larger and larger. Practically speaking, I don’t currently see a use case for this kind of analysis, but it is cool nonetheless.</li>
<li><strong>Unsupervised Learning for Solving the Travelling Salesman Problem</strong> <d-cite key="min2023unsupervised"></d-cite>: They use a simple unsupervised graph neural network with surrogate loss objectives that provably move towards the objective, that being minimizing the path cost and ensuring the path is a Hamiltonian cycle. I’m really curious to see future GNN works on approximating solutions to NP-hard/NP-complete problems based on derived surrogate objective functions.</li>
<li><strong>Lovász Principle for Unsupervised Graph Representation Learning</strong> <d-cite key="sun2023lovsz"></d-cite>: Math researchers have done lots of incredible work in study global and local properties of graphs, and I expect that we will continue to see these results be useful in GNNs. Unsupervised learning for graph neural networks makes so much sense, because so much structure comes from the graph itself regardless of the domain it is describing. Having learned about Lovász numbers in an extremal combinatics course taught by Professor Alon Noga himself, it was cool to see them re-appear in an ML setting.</li>
</ol>
<h2 id="privacy-and-federated-learning">Privacy and Federated Learning</h2>
<p>Trust in AI and the companies that build these AIs is extremely important. This year’s conference had a strong emphasis on privacy and data protection methods, as well as federated learning methods <d-footnote>I would recommend read a survey paper or some online notes for a better explanation, but the basic idea behind federated learning is that in a distributed or cloud setting, we often want to use training data from clients (e.g. data on your mobile device), but we don't want to actually transfer this data to a centralized server for privacy reasons. Instead, we train a copy of the mobile locally, then transfer the gradients over to a server. Doing this at scale is quite difficult, as we are essentially doing sequences of delayed gradient updates. </d-footnote>. Privacy is a fairly math-heavy topic (especially outside of ML) because it often considers worst-case scenarios with high probability, so a lot of the papers in this domain are quite technical.</p>
<ol>
<li><strong>Differential Privacy.</strong> Data privacy and anonymity can be mathematically guaranteed under differential privacy (DP) constraints, so adding these DP mechanisms to deep learning models with minimal overhead is an active area of research. Because DP is so mathematically sound, some work goes into studying DP under conditions common in machine learning <d-cite key="knop2023counting,jiang2023gaussian,ghazi2023userlevel"></d-cite> while others go into applying DP to machine learning problems <d-cite key="fan2022kmedian,qiao2023offline"></d-cite>.</li>
<li><strong>Machine Unlearning</strong> looks into removing sensitive information that was present in a trained model’s training distribution, effectively wiping the information from a model altogether <d-cite key="kurmanji2023unbounded,chen2023fast"></d-cite>. These techniques are useful for combatting copyright issues, but they are not well understood <d-cite key="jia2024model"></d-cite> and can even lead to exploits <d-cite key="di2022hidden"> </d-cite>.</li>
<li><strong>Client attacks on Federated learning.</strong> Federated learning involves lots of gradient information from different worker sources. If an attacker got a hold of some workers (e.g. a malicious mobile device user), they could, in theory, inject harmful information into a federated learning system (similar to an adversarial attack, formally called a Byzantine attack). It is far easier in a federated learning setting for attackers to become clients, so many studies look into poisoning attacks <d-cite key="nguyen2023iba,zhang2023afl"></d-cite> and robust defenses against them through things like trust scores <d-cite key="yan2023recess"></d-cite>, zero order optimization with DP guarantees <d-cite key="wang2023a"></d-cite>, and measuring divergence from the average <d-cite key="zhang2023fedfa"></d-cite>.</li>
<li><strong>Failure modes of federated learning.</strong> Federated learning does gradient updates out of sync, which means 1) theoretical analysis is a serious pain and 2) failure modes are more apparent. Furthermore, with extra mechanisms for privacy, an open research question is understanding the convergence guarantees of federated learning under various techniques and mechanisms <d-cite key="chen2023finegrained,zhou2023every">.</d-cite></li>
</ol>
<h3 id="privacy-and-fed-learning-interesting-papers">[<span style="color:orange">Privacy and Fed. Learning</span>] Interesting Papers</h3>
<ol>
<li><strong>Privacy Auditing with One (1) Training Run</strong> <d-cite key="steinke2023privacy"></d-cite>: We generally have to <em>prove</em> that an algorithm is differentially private (which is too hard in most cases!), but there are ways to audit or inspect empirically if an algorithm is differentially private. The problem is that DP is a probabilistic guarantee about the inclusion and exclusion of any data point, so we have to sample taking out data points. But sampling in the DP sense means re-training with or without data, which is extremely expensive. This work remarkably shows that they can audit with O(1) training runs under provable guarantees, which is a huge step from prior works. I skimmed the theoretical work, and it seems that they show the desired concentration bound of their method by showing that their process is stochasticly dominated by a binomial (a trick which appeared on my probability theory PSET!), and I’m excited to sit down and go through the math when I get the chance. Oh also, this paper won Outstanding Paper at this year’s conference.</li>
<li><strong>Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training</strong> <d-cite key="huang2023lockdown"> </d-cite>: In federated learning, we want to defend against bad actors. But because we are adding gradients from many different sources to a centralized model, it is often hard to identify the source of these bad actors. In this work, they explicitly put sparsity constraints on the client to enforce training over a subspace of their data. The hope is that because these subspaces are generally disjoint, bad actors will not make updates in subspaces that good actors work over, making them easier to identify because they are isolated.</li>
<li><strong>Training Private Models That Know What They Don’t Know</strong> <d-cite key="rabanser2023training"></d-cite>: I think this paper is a pretty simple example of the type of performance and computational overhead that privacy constraints can induce. I’m hoping to see these kinds of works extended to larger models and more modern datasets, but they’re nonetheless very important.</li>
</ol>
<h2 id="datasets-benchmarks-challenges">Datasets, Benchmarks, Challenges</h2>
<p>As NeurIPS is an AI-centric conference, there were datasets, benchmarks, and challenges for every topic above. There’s even a Datasets and Benchmarks track at NeurIPS. The more popular topics had more datasets (multimodal, LLM, etc.) and the datasets reflect the current needs of each field. A lot of the references put in earlier sections are dataset papers, so this section is going to be dedicated instead to some interest datasets I found while going through.</p>
<p>One thing I noticed though was a few papers on using <strong>synthetic data</strong> <d-cite key="yang2023freemask,pronovost2023scenario,ntavelis2023autodecoding,zhu2023genimage"></d-cite>! While these are not dataset papers, they seem to imply that synthetic data works well enough for training! I’m curious to see if synthetic datasets will become more prevalent, especially given how easy they are to scale.</p>
<h3 id="interesting-datasets">Interesting Datasets</h3>
<ol>
<li><strong>Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text</strong> <d-cite key="zhu2023multimodal"></d-cite>: This is the multi-modal variant of the original C4 dataset, which has been a standard in LLM pre-training since its release. Because of how prevalent this dataset is going to be, I think it’s at worth at least taking a look at the data that’s going to be a part of most of our generative AI in the near future.</li>
<li><strong>BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks</strong> <d-cite key="milani2023bedd"></d-cite>: MineRL is really cool. If you haven’t seen it already, I highly recommend taking a look, as Minecraft is the type of game that you would expect to be extremely complex for an AI to understand, but also simple enough that it seems feasible to eventually solve. This dataset provides a suite of labelled frame-action pairs and human labels that have been collected over the past two years and is extremely valuable for researchers working on this challenge.</li>
<li><strong>GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image</strong><d-cite key="zhu2023genimage"></d-cite>: This is the closest thing to a synthetic dataset that I found at this year’s conference, but their focus was explicitly on creating AI-generated discriminators. I’m actually really curious to see if someone completely AI-generated a copy of the ImageNet dataset and trained models on it, how good would these models be? What kind of special differences, if any, could we find with these models and the originals?</li>
</ol>
<h2 id="other-topics">Other Topics</h2>
<p>The following sections are dedicated to topics that were either not as popular this year but are still broadly relevant or where I could not really get a sense of the central themes surrounding them. The main issue boils down to not having enough background on the topic, so I have to go through a few papers on the subject before comprehensively understanding what they’re doing. Regardless, they each had some interesting papers to highlight.</p>
<h3 id="interpretability-and-explainable-ai">Interpretability and Explainable AI</h3>
<p>Interpretability and explainable AI is really hard. We know that deep learning models tend to be a black box, and it’s generally because their inner mechanisms are too deep and intertwined with non-linearities that unless we make strong assumptions . I’d highly recommend going through the <a href="https://transformer-circuits.pub">https://transformer-circuits.pub</a> posts (start from the bottom), as they are extremely thorough and have been updated over time as well.</p>
<p>On the topic of mechanistic interpretability<d-footnote>Mechanistic interpretability is breaking down and reverse engineering a network to completely understand the inner workings and structure. Networks are often viewed as a computational graph composed of "circuits" that perform a specific function. I highly recommend looking at https://www.neelnanda.io/mechanistic-interpretability/quickstart</d-footnote>, I don’t necessarily think the works at NeurIPS 2023 are reflective of all that is going on in the community, but there were some interesting papers to share nonetheless.</p>
<ol>
<li><strong>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer</strong> <d-cite key="tian2023scan"></d-cite> They analyze 1-layer transformers without positional encoding or residual connections, so their analysis is a bit different than some earlier transformer mechanistic interpretability works that focus on residual streams. I haven’t gotten the chance to read through their analysis carefully, but they claim that under these conditions, the attention mechanism initially attends to “distinct” (uncommon among many pairs) key tokens and continues putting weight on the highest co-occuring distinct tokens, but eventually these weights get fixed after a certain time in the training process. The idea is that common tokens (i.e. words that probably don’t really add to the semantics) are not attended, naturally filtering them out as dataset sizes increase.</li>
<li><strong>Reverse Engineering Self-Supervised Learning</strong> <d-cite key="benshaul2023reverse"></d-cite>: They do self-supervised learning over CIFAR-100 and attempt to probe the intermediate layers, using the performance of probes over the course of training to justify their claims. Their conclusion is that self-supervised learning algorithms learn intermediate representations that are clustered based on semantic classes, and they show this using the performance of probes <strong>after</strong> accurate model performance, citing regularization constraints as the key driver.</li>
<li><strong>The geometry of hidden representations of large transformer models</strong> <d-cite key="valeriani2023geometry"></d-cite>: This work attempts to uncover common geometrical patterns, mainly intrinsic dimension<d-footnote>Intrinsic dimension is the lowest dimension manifold that can approximate a dataset up to some error. The reason why we care about intrinsic dimension is that high-dimensional data is very hard to work with and significantly increases the complexity and failure modes of a learning algorithm. In practice, however, high-dimensional data like images often contain structure that leads to a low intrinsic dimension.</d-footnote> and a metric they call “neighborhood overlap”, across layers in transformer models. I am not familiar with the tool they use to measure intrinsic dimensionality and how accurate it is, but Figures 1 and 2 in their paper are pretty telling of the conclusions they draw.</li>
<li><strong>Towards Automated Circuit Discovery for Mechanistic Interpretability</strong> <d-cite key="conmy2023automated"></d-cite>: There has been quite a lot of work on studying toy networks in mechanistic interpretability, and this paper attempts to write out a concrete framework for doing mechanistic interpretability research. They then attempt to automate one of the steps, which is activation patching (varying inputs to an activation) to find circuits that exhibit a particular behavior.</li>
<li><strong>Explaining Predictive Uncertainty with Information Theoretic Shapley Values</strong> <d-cite key="watson2023explaining"></d-cite>: Shapley values <d-footnote>https://en.wikipedia.org/wiki/Shapley_value</d-footnote> are the solution to a cooperative game theory problem that satisfy a set of axioms. Informally (and related to explainable AI), they are a way of rigorously identifying which features contributed to a certain model prediction (although it is very expensive and scales poorly with training dataset sizes). In this paper, they motivate using a similar framework for measuring how training data features affect conditional entropy, which is directly linked to model uncertainty.</li>
<li><strong>Theoretical and Practical Perspectives on What Influence Functions Do</strong><d-cite key="schioppa2023theoretical"></d-cite>: Influence Functions (IF) measure the change in a model prediction when re-weighting training examples, effectively relating a model’s output behavior to the training data. More formally, for a training dataset $\mathcal{S}$, suppose we perturb the weighting of data point $x$ by $\delta$. Let $\mathcal{L}(x,\theta)$ be the loss of a model with parameters $\theta$ on datapoint $x$, and let $\theta_{x,\delta}$ be the minimizer of the perturbed dataset. For a test data point $z$, the influence function $I(z,x,\theta^*)$ is defined as</li>
</ol>
\[I(z, x, \theta^{*}) = \nabla_{\delta} \mathcal{L} (z, \theta_{x, \delta}) \biggr|_{\delta=0}\]
<p>which is precisely the change in test loss through perturbation. Of course, this expression is not that interesting, but through Taylor expansion and assumptions about the loss function, it has an even nicer closed form solution (albeit with an inverse Hessian) in terms of $z,x$ and $\theta^*$, the minimizer of the original unperturbed dataset. For the aforementioned closed form solution to make sense, a lot of assumptions have to be made, which this paper tries to break down and explain why it may fail on real problems.</p>
<h3 id="implicit-bias">Implicit Bias</h3>
<p>From a statistical learning perspective, overparameterized neural networks under gradient descent should exhibit overfitting. However, it has been shown empirically that overparameterization is generally helpful and leads to good generalization ability. Additionally, it is well known that there are many machine learning algorithms that provably generalize well on certain domains (e.g. support vector machines on linearly separable data). Implicit bias is the notion that overparameterized deep neural networks tend towards solutions that are similar to algorithms that generalize well as an explanation for their generalization ability. There has been some nice theoretical work in the field, some of which was present at NeurIPS 2023.</p>
<ol>
<li><strong>The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks</strong><d-cite key="frei2023doubleedged"></d-cite>: Usually implicit bias is understood as a net positive for pushing models to generalize better, but this paper rigorously shows convergence towards solutions that are weak to adversarial $\ell_2$ perturbations. The analysis is limited to logistic loss or exponential loss on 2-layer ReLU networks but is entirely theoretical.</li>
<li><strong>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</strong> <d-cite key="wu2023implicit"></d-cite>: If you’ve ever done the proof of gradient descent for $L$-smooth convex functions, you’re probably aware of the Descent Lemma and the $&lt;1/L$ step size requirement. The <em>edge of stability</em><d-cite key="cohen2022gradient"></d-cite> is this step-size range where this monotonicity guarantee is broken, but empirically models still seem to be able to converge. Interestingly, they show superiority of logistic loss over exponential loss theoretically in that at regardless of the step size chosen, logistic loss will converge eventually while exponential loss will diverge from the implicit bias using gradient descent.</li>
</ol>
<h3 id="training-dynamics">Training Dynamics</h3>
<p>For generic gradient-based learning in neural networks, we are often interested in understanding common patterns that emerge during different phases of the training process. Grokking<d-footnote>https://openreview.net/pdf?id=9XFSbDPmdW</d-footnote> is an example of one type of phenomena, where (Nanda et al. 2023) show that in the overparameterized regime, models exhibit clear generalization behavior later into training despite maintaining low train error for a long time. Training dynamics that generalize across a certain class of models are difficult to identify, but discovering them will significantly improve our understanding of how to train models efficiently.</p>
<ol>
<li><strong>Phase Diagram of Early Training Dynamics in Deep Neural Networks: Effect of The Learning Rate, Depth, and Width</strong> <d-cite key="kalra2023phase"></d-cite>: They motivate a bit about why sharpness matters as a metric for training dynamics, an analysis of how different hyperparameter choices affect how loss and sharpness over time. It’s a pretty interesting set of experiments in a toy setting, but I don’t know how observable the phases they observe are when you add all the tricks and complexities of modern deep learning.</li>
<li><strong>Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?</strong> <d-cite key="george2023training"></d-cite>: Similar to the work above, it’s again a rigorous empirical analysis on a toy problem, but it’s important progress towards understanding what our models are learning during training.</li>
<li><strong>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</strong> <d-cite key="adriaensen2023efficient"></d-cite>: I remember laughing when I first found this paper because it is literally inferencing what the training dynamics of another model will look like. It’s not the first work of its kind, but they incorporate the prior training curve data to do Bayesian inference of the training dynamics. I’m honestly also curious why regression doesn’t suffice.</li>
</ol>
<h3 id="embodied-ai">Embodied AI</h3>
<p>Embodied AI is a nascent field, but it focuses on building agents that can utilize multiple modalities. The field is set up to be a pre-cursor to the fabled AGI, but progress on this field hinges on the success of other fields like NLP, multi-modal learning, RL (although this is debated frequently). I didn’t see many works directly focusing on embodied AI, but I’m sure there will be many in the future.</p>
<ol>
<li><strong>Egocentric Planning for Scalable Embodied Task Achievement</strong> <d-cite key="liu2023egocentric"></d-cite>: This was the winning agent for the ALFRED challenge at CVPR 2023<d-footnote>https://embodied-ai.org</d-footnote>, where agents solve language-specified tasks in a first-person simulation. It’s a domain-specific agent, but I think what’s interesting is understanding how they choose to ground skills and actions for planning the next action.</li>
<li><strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</strong> <d-cite key="wang2023describe"></d-cite>: I’ve seen quite a few LLM-based approaches to Minecraft, but this is the first to do zero-shot planning. To add game context, they have a visual-language model map visual observations to language, and they use these models to further decide which LLM-generated goals are more feasible conditioned on the visual state.</li>
</ol>
<h3 id="neural-architecture-search">Neural Architecture Search</h3>
<p>Neural architecture search (NAS) is a class of algorithms that automatically search for model parameters (not just hyperparameters!) to optimize for some metric and has been around for a while. NAS is used a lot in finding model parameters for ultra low-cost machines like in <d-cite key="dong2023packqvit"></d-cite>. It’s actually quite complicated how these algorithms work, and I’d recommend reading Lillian Weng’s blog<d-footnote>https://lilianweng.github.io/posts/2020-08-06-nas/</d-footnote> to get a basic understanding of what people have done. The article is a bit old now, but I also don’t think the field itself has changed that much since.</p>
<ol>
<li><strong>EvoPrompting: Language Models for Code-Level Neural Architecture Search</strong> <d-cite key="chen2023evoprompting"></d-cite>: Along the themes of can LLMs do everything, this paper looks into whether LLMs can aid in NAS. A lot of algorithms in NAS involve evolutionary search, so they query the LLM to generate the code for network parameters and mutate them over time through a soft-prompt tuning process.</li>
</ol>
<h3 id="neural-operators">Neural Operators</h3>
<p>Most data is inherently discretized, or it lies on some well-defined finite-dimensional Euclidean space. However, there are many problems that involve learning <em>functions</em> (e.g. approximating partial differential equations) where we instead want to learn mappings between functional spaces. I think this paper <d-footnote>https://arxiv.org/abs/2108.08481</d-footnote> explains it better than I will (and they write it in a way that’s quite intuitive to follow) but the basic idea is that the architecture and loss functions aren’t going to be any different than your standard neural network problem. The main difference is in framing, as our models are now acting over function spaces, so we treat the linear layers as linear integral operators on function spaces. The key benefit of working over function spaces is we no longer implicitly discretize our data, so varying resolutions of data do not affect our models, and there’s also some extra machinery that we can apply.</p>
<ol>
<li><strong>Convolutional Neural Operators for robust and accurate learning of PDEs</strong> <d-cite key="raonić2023convolutional"></d-cite>: They prove similar universal approximation theorem guarantees for neural operators, but explicitly using convolutional layers. This is a pretty significant work in expanding neural operator use-cases, and they also show some examples of learning PDEs.</li>
</ol>
<h3 id="variational-inference-methods">Variational Inference Methods</h3>
<p>Bayesian methods are provably good at examining uncertainty in the posterior given your prior information and beliefs, but in practice they require approximating different intractible integrals in the closed for computation. More formally, suppose we want to compute the posterior $P(\theta|X)$, which represents the distribution over the parameters given our data. From Bayes rule,</p>
\[P \left(\theta|\mathbf{X} \right) = \frac{P \left(\mathbf{X}|\theta \right)P\left(\theta \right)}{P \left(\mathbf{X} \right)} = \frac{P \left(\mathbf{X}|\theta \right)P \left(\theta \right)}{\int_{\theta}P \left(\mathbf{X}|\theta^{\prime} \right) P \left(\theta^{\prime} \right) d \theta^{\prime}}\]
<p>The idea is that the denominator is intractible, but it’s actually a constant, so we can learn the posterior up to some normalization factor. So we instead will learn a simpler distribution \(Q(\mathbf{\theta})\) to approximate the posterior. The techniques for minimizing the difference between these distributions during optimization are well known and used frequently in applied ML.</p>
<ol>
<li><strong>Joint Prompt Optimization of Stacked LLMs using Variational Inference</strong><d-cite key="sordoni2023joint"></d-cite>: This paper presents a very <em>unique</em> idea. Basically, if we stack langauge models (I had no idea people did this), you can treat the output of the $N-1$th language model as a latent parameterization of the $N$th language model, so we can perform variational inference to get a good estimate of the generative distribution of the $N$th language model. This theme of prompts being parameters is not new, especially if you’ve read earlier parts of this post, but the purpose of this work is to show that stacking language models can, in theory, provide better performance with a bit of extra machinery.</li>
</ol>
<h3 id="quantum-information-theory">Quantum Information Theory</h3>
<p>Quantum computers are notably faster at solving certain classes of problems (e.g. Shor’s algorithm for prime factorization), so if they end up replacing modern processors, we ideally want to ensure machine learning algorithms are efficient on them. While there were barely any papers on this topic, I did think it was interesting to look into.</p>
<ol>
<li><strong>On quantum backpropagation, information reuse, and cheating measurement collapse</strong> <d-cite key="abbas2023quantum"></d-cite>: The backpropagation relies on re-using intermediate computations to achieve a linear runtime, which almost the entirety of deep learning is built on. In quantum mechanics, however, measurements fundamentally change the quantum state describing a system, so storing copies of a quantum state for future use doesn’t really make sense. It’s a really unique and interesting challenge to balance the tradeoffs between quantum computing and classical computing (freshman year me was once interested in pursuing quantum computing, but alas), and this paper provides a unique solution to achieve backpropagation scaling, potentially enabling the development of scalable overparameterized neural networks on quantum computers.</li>
</ol>
<h3 id="energy-based-models">Energy-based Models</h3>
<p>Energy based models (EBM) are a different way to view and train probabilistic models based on learning an energy function, but they provably can represent a wide varieties of algorithms such as k-means and maximum likelihood estimation<d-footnote>Yann Lecun has made lots of talks about energy-based models. I found these slides online: https://cs.nyu.edu/~yann/talks/lecun-20060816-ciar-1-ebm.pdf</d-footnote>. The key benefit is that we can ignore the intractible normalization constant computation needed for variational methods. Intuitively, I think it’s easiest to understand them in a contrastive learning framework, where an energy function $F: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ describes similarity between pairs of data points. GFlowNets<d-cite key="NEURIPS2021_e614f646"></d-cite> are an exciting recent development of EBMs for generative modeling, and a few papers at this conference studied applications of them<d-cite key="zhang2023let,atanackovic2023dyngfn,zhu2023sampleefficient"></d-cite>.</p>
<ol>
<li><strong>Energy Transformer</strong><d-cite key="hoover2023energy"></d-cite>: I think EBMs have the potential for a major breakthrough, but it really depends on whether they can reliably crack a really hard or popular problem. This work adapts EBMs for modern deep learning mechanisms like attention, and I’m interested to see if people take this further to things like LMs!</li>
</ol>
<h3 id="curriculum-learning">Curriculum Learning</h3>
<p>Curriculum learning is the notion of learning simple tasks first before learning complex tasks. This intuitively makes sense in RL, where we want to learn simple skills before learning complex high-level strategies, but it is not limited to RL. I have always been kind of skeptic of curriculum learning because it is not well understood, but at the same time there have been some successful use cases of it.</p>
<ol>
<li><strong>Curriculum Learning With Infant Egocentric Videos</strong><d-cite key="sheybani2023curriculum"></d-cite>: Funnily enough, this was a research direction I was interested in taking back in 2021, but I just never got access to the data to do it. I’m curious though if curriculum learning is particularly useful for self-supervised learning because it helps form the intermediate representation space in a nicer way or order, i.e. starting from representation space A, it’s a lot easier to move to B through gradient-based learning, so we should learn A first.</li>
</ol>
<h3 id="anomaly-detection">Anomaly Detection</h3>
<p>Anomaly detection (AD)<d-footnote>https://arxiv.org/abs/2007.02500</d-footnote> generally refers to detecting outliers or unexpected behavior in data. These methods are especially important for tasks where we want our models to be risk-averse. I was surprised to see quite a few works focused on this field at this year’s conference, but considering how diverse the applications are, it kind of makes sense. I’m not familiar with the field at all, but a few of them did pique my interest.</p>
<ol>
<li><strong>Unsupervised Anomaly Detection with Rejection</strong><d-cite key="perini2023unsupervised"></d-cite>: In my mind anomaly detection makes the most sense in as unsupervised or self-supervised learning problem because they inherently pop up in any domain. Their work deals providing strong theoretical guarantees for a general unsupervised algorithm that to reject low-confidence outputs.</li>
<li><strong>Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach</strong> <d-cite key="yoon2023energybased"></d-cite>: One way of detecting anomalies is to have a general understanding of what your data should look like. In this work, the authors follow this idea by training an energy-based model to approximate the low-dimensional manifold that the training data lies on and use the energy function as a score for identifying anomalies.</li>
</ol>
<h3 id="class-imbalance-approaches">Class Imbalance Approaches</h3>
<p>For multi-class classification problems, we generally want a uniform distribution of class labels across the dataset so models do not converge to the most frequently occurring label with probability 1. Class imbalance is a long-standing problem in optimization-based methods, and there has been a lot of work to try and combat it (e.g. data augmentation to balance classes, sampling, new loss funcitons).</p>
<ol>
<li><strong>Simplifying Neural Network Training Under Class Imbalance</strong><d-cite key="shwartzziv2023simplifying"></d-cite>: This paper makes some pretty strong claims: that just by tuning hyperparameters like batch sizes, label smoothing, optimizers, and data augmentation, we can combat class imbalance. The experiments they run are on fairly simple and old datasets, but this is pretty common for works that study general deep learning phenomena. I’m just not sure how well these observations hold at scale.</li>
</ol>
<h3 id="continual-learning">Continual Learning</h3>
<p>I was interested in continual learning, also known as lifelong learning, when I first discovered machine learning. I think continual learning, like multimodal learning, is another important piece towards artificial general intelligence (AGI), but it is not as popular at the moment. The general idea is a learning framework that continues to adapt and learn over time, but well-known problems like catastrophic forgetting (model learns A then B. model will forget A.) and plasticity (how easily does a model learn something new) make continual learning extremely difficult. It is entirely possible that completely different paradigms are necessary for continual learning, but this is an active field of research<d-footnote>A useful introduction to pre-existing works is: https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning</d-footnote>.</p>
<ol>
<li><strong>A Definition of Continual Reinforcement Learning</strong><d-cite key="abel2023definition"></d-cite>: Reinforcement learning is explicitly a reward-maximizing algorithm under a fixed environment, which is sort of at odds with continual learning, where we want a policy to continue to adapt. This paper lays some of the groundwork for defining the necessary vocabulary and tools to approach continual learning in a reinforcement learning setting.</li>
<li><strong>RanPAC: Random Projections and Pre-trained Models for Continual Learning</strong><d-cite key="mcdonnell2023ranpac"></d-cite>: We ideally want to leverage pre-trained foundation models as a base for continual learning, but because we do not have a strong mechanistic understanding of these models, it is unclear how changing these weights over time in a continual learning sense will affect the model performance. This work is a first step into performing parameter updates on pre-trained models without forgetting.</li>
</ol>
<h3 id="deep-learning-theory">Deep Learning Theory</h3>
<p>Deep learning theory is specifically the study of deep neural network models, and generally centers around dense linear layers with activations (at least for now). The most well known result is probably the Neural Tangent Kernel (NTK) <d-cite key="jacot2020neural"></d-cite>, which describes the behavior of networks as you take their layer width to infinity. Deep learning theory is an active area of research that I personally know little about, but at the very least, I’ve observed two common approaches at this conference that were used to study it. That being said, these are definitely not the only two tools used.</p>
<ol>
<li><strong>Kernel methods.</strong> This was formalized in the NTK work, but there is a provable duality between gradient descent in the infinite width limit and kernel gradient descent over the NTK as the kernel. Because of the rich theory present in kernel methods, we can apply these tools to study the behavior of neural networks as well.</li>
<li><strong>Mean-field theory</strong> has historically been applied in probability theory and physics settings (thanks to my roommate Evan for explaining the basic idea to me), and is essentially a suite of tools for solving extremely high-dimensional and complex dynamics by approximating their behavior as an “average”. It has been applied extensively to deep learning theory as well for studying the behavior of infinite-width and infinite-depth networks as an alternative to viewing everything as kernels.</li>
</ol>
<p>There’s a bit too much necessary background information that goes into the machinery required for deep learning theory for me to really understand any of these papers, so unfortunately I do not have anything to list for interesting papers (yet at least!). I can list some examples though: <d-cite key="bordelon2023dynamics,fiedler2023kernelbased,kumano2023adversarial">&lt;/d-footnote&gt;</d-cite></p>
<h3 id="bio-inspired-ai">Bio-inspired AI</h3>
<p>Artificial Intelligence is an extension of our desire to mimic biological intelligence (although I wish it wasn’t), so naturally we have a lot to gain by using ideas we discover from biology. I think over time we will always see a steady number of works on bio-inspired AI, but so far, I haven’t seen too many works of this type that really take off (e.g. spiking neural networks). Part of the problem is that we know very little about our brains and how they function, so potentially once we figure that out, we’ll start implementing those mechanisms in our AI!</p>
<ol>
<li><strong>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?</strong> <d-cite key="pandey2023vision"></d-cite>: I think it’s always sort of assumed that humans are far more data efficient than neural networks. They compare ViT performance on object recognition against newborn chicks with the same visual data and show that ViTs actually solved the same tasks. This paper is interesting, but I think the task is too simple and doesn’t capture the efficiency and task complexity tradeoff that would be more interesting to know.</li>
</ol>
<h3 id="domain-specific-applications-of-ai">Domain-specific applications of AI</h3>
<p>There were a lot of very cool domain-specific applications of AIML at NeurIPS this year, most of
which was completely beyond me. Applications and datasets for the natural sciences were probably the most popular, especially related to protein modeling or protein functional prediction <d-cite key="liu2023predicting,gao2023proteininvbench,ahdritz2023openproteinset"></d-cite> and molecules <d-cite key="liu2023symmetryinformed"></d-cite>. There were also works in chemistry <d-cite key="guo2023large,tavakoli2023ai">, law <d-cite key="östling2024cambridge"></d-cite>, and even circuit prediction<d-cite key="zou2023circuit"></d-cite>!</d-cite></p>
<ol>
<li><strong>Circuit As Set of Points</strong><d-cite key="zou2023circuit"></d-cite>: I wanted to highlight this paper because I thought it was zany. They literally treat circuit discovery as a <em>point-cloud prediction</em> problem instead of like a graph as in prior works. The authors say it’s to avoid pre-processing, so they’re literally just feeding in a raw circuit and treating it like a point cloud. Works like these are honestly really exciting, even if doesn’t end up become the standard method.</li>
</ol>
<h3 id="reproducibility-studies">Reproducibility Studies</h3>
<p>So I didn’t know reproducibility experiments were publishable at major AI conferences like NeurIPS, but I’m happy to discover that it is. This year featured quite a few reproducibility experiments for older works, and it seems like they all follow a kind of template of how they should be conducted. I have experience working with repositories that just do not align with the results of described in the papers, so I know how annoying it is to not know if a paper actually works. I saw this paper <d-cite key="kleuver2023reproducibility"></d-cite> that attempts to reproduce the results of <d-cite key="keswani2022proto2proto"></d-cite>, but I found it funny because they basically roast their documentation and go into detail about the weaknesses in their experiments. With AI research rapidly growing, I think it’s getting more and more important that we set a bar for reproducibility.</p>
<h2 id="other-papers-i-liked">Other Papers I Liked</h2>
<ol>
<li>
<p><strong>Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models.</strong> <d-cite key="ortizjimenez2023task"></d-cite>: It seems that literally adding the weights of a bunch of fine-tuned experts on distinct downstream tasks can actually lead to a generalized model that solves all of them. From a linear algebra perspective, this only makes sense if the fine-tuning process pushes the weights into their own distinctive regions of the weight space. They motivate this sort of behavior as weight disentanglement and use it to fine-tune models for better weight addition properties.</p>
</li>
<li>
<p><strong>Language-based Action Concept Spaces for Video Self-Supervised Learning.</strong> <d-cite key="ranasinghe2023languagebased"></d-cite>: I’ve recently been interested in more abstract representations of concepts like actions for building agents that can “think” using these representations. I think a lot of works on thinking in terms of actions and skills has been in terms of LLMs, but this work examines it from an encoder perspective.</p>
</li>
<li>
<p><strong>The Grand Illusion: The Myth of Software Portability and Implications for ML Progress</strong> <d-cite key="mince2023grand"></d-cite>: I generally take for-granted the libraries that I use, so it was cool to see a study on the performance of popular frameworks across a variety of devices. These kinds of details are the things that I’ve been more and more interested in understanding, so I’m happy to see a paper like this at the conference.</p>
</li>
<li>
<p><strong>Human-Guided Complexity-Controlled Abstractions</strong> <d-cite key="peng2023humanguided"></d-cite>: To develop agents that can interact with the environment the way we do, we first have to understand how to integrate our mental abstraction hierarchy into an agent’s. If you look at some of the papers I talked about earlier like <d-cite key="yang2023hierarchical,evans2023creating"></d-cite>, it is clear that people are interested in building action hierarchies for agents. This paper looks into understanding what level of abstraction or complexity is required to understand and execute actions for different types of tasks. I’m hoping to build a mental model of how we would go about building robust systems that can act in this way.</p>
</li>
<li>
<p><strong>The Tunnel Effect: Building Data Representations in Deep Neural Networks</strong> <d-cite key="masarczyk2023tunnel"></d-cite>: This paper provides some empirical insight into how data is represented throughout generic overparameterized neural networks. They propose this tunnel effect hypothesis, where the early layers of the network focus on linearly separable representations that focus on learning the actual task, while the later layers are just compression layers that harm generalization performance. They also suggest that regardless of model capacity (as long as it is sufficient), models will allocate the same amount of capacity to a specific task, which may also offer some insight into the implicit bias of models and why they tend not to overfit in the overparameterized regime.</p>
</li>
</ol>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>This was a really rewarding process for me, not only from a knowledge standpoint. Before doing this little exercise, I was feeling a bit burnt out from school and learning in general. For a while, I had been thinking really hard researching how to inject language information for policy learning, but a lot of directions I had in mind just didn’t seem to make sense in the end. I had spent a lot of time reading in this specific direction, and I was a bit tired of seeing the same flavour of methods being applied. So it was honestly really refreshing to take a step back and reel in what people had been doing. I didn’t get a chance to go to NeurIPS 2023 (or any major conference for that matter) because of my courseload, but at least now I can say I know at least a little bit about what was going on there!</p>
<p>During this process I also compiled a list of open research questions I think are worth pursuing, which I may clean up in the future and put up. If you made it this far, thanks for giving it a read! It took a long time to synthesize these resources and figure out the structure of this article, so regardless of your background or prior knowledge, I hope this was at least somewhat useful for you!</p>
<h2 id="random-terms">Random Terms</h2>
<p>I kept track of a list of terms that I had to Google while going through these abstracts. I probably
also ended up looking into more and forgot to put them in this list, but in case you’re interested,
<del>here they are below</del>. Edit: Ok so, this article ended up being <strong>way</strong> longer than intended, so instead if you want this list you should email me. My email is on my home page.</p>
<h2 id="citation">Citation</h2>
<p>Just as a formality, if you want to cite this for whatever reason, use the BibTeX below.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zhang2024neuripshighlights,
  title   = "Highlights of NeurIPS 2023 from Abstracts",
  author  = "Zhang, Alex",
  journal = "Alex's Writing",
  year    = "2024",
  month   = "Jan",
  url     = "https://alexzhang13.github.io/blog/2024/neurips2023/"
}
</code></pre></div></div>]]></content><author><name>Alex Zhang</name></author><category term="blog" /><category term="summary" /><category term="generative" /><category term="ai" /><summary type="html"><![CDATA[Just me reading through every paper abstract...]]></summary></entry></feed>