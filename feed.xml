<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://alexzhang13.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alexzhang13.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-06T22:44:45+00:00</updated><id>https://alexzhang13.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Highlights of NeurIPS 2023 from Abstracts</title><link href="https://alexzhang13.github.io/blog/2024/neurips2023/" rel="alternate" type="text/html" title="Highlights of NeurIPS 2023 from Abstracts"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://alexzhang13.github.io/blog/2024/neurips2023</id><content type="html" xml:base="https://alexzhang13.github.io/blog/2024/neurips2023/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>To celebrate the end of my graduate school application deadlines and finals week disaster, I decided to spend my winter break <b>going through and reading every single abstract of the accepted papers in the NeurIPS 2023</b> (which I unfortunately couldn’t attend). It was a long and sometimes mind-numbing process (especially since the TeX wasn’t rendering on the <a href="neurips.cc">neurips.cc</a> website), but it was really cool to see all these works and ideas that I had no idea were being done. Luckily, I am somewhat familiar with quite a number of these papers because they popped off on <a href="https://arxiv.org/list/cs.AI/recent">arXiv</a> or <a href="https://twitter.com/home">Twitter</a> when they were first announced, so I wasn’t discovering every single paper for the first time. Here is just a highlight of what I found interesting and the general vibes I had while reading over the last two weeks, but keep in mind that I am an undergraduate student that has not worked with or spent a lot of time with a variety of popular topics (e.g. <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a>, <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a>, <a href="https://en.wikipedia.org/wiki/Causal_inference">causal inference</a>). I’ve structured this post into a <strong>high-level overview for each topic</strong> of what I observed, followed by <strong>short discussions on papers I found interesting</strong>, and ending with <strong>open research questions I think are worth thinking about</strong>. Each discussion is loaded with references to the relevant NeurIPS papers, and I’ve tried to ensure that every citation in this post is from NeurIPS 2023. If you want to read the abstracts on your own, they’re available publicly at <a href="https://neurips.cc/virtual/2023/papers.html">https://neurips.cc/virtual/2023/papers.html</a>. Finally, I ended up searching up so many terms, I eventually started keeping track of them in case the curious reader wants to know at the bottom of this post.</p> <h2 id="the-overall-vibes">The Overall Vibes</h2> <figure> <img src="/assets/img/neurips2023.png" width="400" alt="Visualization of NeurIPS 2023"/> <figcaption>Visualization generated from <a href="https://neurips2023.vizhub.ai/?ref=blog.roboflow.com&amp;brushed=%255B%255B14.600000381469727%252C-10%255D%252C%255B798.7249755859375%252C796.664306640625%255D%255D">here.</a></figcaption> </figure> <p>2023 was a great year for AI (generative models especially), and the number of submissions to NeurIPS 2023 reflects that. This year, there were an astonishing <b>12345</b> submissions and a total of <b>3584</b> papers. Honestly, I expected to go into this process marking only my top 10 or 20 favorite papers, but I discovered so many absolutely fascinating works on the most random things like applying 3D neural rendering methods to Unreal Engine. Of course, from the abstract alone, it is not possible to grasp the quality or impact of a work, and in an ideal world I would have read each paper in their entirety and tried their code. Regardless, after reading through, these are some of the main themes that stuck out to me this year:</p> <ol> <li><strong>Multimodal is the new “thing”</strong>. Rather unsurprisingly, combining vision and language is a hot topic <d-cite key="huang2023language,wang2023connecting,luo2023cheap,gadre2023datacomp,zhu2023multimodal,mizrahi2023m,yin2023vlattack,cheng2023metaadapter,dai2023instructblip,wortsman2023stable,wu2023parameter"></d-cite>. With LLMs demonstrating impressive performance in the past few years, leveraging these models to reason over visual data is a natural progression of the technology.</li> </ol> <h2 id="large-language-models">Large Language Models</h2> <p>This is the section of papers I was most familiar with before reading through the abstracts. Generally, there have been a lot of works exploring the reasoning capabilities of LLMs through prompting or fine-tuning. I think the LLM papers at this conference reflect the interests of the NLP community for the past year even though a lot of these works can be considered <strong>at least a year old at this point</strong> (which in this field is old!). One general gripe I have with these works is that they often like to make claims about general LLM behavior, but often evaluate on an arbitrary LLM of their choosing. It would be nicer if there was some consistency here, but it’s probably because the field is so fast-moving combined with the fact that many models are just inaccessible to 95% of labs. There is also this interesting interplay of whether data purity or extra machinery is the driving factor towards improving performance. These are the general themes that I’ve noticed:</p> <ol> <li><strong>Reasoning.</strong> How can we apply algorithms or intuitions to fine-tune LLMs to be better planners, reasoners, etc. like in <d-cite key="phan2023training"></d-cite>? Furthermore, how can we explicitly improve their reasoning capabilities without gradient updates like in <d-cite key="yao2023tree"></d-cite>?</li> <li><strong>Logical Reasoning.</strong> Can LLMs be used for logical tasks like mathematics <d-cite key="zhang2023evaluating,frieder2023mathematical"></d-cite> and coding <d-cite key="yang2023intercode"></d-cite>? What are the pitfalls and potential solutions? We are interested in analyzing the limits of their abilities, as well as understand why and when they fail <d-cite key="hanna2023does"></d-cite>.</li> <li><strong>Agents.</strong> Can LLMs be considered cognitive agents? That is, can we equip them with capabilities (tools, externals APIs <d-cite key="schick2023toolformer"></d-cite>, etc.) such that they can interact with environments through text generation? Furthermore, can they interact, think, and reflect in an anthropomorphic way? <d-cite key="madaan2023selfrefine,shinn2023reflexion"></d-cite></li> <li><strong>Efficiency.</strong> A key bottleneck in autoregressive LLM inference on GPUs is data movement, so research into circumventing this issue by exploiting parallelism and cached data is key (e.g. speculative decoding) <d-cite key="jin2023s3,kim2023speculative,sun2023spectr,zhu2023optimal"></d-cite>. Furthermore, can we improve the speed and cost of both training and inference on LLMs at both a systems-level (e.g. take advantage of properties of GPU memory footprints and throughput speeds) and model-level (low-rank fine-tuning, adaptors, sparsity, etc.) <d-cite key="portes2023mosaicbert,pagliardini2023faster,dettmers2023qlora"></d-cite>?</li> <li><strong>Scaling and Fine-tuning Models.</strong> What advances can we make to push the capabilities of foundation models (e.g. MoE) <d-cite key="xue2023repeat"></d-cite>? What procedures can we use to efficiently fine-tune <d-cite key="malladi2023finetuning,dubois2023alpacafarm"></d-cite> these models for downstream tasks or direct their generations towards user-aligned behavior (also can we do this without reinforcement learning (RL) <d-cite key="rafailov2023direct,zhou2023lima"></d-cite>) ? Or, what techniques can we use to bridge the gap between smaller models and huge models? Also, what kind of scaling laws can we identify between model parameters and data <d-cite key="muennighoff2023scaling"></d-cite>?</li> <li><strong>Memory and Context Windows.</strong> The context window of an LLM is inherently limited by memory constraints and the quadratic runtime of the standard attention mechanism. How do we increase the size of the input context window without degrading performance? The primary methods that are being investigated are external memory stores in LLMs to process huge chunks of texts like a book <d-cite key="wang2023augmenting,bertsch2023unlimiformer,mohtashami2023landmark"></d-cite> and summarization tokens <d-cite key="mu2023learning"></d-cite>.</li> <li><strong>Emergent Capabilities.</strong> It has been observed that LLMs seem to be suddenly able to perform tasks after scaling up to a certain point. Can we characterize these emergent abilities of LLMs effectively and why we observe them <d-cite key="schaeffer2023emergent"></d-cite>?</li> <li><strong>Controlling Generation.</strong> Can we 1) hard constrain the outputs and 2) steer LLM generations to be what we want? Typically (2) has been done with instruction-tuning and RLHF, but methods like <d-cite key="li2023inferencetime"> </d-cite> modify activation patterns in the model during inference while <d-cite key="li2023guiding"></d-cite> learns a smaller auxiliary model to edit the prompt of the larger model, which we can effectively treat as a set of parameters. Furthermore, what methods can we come up with to ensure preference-based alignment is accurate and robust <d-cite key="wang2023aligning"></d-cite></li> <li><strong>LLMs can do [insert task]</strong>. How far can we go with zero-shot, few-shot, and in-context learning (ICL) to mimic known algorithmic procedures entirely through prompting? For example, policy iteration from RL <d-cite key="brooks2023large"> </d-cite> or time-series forecasting <d-cite key="gruver2023large"> </d-cite>.</li> <li><strong>Evaluating Language Models.</strong> What kind of metrics and benchmarks are needed to effectively evaluate the abilities of different LMs? Is it by using a strong LLM as a judge <d-cite key="zheng2023judging"></d-cite>? Also, we generally just want more benchmarks for evaluating abilities like factuality <d-cite key="chen2023felm"></d-cite>, coding <d-cite key="yang2023intercode"></d-cite>, and domain-specific information <d-cite key="liu2023benchmarking,guha2023legalbench,guo2023large"></d-cite>.</li> </ol> <h3 id="interesting-papers">Interesting Papers</h3> <p>Below is a list of papers I particularly liked and think are worth reading in their entirety. Of course, there were plenty of other extremely interesting and useful works at this conference, so please do not take this as some kind of definitive ranking of papers:</p> <ol> <li><strong>Tree-of-Thoughts</strong> <d-cite key="yao2023tree"></d-cite>: A simple yet highly useful idea, tree-of-thoughts (ToT) is simply an extension of chain-of-thoughts where a model can now traverse through its own “thought” chains as a tree. This simple application of graph traversal to CoT (which also gives beam search vibes) has been used extensively for prompting in the last year. This paper actually came from my PI’s lab :)</li> <li><strong>Toolformer</strong> <d-cite key="schick2023toolformer"></d-cite>: Another well-known and simple approach, Toolformer is an extremely general framework for fine-tuning a model to be able to use user-specified APIs like a calculator or a search engine. It’s really practical and their experiments use GPT-J <d-footnote>https://huggingface.co/docs/transformers/model_doc/gptj</d-footnote>, so it’s quite easy to replicate for your own use cases.</li> <li><strong>Are Emergent Abilities of Large Language Models a Mirage?</strong> <d-cite key="schaeffer2023emergent"></d-cite>:</li> <li><strong>SPRING: Studying Papers and Reasoning to Play Games</strong> <d-cite key="wu2023spring"></d-cite>:</li> <li><strong>RapidBERT: Pretraining BERT from Scratch for $20</strong> <d-cite key="portes2023mosaicbert"></d-cite>:</li> <li><strong>Scaling Data-Constrained Language Models</strong> <d-cite key="muennighoff2023scaling"></d-cite>:</li> <li><strong>Collaborative Alignment of NLP Models</strong><d-cite key="khani2023collaborative"></d-cite>: We have been conditioned to use single, huge foundation models because they work well in practice, but this paper looks into whether the learning of several, concept-aligned models with some meta-chooser on top actually works just as well. The primary benefit is the ability to modularize and parallelize LLMs, making them more flexible and also potentially faster.</li> <li><strong>Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.</strong> <d-cite key="wen2023hard"></d-cite>:</li> <li><strong>OpenAssistant Conversations - Democratizing Large Language Model Alignment</strong> <d-cite key="kopf2023openassistant"></d-cite>: So I actually followed this project on YouTube through Yannic Kilcher’s channel <d-footnote>https://youtube.com/yannickilcher&lt;/d-cite&gt;, and the central idea is open-sourcing the data required for RLHF because its extremely expensive to curate 60k+ human preference samples. I believe that open-source communities are extremely important for AI, and it's exciting to see them produce extremely useful projects like this one. Because it is open-source, a large chunk of the paper discusses quality control and reducing "bad" or "toxic" data.</d-footnote></li> </ol> <h2 id="multi-modal-learning">Multi-modal Learning</h2> <p>This year had a heavy focus on multimodal (mostly vision + language) models, with many companies/labs introducing their own shiny foundation models and associated datasets to compete with GPT-4. A lot of the desirable features in large multimodal models parallel that of large language models, so many of the research questions and themes are quite similar. In my mind, a core difference is the combinatorially larger amount of paired or associated data required to build a model that can interchangebly handle two different modalities. The obvious direction is to continue to scale the size of visual-language datasets with the size of new models, but I suspect that fundamentally answering how to “ground” two different modalities from a representation learning perspective may be able to reduce the necessary scale. At the end of the day though, this is still an open research question which I don’t know the answer to. The general themes I observed were</p> <ol> <li> <p><strong>New Foundation Models.</strong> Large multimodal models, specifically vision-language, are the logical next progression to LLMs. Thus, it’s rather unsurprising that many research groups are racing to build the next big model <d-cite key="huang2023language,mizrahi2023m"></d-cite> with the same capabilities of LLMs like being instruction-tuned <d-cite key="dai2023instructblip"></d-cite> and in-context learning. However, it looks like the training mechanisms for making these models robust are still pretty elementary. For example, <d-cite key="huang2023language"></d-cite>, they simply treat everything as a token, but use a special embedding token for images and attentive pooling to reduce the complexity of these embeddings, then use the interleaved text and image data for standard log-likelihood optimization. At this conference, I didn’t see many actual models being showcased (although I’ve seen them over time on Twitter), but the large number of datasets seem to indicate that this is a growing direction.</p> </li> <li> <p><strong>Datasets and Benchmarks!</strong> We’ve observed several instances of “good” data being key to getting LLMs to work better. The same logic applies to multi-modal models, except because there generally is no 1-1 mapping between tokens in each modality, this is quite hard. Regardless, there are lots of multi-modal datasets and benchmarks being curated, either by scraping and filtering data on the internet <d-cite key="zhu2023multimodal,gadre2023datacomp"></d-cite> or by curating the data <d-cite key="pătrăucean2023perception,zhang2023m3exam"></d-cite>.</p> </li> <li> <p><strong>The Shared Representation.</strong> As far as I’m aware, the two main ways of building a multi-modal learning model are to tokenize each modality and train it as a decoder model on negative-log-likelihood loss <d-cite key="huang2023language"></d-cite> or train it CLIP-style<d-footnote>CLIP or Contrastive Language-Image Pre-Training is an contrastive learning-based encoding method for embedding images and language into the same embedding space. The benefit is that we can query into this embedding space using text or images, and query from this embedding space to generate either text or images. The idea was popularized from its used in DALL-E, and is the standard for text-to-image models. Read more here: https://openai.com/research/clip </d-footnote> as an encoder model. <d-cite key="oldfield2023parts,qiu2023controlling,samuel2023normguided,oh2023geodesic"></d-cite> In both cases, we want to understand this latent representation embedded either in the model layers or in the embedding space to see if we can exploit its properties <d-cite key="samuel2023normguided,oh2023geodesic"></d-cite>.</p> </li> <li> <p><strong>Complex text prompts.</strong> It is known that text-to-image models often <strong>exhibit bag-of-words behavior</strong>, which means it lacks a strong understanding of syntax and logical quantifiers in a sentence. If you’ve ever played with Midjourney <d-footnote>A popular text-to-image generator: https://www.midjourney.com/home?callbackUrl=%2Fexplore</d-footnote> or DALL-E2 <d-footnote>A popular text-to-image generator by OpenAI: https://openai.com/dall-e-2</d-footnote>. Several works attempt to inject compositional reasoning in these models to solve this <d-cite key="doveh2023dense,zhao2023unicontrolnet"></d-cite>.</p> </li> <li> <p><strong>Multi-modal video understanding.</strong> Processing videos adds a temporal dimension to these models that is quite difficult. Even standard video understanding models have been difficult to get right for a while now. The naive approach is to concatenate frames and pass them into a image-language model, so there has been some interest in making the language component temporally aware <d-cite key="yu2023selfchained"></d-cite>.</p> </li> <li> <p><strong>All the same questions for LLMs.</strong> Fundamentally, these models are just huge transformers. Even the vision components are basically just LLMs where the vocabulary is image patches (although the vocabulary is much bigger and not fixed I suppose). Regardless, any open problem for an LLM (efficiency <d-cite key="luo2023cheap,wortsman2023stable"></d-cite>, robustness<d-cite key="zhao2023evaluating,yin2023vlattack"></d-cite>, fine-tuning algorithms <d-cite key="wu2023parameter,cheng2023metaadapter"></d-cite>) is essentially also a problem for multi-modal models. The difference though, is that we can assume that we are given a model that is good at each modality, so “bridging” the modalities is what we need to actually solve.</p> </li> </ol> <h3 id="interesting-papers-1">Interesting Papers</h3> <p>There are a lot of what I like to call <em>low-hanging fruit</em> in multi-modal models that have been solved, and while they are still interesting and definitely more applicable to most problems, I wanted to focus on some works that I thought were cool.</p> <ol> <li><strong>4M: Massively Multimodal Masked Modeling</strong> <d-cite key="mizrahi2023m"></d-cite>: Apple doesn’t usually publish in machine learning conferences (e.g. they didn’t let me publish or extensively discuss my work when I was there), but I have to say, I thought this paper was pretty cool. I mentioned in point (3) that a lot multi-modal models are either decoder-based (token-style) or encoder-based (embedding-style), but the authors of this work discretize the shared embedding space and embed using tokens instead of a continuous embedding.</li> <li><strong>Connecting Multi-modal Contrastive Representations</strong> <d-cite key="wang2023connecting"></d-cite>:</li> <li><strong>A Theory of Multimodal Learning</strong> <d-cite key="lu2023theory"></d-cite>:</li> <li><strong>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</strong> <d-cite key="yin2023vlattack"></d-cite>:</li> <li><strong>Geodesic Multi-Modal Mixup for Robust Fine-Tuning</strong> <d-cite key="oh2023geodesic"></d-cite>: We want to understand the landscape of multi-modal embeddings and see if we can impose nice properties of this space like making it isotropic. In this paper, they first propose that CLIP embeddings (ZS) and naively fine-tuned embeddings (FT) have an inherent uniformity issue that distinctly separates “text” and “images” into different subspaces. Ideally though, they argue that we want the distribution over the space (they constrain it to be a hypersphere) to be based on the semantics. Their method proposes to mold this space by generating “mixed” hard-negative samples to use with the standard contrastive loss during fine-tuning.</li> </ol> <figure> <center> <img src="/assets/img/oh2023geodesic.png" style="width:60%" alt="Visualization of CLIP embedding space."/> </center> </figure> <p>I’m curious because in the past, isotropic properties of “word embeddings” was thought to be a necessary thing, but it turns out we don’t really care, and a lot of methods that try to constrain this didn’t turn out to be that useful. I wonder how that applies here.</p> <h2 id="transformers">Transformers</h2> <p>With the Transformer module being the standard building block for scalable models, it is important that progress is made on improving their usage as a whole. Just as a side note, I think generally the term “transformer” is now overloaded to mean any structure using positional encodings plus an attention-mechanism, feedforward layers, and normalization in some repeated fashion, and does not necessarily refer to the original Transformer architecture. Interestingly, one thing I didn’t really find at this conference was investigating the enforcement of constraints or inductive biases like equivariance to Transformers, which may be an indicator of Sutton’s Bitter Lesson <d-footnote>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</d-footnote>. The general themes were</p> <ol> <li><strong>Studying Transformer Models.</strong> Are there provable or empirically well-understood limitations of Transformers that may be severely limiting for future research directions? We know that Transformers seem to excel at reasoning but also frequently fail in simple cases, so <d-cite key="dziri2023faith"></d-cite> argue through a series of compositional tasks that these models (GPT-3,4) tend to <strong>pattern match reasoning chains</strong>. Furthermore, in <d-cite key="sanford2023representational"></d-cite>, they try to examine the function classes that Transformers can efficiently approximate, which is especially important for upper bounding the representational capacity of models as they scale. Finally, there are works that examine/ablate features of the Transformer <d-cite key="kazemnejad2023impact"></d-cite> to study their impact.</li> <li><strong>Efficiency.</strong> How do we modify parts of the transformer to be more efficient to 1) scale them for bigger models and 2) use them on low-compute devices? In <d-cite key="baykal2023alternating"></d-cite>, they selectively act on a fixed block of the embeddings at any layer to increase model capacity while keeping inference latency fixed. In <d-cite key="anagnostidis2023dynamic"></d-cite>, they train layers to selectively drop tokens by imposing a sparsity constraint that affects their modified attention mechanism. Meanwhile, works like <d-cite key="xi2023training"></d-cite> focus on preserving performance for quantized transformers and <d-cite key="liang2023mcuformer"></d-cite> focus on deploying transformers on microcontrollers.</li> <li><strong>Modifications to Attention.</strong> Can we make attention mechanisms more efficient <d-cite key="chen2023primalattention"></d-cite>? As in sub-quadratic runtime <d-cite key="anagnostidis2023dynamic,yu2023megabyte"></d-cite>? Sparse <d-cite key="pagliardini2023faster"></d-cite>? Or can we even replace attention <d-cite key="fu2023monarch"></d-cite>?</li> <li><strong>Memory.</strong> Can we increase context-window lengths or add external memory for transformers? This question is tied heavily to LLMs, and hence the methods (external memory source or summarization) are similar. Other than the works tied to LLMs, I only really found <d-cite key="zeng2023vcc"></d-cite>, which basically learns to compress token sequences into “VIP”-tokens that represent what’s most important. My only concern is whether these tokens are domain-specific and how a fully-trained model fairs for transfer learning to other modalities.</li> </ol> <h3 id="interesting-papers-2">Interesting Papers</h3> <ol> <li><strong>Faith and Fate: Limits of Transformers on Compositionality</strong> <d-cite key="dziri2023faith"> </d-cite>:</li> <li><strong>Geometric Algebra Transformer</strong> <d-cite key="brehmer2023geometric"> </d-cite>:</li> <li><strong>Pretraining Task Diversity and The Emergence of Non-Bayesian In-context Learning for Regression</strong><d-cite key="raventós2023pretraining"> </d-cite>:</li> <li><strong>Fast Attention Requires Bounded Entries</strong><d-cite key="alman2023fast"> </d-cite>:</li> <li><strong>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment</strong><d-cite key="ni2023transformers"> </d-cite>:</li> <li><strong>MotionGPT: Human Motion as a Foreign Language</strong><d-cite key="jiang2023motiongpt"> </d-cite>:</li> </ol> <h2 id="reinforcement-learning">Reinforcement Learning</h2> <p>Reinforcement Learning was a huge topic this year, with many papers discussing RL in the context of other works like diffusion models <d-cite key="he2023diffusion,kang2023efficient"> </d-cite> or in-context learning <d-cite key="lee2023supervised"> </d-cite>. There is a distinction between classical RL and deep RL works, the former of which are primarily theoretical, and the latter of which are primarily empirical. The main difference is the use of a neural network to approximate tabular mappings, especially in settings involving an infinite or combinatorially large state and/or action space. I’m not entirely sure what direction the field has been in the past few years because it is so broad, but I have noticed an emphasis on sample efficiency and the utilization of priors to accelerate RL exploration. This observation is probably because earlier successes in deep RL have primarily been through OpenAI or DeepMind brute forcing domain-specific trajectories into a model, so now that we know that RL works at scale, we can try to work towards making it efficient. Lastly, I noticed a lot of papers dealing with offline RL <d-cite key="chen2023conservative,hong2023beyond"></d-cite>, where models that make updates without interacting with the environment.</p> <ol> <li><strong>Robustness.</strong> I’m specifically referring to the robustness of RL training algorithms and preventing failure modes. It is fairly well-known that RL is quite delicate and requires a lot of tricks <d-cite key="rlblogpost"> </d-cite> to get working in practice, so it is unsurprising that a lot of work goes into improving the robustness of these algorithms. There are many failure modes of RL that are addressed in this conference, and I highlight them below: <ol> <li>Balancing the ratio of updates to timesteps is important for trading off convergence and sample efficiency. To prevent <strong>primacy bias</strong> (favoring early explorations), deep RL methods often perform a “reset” of their weights while storing the transition data in a replay buffer. Doing this can cause the model to diverge on reset, so <d-cite key="kim2023sampleefficient"></d-cite> attempt to circumvent this issue by using an ensemble of agents and perform random resets so at least one agent is not reset.</li> <li>We often want RL agents to act <em>conservatively</em>, so when they reach an unseen state, they do not act wild. In <d-cite key="chen2023conservative"></d-cite>, they add penalties to out-of-distribution states and prove that in an offline RL setting, they achieve a conservative estimation of the expected value function.</li> <li>Deploying RL in realistic settings often implies the need for safety constraints to avoid exploring unsafe states. Works like <d-cite key="wachi2023safe,kim2023sampleefficient"></d-cite> look into imposing these constraints with high probability.</li> <li>There are many tricks involved in RL training, many of which are problem-dependent. For example, importance weighting over the training dataset in offline RL to prevent being influenced by low-reward trajectories <d-cite key="hong2023beyond"></d-cite>. Or reducing variance in the learning process with multi-step surrogate rewards <d-cite key="zhong2022long"></d-cite>. In <d-cite key="ma2023learning"></d-cite>, they analyze why augmenting visual observations leads to better sample efficiency. Some works even investigate tricks from other models like reward scaling and regularization schemes and apply them more generally <d-cite key="sullivan2023reward"></d-cite>.</li> </ol> </li> <li><strong>Improving Exploration by Leveraging Priors</strong> Reward functions play a huge role in the convergence of RL training. In most settings, the only “true” reward is a sparse reward given for achieving a goal (e.g. winning a chess game). However, propagating this sparse reward through a combinatorially large state and transition function space is fundamentally difficult, so people often design intrinsic reward functions to better guide an agent towards maximizing the true reward. Some methods attempt to generally identify good transitions and states to explore <d-cite key="lin2023mimex,jain2023maximum"></d-cite> by looking for state diversity and orthogonality, while other methods focus on automating a reward designer’s intent by conditioning on images, videos, or language through some pre-trained embedding space and push exploration in a certain direction using these exploration “hints” <d-cite key="kim2023guide,escontrela2023video,wu2023read,gupta2023behavior"></d-cite>. Meanwhile, <d-cite key="nikishin2023deep"></d-cite> propose that even with sufficient exploration, a model may not learn a good policy, so dynamically modifying the model during training may fix that!</li> <li><strong>Learning World Models for Model-based RL (MBRL).</strong> If you’re familiar with the basic formulation for the dynamic programming update functions used in RL, you’ll know that knowing the <em>transition function</em>, or the model of the environment dynamics, is an extremely powerful guarantee that you generally do not have. However, it is possible to try and “learn” this model to apply MBRL methods. One of my research interests as of late is learning and editing world models using language, so it is pretty exciting to see these works. A central theme in the use of world models is learning and acting “in imagination”, which means treating the world model itself as an environment that the agent can interact in, which is especially useful for environments where interacting is costly or dangerous. In <d-cite key="chung2023thinker"></d-cite>, they use the world model as a way for the model to “think” before it generates an action. In <d-cite key="guan2023leveraging"></d-cite>, they learn strictly formatted world models in a standardized language <d-footnote>https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language</d-footnote> that models can interact with. Furthermore, many world models use an recurrent neural network (RNN) as the backbone for historical reason, so in <d-cite key="deng2023facing"></d-cite>, they experiment with different models for the world model backbone and propose their own with better</li> <li><strong>Multi-agent RL.</strong></li> <li><strong>Goal-conditioned RL.</strong> Goal-conditioned RL is a class of algorithms or problems where decisions are conditioned on both a state and a goal.</li> <li><strong>Theoretical Analysis.</strong> There were a lot of papers on bandit problems (especially adversarial or contextual bandits) <d-cite key="olkhovskaya2023first"></d-cite> and provable regret/convergence bounds <d-cite key="whitehouse2023on"></d-cite>, most of which I was not really able to understand. While I think following the math itself and reading through it is quite fun, I’m just not familiar with what problems people are interested in and what is unsolved, so it’s hard to gather from the abstracts or even a quick skim of the papers what the immediate impact is. That’s not to say that these works are not interesting or useful, but I am going to be careful not to say something false about their results. However, I did find two works that prove convergence guarantees for <em>deep RL</em>!. In <d-cite key="zhang2023convergence"></d-cite>, they prove convergence guarantees for deep Q learning using $\epsilon$-greedy exploration under accelerated gradient descent (momentum) under some pretty minimal assumptions. In <d-cite key="gaur2023global,zhong2023theoretical"></d-cite>, they limit their analysis to linear MDPs and simplified neural networks, but show convergence guarantees for actor-critic and proximal policy optimization (PPO) methods respectively.</li> </ol> <h3 id="interesting-papers-3">Interesting Papers</h3> <ol> <li>Conditional Mutual Information for Disentangled Representations in Reinforcement Learning</li> <li>Creating Multi-Level Skill Hierarchies in Reinforcement Learning &lt;d-cite key=”evans2023creating”&gt;&lt;/d-cite&gt;:</li> <li><strong>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</strong><d-cite key="chen2023efficient"></d-cite>: An interesting question is sort of the “error” of observability. This paper tries to theoretically tackle the worst-case performance induced by this.</li> <li><strong>Learning to Influence Human Behavior with Offline Reinforcement Learning</strong> <d-cite key="hong2023learning"></d-cite>:</li> <li><strong>Is RLHF More Difficult Than Standard RL?</strong> <d-cite key="wang2023rlhf"></d-cite>:</li> <li><strong>A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</strong> <d-cite key="zhong2023theoretical"></d-cite>: I’ve understood PPO as a series of empirical tricks and approximations to the theoretically motivated Trust Region Policy Optimization (TRPO), so I always thought that studying its theoretical properties to provably converge has been lacking. Even if this study is applied to a simple RL setting, it’s an important first step towards theoretically motivating PPO.</li> </ol> <h2 id="generative-ai">Generative AI</h2> <p>The sections on LLMs and multimodal learning also fall under the category of</p> <h3 id="diffusion-models">Diffusion Models</h3> <h2 id="computer-vision">Computer Vision</h2> <h2 id="graph-neural-networks">Graph Neural Networks</h2> <h2 id="adversarial-attacks-and-model-poisoning">Adversarial Attacks and Model Poisoning</h2> <h2 id="privacy-and-federated-learning">Privacy and Federated Learning</h2> <h2 id="knowledge-distillation-and-memory-reduction-schemes">Knowledge Distillation and Memory Reduction Schemes</h2> <h2 id="implicit-bias">Implicit Bias</h2> <h2 id="interpretability-and-explainable-ai">Interpretability and Explainable AI</h2> <h2 id="training-dynamics">Training Dynamics</h2> <h2 id="datasets-benchmarks-challenges">Datasets, Benchmarks, Challenges</h2> <h2 id="other-topics">Other Topics</h2> <p>The following sections are dedicated to topics that were either not as popular this year but are still broadly relevant or where I could not really get a sense of the central themes surrounding them. The main issue boils down to not having enough background on the topic, so I have to go through a few papers on the subject before comprehensively understanding what they’re doing. Regardless, they each had some interesting papers to highlight.</p> <h3 id="embodied-ai">Embodied AI</h3> <h3 id="neural-architecture-search">Neural Architecture Search</h3> <h3 id="variational-inference-methods">Variational Inference Methods</h3> <h3 id="quantum-information-theory">Quantum Information Theory</h3> <h3 id="gflownets-and-energy-based-models">GFlowNets and Energy-based Models</h3> <h3 id="curriculum-learning">Curriculum Learning</h3> <h3 id="anomaly-detection">Anomaly Detection</h3> <h3 id="class-imbalance-approaches">Class Imbalance Approaches</h3> <h3 id="continual-learning">Continual Learning</h3> <h3 id="bio-inspired-ai">Bio-inspired AI</h3> <h3 id="domain-specific-applications-of-ai">Domain-specific applications of AI</h3> <p>There were a lot of very cool domain-specific applications of AIML at NeurIPS this year, most of which was completely beyond me.</p> <h3 id="mean-field-theory">Mean-field Theory</h3> <h2 id="other-papers-i-liked">Other Papers I Liked</h2> <h2 id="final-thoughts">Final Thoughts</h2> <h2 id="open-research-questions">Open Research Questions</h2> <ol> <li>“An interesting research direction is to explore whether the benefits of Recycled-AltUp can be explained entirely by more favorable training dynamics.”</li> </ol> <h2 id="random-terms">Random Terms</h2> <p>I kept track of a list of terms that I had to Google while going through these abstracts. I probably also ended up looking into more and forgot to put them in this list, but in case you’re interested, here they are below:</p>]]></content><author><name>Alex Zhang</name></author><category term="blog"/><category term="summary"/><category term="generative"/><category term="ai"/><summary type="html"><![CDATA[Just me reading through every paper abstract...]]></summary></entry></feed>